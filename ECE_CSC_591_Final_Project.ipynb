{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65b1b1a51c1e4fd2841a901e6f98d864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee4466ad132b44e681b57338a55412a5",
              "IPY_MODEL_73f62d667f6144dba1c0dd27fad46635",
              "IPY_MODEL_7b879f935b334ab9b22d3465064848d8"
            ],
            "layout": "IPY_MODEL_b7a11f94bcee44f3a33d25c3cc389655"
          }
        },
        "ee4466ad132b44e681b57338a55412a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a299ea708994b7a9cfa5b6534ae818e",
            "placeholder": "​",
            "style": "IPY_MODEL_55736365d0664a38a87d40390a870525",
            "value": "Loss: 0.2903: 100%"
          }
        },
        "73f62d667f6144dba1c0dd27fad46635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31a7887a944c4c19a2f8635ad77d2bb0",
            "max": 469,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2adb166b406c46cb918de2af13199d71",
            "value": 469
          }
        },
        "7b879f935b334ab9b22d3465064848d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d17cfb2d1e4b4717b9fb49413ecff0bb",
            "placeholder": "​",
            "style": "IPY_MODEL_4e68c81934c34106bb85109bcc2c278d",
            "value": " 469/469 [11:26&lt;00:00,  6.03s/it]"
          }
        },
        "b7a11f94bcee44f3a33d25c3cc389655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a299ea708994b7a9cfa5b6534ae818e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55736365d0664a38a87d40390a870525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31a7887a944c4c19a2f8635ad77d2bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2adb166b406c46cb918de2af13199d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d17cfb2d1e4b4717b9fb49413ecff0bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e68c81934c34106bb85109bcc2c278d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy import linalg\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NBHx5JrRtDi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "ZtXGjTJztDmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set manual seed to a constant get a consistent output\n",
        "manualSeed =999\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G8rPRc6tIWg",
        "outputId": "4fc307b2-7ec4-4d2b-eafa-309bb8157314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  999\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79e566ea5210>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "# dataset = dset.CIFAR10(root=\"./data\", download=True,\n",
        "#                            transform=transforms.Compose([\n",
        "#                                transforms.Resize(64),\n",
        "#                                transforms.ToTensor(),\n",
        "#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "#                            ]))\n",
        "dataset = dset.FashionMNIST(root=\"./data\", download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(64),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5,), (0.5, )),\n",
        "                           ]))\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "bXcp1fERtIaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b73f1263-7f5f-4066-8b3c-572b91435ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16854826.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 265708.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5000026.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 6420540.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nc=1\n",
        "\n",
        "#checking the availability of cuda devices\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# number of gpu's available\n",
        "ngpu = 1\n",
        "# input noise dimension\n",
        "nz = 100\n",
        "# number of generator filters\n",
        "ngf = 64\n",
        "\n",
        "#number of discriminator filters\n",
        "ndf = 64\n"
      ],
      "metadata": {
        "id": "K7D72SlOtIdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1 and classname.find('Prune') != -1:\n",
        "        m.conv.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('Linear') != -1 and classname.find('Prune') != -1:\n",
        "        m.linear.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "    elif classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n"
      ],
      "metadata": {
        "id": "LtUCUge2toO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PruneLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(PruneLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "        self.mask = np.ones([self.out_features, self.in_features])\n",
        "        m = self.in_features\n",
        "        n = self.out_features\n",
        "        self.sparsity = 1.0\n",
        "        # Initailization\n",
        "        self.linear.weight.data.normal_(0, math.sqrt(2. / (m+n)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "        pass\n",
        "\n",
        "    def prune_by_percentage(self, q=5.0):\n",
        "        \"\"\"\n",
        "        Pruning the weight paramters by threshold.\n",
        "        :param q: pruning percentile. 'q' percent of the least\n",
        "        significant weight parameters will be pruned.\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        Prune the weight connections by percentage. Calculate the sparisty after\n",
        "        pruning and store it into 'self.sparsity'.\n",
        "        Store the pruning pattern in 'self.mask' for further fine-tuning process\n",
        "        with pruned connections.\n",
        "        --------------Your Code---------------------\n",
        "        \"\"\"\n",
        "\n",
        "        current_weights = self.linear.weight.data.view(-1).cpu().numpy()\n",
        "        threshold = np.percentile(np.abs(current_weights), q)\n",
        "\n",
        "        self.mask = np.abs(current_weights) >= threshold\n",
        "\n",
        "        mask_as_tensor = torch.from_numpy(self.mask).float().to(self.linear.weight.device)\n",
        "        mask_as_tensor = mask_as_tensor.view_as(self.linear.weight.data)\n",
        "\n",
        "        self.linear.weight.data *= mask_as_tensor\n",
        "\n",
        "        self.sparsity = 1 - (np.sum(self.mask) / len(current_weights))\n",
        "\n",
        "\n",
        "    def prune_by_std(self, s=0.25):\n",
        "        \"\"\"\n",
        "        Pruning by a factor of the standard deviation value.\n",
        "        :param std: (scalar) factor of the standard deviation value.\n",
        "        Weight magnitude below np.std(weight)*std\n",
        "        will be pruned.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Prune the weight connections by standarad deviation.\n",
        "        Calculate the sparisty after pruning and store it into 'self.sparsity'.\n",
        "        Store the pruning pattern in 'self.mask' for further fine-tuning process\n",
        "        with pruned connections.\n",
        "        --------------Your Code---------------------\n",
        "        \"\"\"\n",
        "\n",
        "        current_weights = self.linear.weight.data.view(-1).cpu().numpy()\n",
        "        threshold = np.std(current_weights) * s\n",
        "\n",
        "        self.mask = np.abs(current_weights) >= threshold\n",
        "\n",
        "        mask_as_tensor = torch.from_numpy(self.mask).float().to(self.linear.weight.device)\n",
        "        mask_as_tensor = mask_as_tensor.view_as(self.linear.weight.data)\n",
        "\n",
        "        self.linear.weight.data *= mask_as_tensor\n",
        "\n",
        "        self.sparsity = 1 - (np.sum(self.mask) / len(current_weights))\n",
        "\n",
        "\n",
        "\n",
        "class PrunedConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n",
        "        super(PrunedConv, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
        "\n",
        "        # Expand and Transpose to match the dimension\n",
        "        self.mask = np.ones_like([out_channels, in_channels, kernel_size, kernel_size])\n",
        "\n",
        "        # Initialization\n",
        "        n = self.kernel_size * self.kernel_size * self.out_channels\n",
        "        m = self.kernel_size * self.kernel_size * self.in_channels\n",
        "        self.conv.weight.data.normal_(0, math.sqrt(2. / (n+m) ))\n",
        "        self.sparsity = 1.0\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        return out\n",
        "\n",
        "    def prune_by_percentage(self, q=5.0):\n",
        "        \"\"\"\n",
        "        Pruning by a factor of the standard deviation value.\n",
        "        :param s: (scalar) factor of the standard deviation value.\n",
        "        Weight magnitude below np.std(weight)*std\n",
        "        will be pruned.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Prune the weight connections by percentage. Calculate the sparisty after\n",
        "        pruning and store it into 'self.sparsity'.\n",
        "        Store the pruning pattern in 'self.mask' for further fine-tuning process\n",
        "        with pruned connections.\n",
        "        --------------Your Code---------------------\n",
        "        \"\"\"\n",
        "\n",
        "        current_weights = self.conv.weight.data.view(-1).cpu().numpy()\n",
        "        threshold = np.percentile(np.abs(current_weights), q)\n",
        "\n",
        "        self.mask = np.abs(current_weights) >= threshold\n",
        "\n",
        "        mask_as_tensor = torch.from_numpy(self.mask).float().to(self.conv.weight.device)\n",
        "        mask_as_tensor = mask_as_tensor.view_as(self.conv.weight.data)\n",
        "\n",
        "        self.conv.weight.data *= mask_as_tensor\n",
        "\n",
        "        self.sparsity = 1 - (np.sum(self.mask) / len(current_weights))\n",
        "\n",
        "    def prune_by_std(self, s=0.25):\n",
        "        \"\"\"\n",
        "        Pruning by a factor of the standard deviation value.\n",
        "        :param s: (scalar) factor of the standard deviation value.\n",
        "        Weight magnitude below np.std(weight)*std\n",
        "        will be pruned.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Prune the weight connections by standarad deviation.\n",
        "        Calculate the sparisty after pruning and store it into 'self.sparsity'.\n",
        "        Store the pruning pattern in 'self.mask' for further fine-tuning process\n",
        "        with pruned connections.\n",
        "        --------------Your Code---------------------\n",
        "        \"\"\"\n",
        "\n",
        "        current_weights = self.conv.weight.data.view(-1).cpu().numpy()\n",
        "        threshold = np.std(current_weights) * s\n",
        "\n",
        "        self.mask = np.abs(current_weights) >= threshold\n",
        "\n",
        "        mask_as_tensor = torch.from_numpy(self.mask).float().to(self.conv.weight.device)\n",
        "        mask_as_tensor = mask_as_tensor.view_as(self.conv.weight.data)\n",
        "\n",
        "        self.conv.weight.data *= mask_as_tensor\n",
        "\n",
        "        self.sparsity = 1 - (np.sum(self.mask) / len(current_weights))\n",
        "\n",
        "class PrunedConvTrans(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n",
        "        super(PrunedConvTrans, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
        "\n",
        "        # Expand and Transpose to match the dimension\n",
        "        self.mask = np.ones_like([out_channels, in_channels, kernel_size, kernel_size])\n",
        "\n",
        "        # Initialization\n",
        "        n = self.kernel_size * self.kernel_size * self.out_channels\n",
        "        m = self.kernel_size * self.kernel_size * self.in_channels\n",
        "        self.conv.weight.data.normal_(0, math.sqrt(2. / (n+m) ))\n",
        "        self.sparsity = 1.0\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        return out\n",
        "\n",
        "    def prune_by_percentage(self, q=5.0):\n",
        "        \"\"\"\n",
        "        Pruning by a factor of the standard deviation value.\n",
        "        :param s: (scalar) factor of the standard deviation value.\n",
        "        Weight magnitude below np.std(weight)*std\n",
        "        will be pruned.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Prune the weight connections by percentage. Calculate the sparisty after\n",
        "        pruning and store it into 'self.sparsity'.\n",
        "        Store the pruning pattern in 'self.mask' for further fine-tuning process\n",
        "        with pruned connections.\n",
        "        --------------Your Code---------------------\n",
        "        \"\"\"\n",
        "\n",
        "        current_weights = self.conv.weight.data.view(-1).cpu().numpy()\n",
        "        threshold = np.percentile(np.abs(current_weights), q)\n",
        "\n",
        "        self.mask = np.abs(current_weights) >= threshold\n",
        "\n",
        "        mask_as_tensor = torch.from_numpy(self.mask).float().to(self.conv.weight.device)\n",
        "        mask_as_tensor = mask_as_tensor.view_as(self.conv.weight.data)\n",
        "\n",
        "        self.conv.weight.data *= mask_as_tensor\n",
        "\n",
        "        self.sparsity = 1 - (np.sum(self.mask) / len(current_weights))\n",
        "\n",
        "    def prune_by_std(self, s=0.25):\n",
        "        \"\"\"\n",
        "        Pruning by a factor of the standard deviation value.\n",
        "        :param s: (scalar) factor of the standard deviation value.\n",
        "        Weight magnitude below np.std(weight)*std\n",
        "        will be pruned.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Prune the weight connections by standarad deviation.\n",
        "        Calculate the sparisty after pruning and store it into 'self.sparsity'.\n",
        "        Store the pruning pattern in 'self.mask' for further fine-tuning process\n",
        "        with pruned connections.\n",
        "        --------------Your Code---------------------\n",
        "        \"\"\"\n",
        "\n",
        "        current_weights = self.conv.weight.data.view(-1).cpu().numpy()\n",
        "        threshold = np.std(current_weights) * s\n",
        "\n",
        "        self.mask = np.abs(current_weights) >= threshold\n",
        "\n",
        "        mask_as_tensor = torch.from_numpy(self.mask).float().to(self.conv.weight.device)\n",
        "        mask_as_tensor = mask_as_tensor.view_as(self.conv.weight.data)\n",
        "\n",
        "        self.conv.weight.data *= mask_as_tensor\n",
        "\n",
        "        self.sparsity = 1 - (np.sum(self.mask) / len(current_weights))\n"
      ],
      "metadata": {
        "id": "ryv7Ig6AHrUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            PrunedConvTrans(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            PrunedConvTrans(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            PrunedConvTrans(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            PrunedConvTrans(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            PrunedConvTrans(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "            return output"
      ],
      "metadata": {
        "id": "j0voVuD9toS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            PrunedConv(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            PrunedConv(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            PrunedConv(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            PrunedConv(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            PrunedConv(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "\n",
        "        return output.view(-1, 1).squeeze(1)\n"
      ],
      "metadata": {
        "id": "3sTVhv-3tuIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gSS3KAe2G08w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "#load weights to test the model\n",
        "#netG.load_state_dict(torch.load('weights/netG_epoch_24.pth'))\n",
        "print(netG)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa4bohYBtuMw",
        "outputId": "ca9d5a81-9308-4958-997a-74ce0beaae1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): PrunedConvTrans(\n",
            "      (conv): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    )\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): PrunedConvTrans(\n",
            "      (conv): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): PrunedConvTrans(\n",
            "      (conv): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): PrunedConvTrans(\n",
            "      (conv): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): PrunedConvTrans(\n",
            "      (conv): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "netD = Discriminator(ngpu).to(device)\n",
        "netD.apply(weights_init)\n",
        "#load weights to test the model\n",
        "#netD.load_state_dict(torch.load('weights/netD_epoch_24.pth'))\n",
        "print(netD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycqpYQ8Kt5f-",
        "outputId": "245fa614-4367-4807-cc0a-e927123167e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): PrunedConv(\n",
            "      (conv): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): PrunedConv(\n",
            "      (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): PrunedConv(\n",
            "      (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): PrunedConv(\n",
            "      (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): PrunedConv(\n",
            "      (conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    )\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "\n",
        "# setup optimizer\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0003, betas=(0.5, 0.999))\n",
        "\n",
        "fixed_noise = torch.randn(128, nz, 1, 1, device=device)\n",
        "real_label = 1\n",
        "fake_label = 0\n"
      ],
      "metadata": {
        "id": "sgoXqOCEt5kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "niter = 15\n",
        "g_loss = []\n",
        "d_loss = []\n",
        "\n",
        "for epoch in range(niter):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        # train with real\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data[0].to(device)\n",
        "        batch_size = real_cpu.size(0)\n",
        "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
        "\n",
        "        output = netD(real_cpu)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        # train with fake\n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' % (epoch, niter, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        #save the output\n",
        "        if i % 100 == 0:\n",
        "            print('saving the output')\n",
        "            vutils.save_image(real_cpu,'output/real_samples.png',normalize=True)\n",
        "            fake = netG(fixed_noise)\n",
        "            vutils.save_image(fake.detach(),'output/fake_samples_epoch_%03d.png' % (epoch),normalize=True)\n",
        "\n",
        "    # Check pointing for every epoch\n",
        "    torch.save(netG.state_dict(), 'weights/netG_epoch_%d.pth' % (epoch))\n",
        "    torch.save(netD.state_dict(), 'weights/netD_epoch_%d.pth' % (epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RYdMZfsbt9r_",
        "outputId": "416d4f64-d115-432f-8b7c-4a02baf9b2ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0/15][0/469] Loss_D: 3.9052 Loss_G: 0.7918 D(x): 0.9681 D(G(z)): 0.9663 / 0.5112\n",
            "saving the output\n",
            "[0/15][1/469] Loss_D: 3.6212 Loss_G: 1.5575 D(x): 0.9673 D(G(z)): 0.9545 / 0.2752\n",
            "[0/15][2/469] Loss_D: 2.8868 Loss_G: 2.6249 D(x): 0.9415 D(G(z)): 0.9113 / 0.1060\n",
            "[0/15][3/469] Loss_D: 1.9875 Loss_G: 3.8496 D(x): 0.8975 D(G(z)): 0.7787 / 0.0400\n",
            "[0/15][4/469] Loss_D: 1.5935 Loss_G: 4.4465 D(x): 0.8262 D(G(z)): 0.6658 / 0.0208\n",
            "[0/15][5/469] Loss_D: 1.7009 Loss_G: 4.6070 D(x): 0.7487 D(G(z)): 0.6774 / 0.0149\n",
            "[0/15][6/469] Loss_D: 1.7460 Loss_G: 4.9374 D(x): 0.7873 D(G(z)): 0.6930 / 0.0122\n",
            "[0/15][7/469] Loss_D: 1.6108 Loss_G: 5.5265 D(x): 0.7890 D(G(z)): 0.6674 / 0.0066\n",
            "[0/15][8/469] Loss_D: 1.8009 Loss_G: 5.8860 D(x): 0.7778 D(G(z)): 0.7090 / 0.0042\n",
            "[0/15][9/469] Loss_D: 1.4667 Loss_G: 6.6301 D(x): 0.7948 D(G(z)): 0.6427 / 0.0021\n",
            "[0/15][10/469] Loss_D: 1.2307 Loss_G: 6.9334 D(x): 0.8093 D(G(z)): 0.5594 / 0.0016\n",
            "[0/15][11/469] Loss_D: 1.0704 Loss_G: 6.8640 D(x): 0.7551 D(G(z)): 0.4627 / 0.0016\n",
            "[0/15][12/469] Loss_D: 1.0658 Loss_G: 7.0873 D(x): 0.7811 D(G(z)): 0.4643 / 0.0013\n",
            "[0/15][13/469] Loss_D: 1.1003 Loss_G: 7.8620 D(x): 0.8222 D(G(z)): 0.5201 / 0.0006\n",
            "[0/15][14/469] Loss_D: 1.2017 Loss_G: 8.2534 D(x): 0.7379 D(G(z)): 0.4734 / 0.0004\n",
            "[0/15][15/469] Loss_D: 1.1593 Loss_G: 8.7503 D(x): 0.7766 D(G(z)): 0.5007 / 0.0003\n",
            "[0/15][16/469] Loss_D: 1.1064 Loss_G: 8.7794 D(x): 0.7234 D(G(z)): 0.4462 / 0.0002\n",
            "[0/15][17/469] Loss_D: 1.1383 Loss_G: 8.3400 D(x): 0.7024 D(G(z)): 0.3969 / 0.0003\n",
            "[0/15][18/469] Loss_D: 0.9300 Loss_G: 8.5703 D(x): 0.7592 D(G(z)): 0.3893 / 0.0003\n",
            "[0/15][19/469] Loss_D: 0.6797 Loss_G: 8.0471 D(x): 0.7850 D(G(z)): 0.2404 / 0.0004\n",
            "[0/15][20/469] Loss_D: 0.8133 Loss_G: 8.8829 D(x): 0.8187 D(G(z)): 0.3715 / 0.0002\n",
            "[0/15][21/469] Loss_D: 0.7128 Loss_G: 8.7887 D(x): 0.8121 D(G(z)): 0.3008 / 0.0002\n",
            "[0/15][22/469] Loss_D: 1.0592 Loss_G: 9.7071 D(x): 0.7926 D(G(z)): 0.4516 / 0.0001\n",
            "[0/15][23/469] Loss_D: 0.6381 Loss_G: 7.6067 D(x): 0.7371 D(G(z)): 0.1383 / 0.0007\n",
            "[0/15][24/469] Loss_D: 0.9733 Loss_G: 9.5691 D(x): 0.8524 D(G(z)): 0.5021 / 0.0001\n",
            "[0/15][25/469] Loss_D: 0.5062 Loss_G: 7.5930 D(x): 0.7958 D(G(z)): 0.0812 / 0.0007\n",
            "[0/15][26/469] Loss_D: 0.9849 Loss_G: 9.4642 D(x): 0.8634 D(G(z)): 0.5163 / 0.0001\n",
            "[0/15][27/469] Loss_D: 0.4137 Loss_G: 7.8063 D(x): 0.8286 D(G(z)): 0.1265 / 0.0005\n",
            "[0/15][28/469] Loss_D: 0.9080 Loss_G: 9.3422 D(x): 0.8639 D(G(z)): 0.4648 / 0.0001\n",
            "[0/15][29/469] Loss_D: 0.4445 Loss_G: 7.4890 D(x): 0.8081 D(G(z)): 0.0980 / 0.0007\n",
            "[0/15][30/469] Loss_D: 0.8635 Loss_G: 9.3811 D(x): 0.8481 D(G(z)): 0.4466 / 0.0001\n",
            "[0/15][31/469] Loss_D: 0.5305 Loss_G: 7.4989 D(x): 0.7917 D(G(z)): 0.1036 / 0.0007\n",
            "[0/15][32/469] Loss_D: 0.9178 Loss_G: 9.7073 D(x): 0.8395 D(G(z)): 0.4533 / 0.0001\n",
            "[0/15][33/469] Loss_D: 0.3412 Loss_G: 7.5676 D(x): 0.8350 D(G(z)): 0.0565 / 0.0006\n",
            "[0/15][34/469] Loss_D: 0.6429 Loss_G: 9.0484 D(x): 0.8880 D(G(z)): 0.3682 / 0.0001\n",
            "[0/15][35/469] Loss_D: 0.6118 Loss_G: 6.9723 D(x): 0.7590 D(G(z)): 0.0626 / 0.0013\n",
            "[0/15][36/469] Loss_D: 0.8253 Loss_G: 9.2582 D(x): 0.8353 D(G(z)): 0.3859 / 0.0001\n",
            "[0/15][37/469] Loss_D: 0.4107 Loss_G: 7.3110 D(x): 0.8198 D(G(z)): 0.0330 / 0.0009\n",
            "[0/15][38/469] Loss_D: 0.4116 Loss_G: 6.7309 D(x): 0.8872 D(G(z)): 0.1912 / 0.0016\n",
            "[0/15][39/469] Loss_D: 0.5361 Loss_G: 9.0779 D(x): 0.8999 D(G(z)): 0.2971 / 0.0001\n",
            "[0/15][40/469] Loss_D: 0.3187 Loss_G: 7.2058 D(x): 0.8448 D(G(z)): 0.0394 / 0.0010\n",
            "[0/15][41/469] Loss_D: 0.4299 Loss_G: 7.3095 D(x): 0.8870 D(G(z)): 0.2066 / 0.0008\n",
            "[0/15][42/469] Loss_D: 0.4842 Loss_G: 7.1107 D(x): 0.8337 D(G(z)): 0.1277 / 0.0010\n",
            "[0/15][43/469] Loss_D: 0.5396 Loss_G: 8.0988 D(x): 0.8672 D(G(z)): 0.2060 / 0.0004\n",
            "[0/15][44/469] Loss_D: 0.2307 Loss_G: 7.4459 D(x): 0.9159 D(G(z)): 0.0995 / 0.0007\n",
            "[0/15][45/469] Loss_D: 0.4371 Loss_G: 7.8011 D(x): 0.8701 D(G(z)): 0.1892 / 0.0005\n",
            "[0/15][46/469] Loss_D: 0.3749 Loss_G: 6.9530 D(x): 0.8597 D(G(z)): 0.1068 / 0.0011\n",
            "[0/15][47/469] Loss_D: 0.3953 Loss_G: 8.3253 D(x): 0.9062 D(G(z)): 0.2230 / 0.0003\n",
            "[0/15][48/469] Loss_D: 0.2923 Loss_G: 6.9682 D(x): 0.8795 D(G(z)): 0.0597 / 0.0011\n",
            "[0/15][49/469] Loss_D: 0.3508 Loss_G: 7.6782 D(x): 0.9098 D(G(z)): 0.2000 / 0.0005\n",
            "[0/15][50/469] Loss_D: 0.1921 Loss_G: 7.2585 D(x): 0.9294 D(G(z)): 0.0978 / 0.0008\n",
            "[0/15][51/469] Loss_D: 0.3843 Loss_G: 7.8745 D(x): 0.9008 D(G(z)): 0.1807 / 0.0005\n",
            "[0/15][52/469] Loss_D: 0.2209 Loss_G: 7.3174 D(x): 0.9214 D(G(z)): 0.0957 / 0.0008\n",
            "[0/15][53/469] Loss_D: 0.4417 Loss_G: 8.6730 D(x): 0.8834 D(G(z)): 0.2103 / 0.0002\n",
            "[0/15][54/469] Loss_D: 0.2612 Loss_G: 7.2768 D(x): 0.8806 D(G(z)): 0.0353 / 0.0009\n",
            "[0/15][55/469] Loss_D: 0.3566 Loss_G: 7.9845 D(x): 0.9313 D(G(z)): 0.1898 / 0.0005\n",
            "[0/15][56/469] Loss_D: 0.1943 Loss_G: 7.5682 D(x): 0.9408 D(G(z)): 0.0858 / 0.0007\n",
            "[0/15][57/469] Loss_D: 0.1917 Loss_G: 7.2328 D(x): 0.9543 D(G(z)): 0.1027 / 0.0009\n",
            "[0/15][58/469] Loss_D: 0.2941 Loss_G: 7.7191 D(x): 0.9356 D(G(z)): 0.1365 / 0.0006\n",
            "[0/15][59/469] Loss_D: 0.2300 Loss_G: 7.7863 D(x): 0.9362 D(G(z)): 0.1088 / 0.0005\n",
            "[0/15][60/469] Loss_D: 0.1929 Loss_G: 7.4019 D(x): 0.9383 D(G(z)): 0.0956 / 0.0008\n",
            "[0/15][61/469] Loss_D: 0.2533 Loss_G: 7.4811 D(x): 0.9199 D(G(z)): 0.1188 / 0.0007\n",
            "[0/15][62/469] Loss_D: 0.1874 Loss_G: 7.2729 D(x): 0.9383 D(G(z)): 0.0947 / 0.0008\n",
            "[0/15][63/469] Loss_D: 0.2715 Loss_G: 6.9843 D(x): 0.9060 D(G(z)): 0.0906 / 0.0011\n",
            "[0/15][64/469] Loss_D: 0.2588 Loss_G: 7.0817 D(x): 0.9150 D(G(z)): 0.0924 / 0.0010\n",
            "[0/15][65/469] Loss_D: 0.2442 Loss_G: 6.7158 D(x): 0.9130 D(G(z)): 0.0688 / 0.0015\n",
            "[0/15][66/469] Loss_D: 0.1178 Loss_G: 7.0776 D(x): 0.9763 D(G(z)): 0.0871 / 0.0010\n",
            "[0/15][67/469] Loss_D: 0.1414 Loss_G: 6.8894 D(x): 0.9480 D(G(z)): 0.0725 / 0.0012\n",
            "[0/15][68/469] Loss_D: 0.2319 Loss_G: 7.1905 D(x): 0.9299 D(G(z)): 0.1030 / 0.0009\n",
            "[0/15][69/469] Loss_D: 0.1324 Loss_G: 6.7696 D(x): 0.9500 D(G(z)): 0.0694 / 0.0013\n",
            "[0/15][70/469] Loss_D: 0.2411 Loss_G: 6.9980 D(x): 0.9180 D(G(z)): 0.1042 / 0.0011\n",
            "[0/15][71/469] Loss_D: 0.1953 Loss_G: 7.1387 D(x): 0.9289 D(G(z)): 0.0946 / 0.0009\n",
            "[0/15][72/469] Loss_D: 0.2181 Loss_G: 7.0049 D(x): 0.9182 D(G(z)): 0.0876 / 0.0010\n",
            "[0/15][73/469] Loss_D: 0.2953 Loss_G: 7.7820 D(x): 0.9057 D(G(z)): 0.1183 / 0.0005\n",
            "[0/15][74/469] Loss_D: 0.2708 Loss_G: 6.9328 D(x): 0.8963 D(G(z)): 0.0731 / 0.0012\n",
            "[0/15][75/469] Loss_D: 0.3665 Loss_G: 7.5102 D(x): 0.8887 D(G(z)): 0.1190 / 0.0006\n",
            "[0/15][76/469] Loss_D: 0.1347 Loss_G: 7.1388 D(x): 0.9473 D(G(z)): 0.0586 / 0.0009\n",
            "[0/15][77/469] Loss_D: 0.2519 Loss_G: 6.3685 D(x): 0.8920 D(G(z)): 0.0842 / 0.0019\n",
            "[0/15][78/469] Loss_D: 0.2438 Loss_G: 8.5083 D(x): 0.9312 D(G(z)): 0.1337 / 0.0002\n",
            "[0/15][79/469] Loss_D: 0.3115 Loss_G: 4.3584 D(x): 0.8062 D(G(z)): 0.0142 / 0.0150\n",
            "[0/15][80/469] Loss_D: 0.7762 Loss_G: 16.9993 D(x): 0.9694 D(G(z)): 0.4906 / 0.0000\n",
            "[0/15][81/469] Loss_D: 1.8583 Loss_G: 10.0178 D(x): 0.3421 D(G(z)): 0.0000 / 0.0001\n",
            "[0/15][82/469] Loss_D: 0.0387 Loss_G: 3.0234 D(x): 0.9792 D(G(z)): 0.0155 / 0.0553\n",
            "[0/15][83/469] Loss_D: 3.1612 Loss_G: 18.0074 D(x): 0.9699 D(G(z)): 0.9287 / 0.0000\n",
            "[0/15][84/469] Loss_D: 3.9448 Loss_G: 13.4491 D(x): 0.0716 D(G(z)): 0.0000 / 0.0000\n",
            "[0/15][85/469] Loss_D: 0.3151 Loss_G: 5.9010 D(x): 0.8222 D(G(z)): 0.0012 / 0.0063\n",
            "[0/15][86/469] Loss_D: 1.7842 Loss_G: 13.3481 D(x): 0.9458 D(G(z)): 0.7619 / 0.0000\n",
            "[0/15][87/469] Loss_D: 1.3935 Loss_G: 8.5845 D(x): 0.3993 D(G(z)): 0.0006 / 0.0003\n",
            "[0/15][88/469] Loss_D: 0.3725 Loss_G: 4.8704 D(x): 0.8791 D(G(z)): 0.1577 / 0.0111\n",
            "[0/15][89/469] Loss_D: 1.6398 Loss_G: 13.0107 D(x): 0.9223 D(G(z)): 0.7107 / 0.0000\n",
            "[0/15][90/469] Loss_D: 2.1824 Loss_G: 8.3489 D(x): 0.3130 D(G(z)): 0.0003 / 0.0004\n",
            "[0/15][91/469] Loss_D: 0.4427 Loss_G: 3.0071 D(x): 0.8100 D(G(z)): 0.1195 / 0.0933\n",
            "[0/15][92/469] Loss_D: 1.8946 Loss_G: 10.3061 D(x): 0.9110 D(G(z)): 0.7518 / 0.0001\n",
            "[0/15][93/469] Loss_D: 1.3356 Loss_G: 7.0694 D(x): 0.5267 D(G(z)): 0.0059 / 0.0038\n",
            "[0/15][94/469] Loss_D: 0.7231 Loss_G: 3.0034 D(x): 0.7615 D(G(z)): 0.2052 / 0.1119\n",
            "[0/15][95/469] Loss_D: 1.9108 Loss_G: 7.4259 D(x): 0.8218 D(G(z)): 0.6865 / 0.0024\n",
            "[0/15][96/469] Loss_D: 1.4245 Loss_G: 3.9637 D(x): 0.5234 D(G(z)): 0.0867 / 0.0597\n",
            "[0/15][97/469] Loss_D: 1.2173 Loss_G: 3.0410 D(x): 0.6672 D(G(z)): 0.3591 / 0.1016\n",
            "[0/15][98/469] Loss_D: 1.3529 Loss_G: 6.1742 D(x): 0.7780 D(G(z)): 0.5561 / 0.0053\n",
            "[0/15][99/469] Loss_D: 1.7869 Loss_G: 1.2407 D(x): 0.3254 D(G(z)): 0.0984 / 0.3764\n",
            "[0/15][100/469] Loss_D: 1.9024 Loss_G: 5.2405 D(x): 0.8310 D(G(z)): 0.7486 / 0.0110\n",
            "saving the output\n",
            "[0/15][101/469] Loss_D: 1.5066 Loss_G: 2.3895 D(x): 0.3831 D(G(z)): 0.0929 / 0.1853\n",
            "[0/15][102/469] Loss_D: 1.2941 Loss_G: 3.5658 D(x): 0.7845 D(G(z)): 0.5285 / 0.0603\n",
            "[0/15][103/469] Loss_D: 1.1425 Loss_G: 2.9971 D(x): 0.5882 D(G(z)): 0.2654 / 0.0794\n",
            "[0/15][104/469] Loss_D: 1.1295 Loss_G: 2.6255 D(x): 0.6226 D(G(z)): 0.3182 / 0.1141\n",
            "[0/15][105/469] Loss_D: 0.9761 Loss_G: 3.8845 D(x): 0.7456 D(G(z)): 0.3737 / 0.0360\n",
            "[0/15][106/469] Loss_D: 1.1181 Loss_G: 1.4945 D(x): 0.5079 D(G(z)): 0.1543 / 0.2835\n",
            "[0/15][107/469] Loss_D: 0.8461 Loss_G: 4.1498 D(x): 0.8829 D(G(z)): 0.4595 / 0.0243\n",
            "[0/15][108/469] Loss_D: 0.5728 Loss_G: 3.2382 D(x): 0.7073 D(G(z)): 0.1043 / 0.0616\n",
            "[0/15][109/469] Loss_D: 0.4682 Loss_G: 2.7410 D(x): 0.8207 D(G(z)): 0.1663 / 0.0979\n",
            "[0/15][110/469] Loss_D: 0.6620 Loss_G: 2.9200 D(x): 0.7910 D(G(z)): 0.2699 / 0.0815\n",
            "[0/15][111/469] Loss_D: 0.7192 Loss_G: 2.7889 D(x): 0.7439 D(G(z)): 0.2574 / 0.0922\n",
            "[0/15][112/469] Loss_D: 0.7021 Loss_G: 2.1833 D(x): 0.6986 D(G(z)): 0.1966 / 0.1542\n",
            "[0/15][113/469] Loss_D: 0.8020 Loss_G: 3.4243 D(x): 0.8255 D(G(z)): 0.3911 / 0.0476\n",
            "[0/15][114/469] Loss_D: 0.8012 Loss_G: 1.9249 D(x): 0.5984 D(G(z)): 0.1158 / 0.1955\n",
            "[0/15][115/469] Loss_D: 0.6906 Loss_G: 3.0288 D(x): 0.8639 D(G(z)): 0.3668 / 0.0691\n",
            "[0/15][116/469] Loss_D: 0.4977 Loss_G: 3.1605 D(x): 0.7906 D(G(z)): 0.1670 / 0.0606\n",
            "[0/15][117/469] Loss_D: 0.5468 Loss_G: 2.4647 D(x): 0.7607 D(G(z)): 0.1617 / 0.1085\n",
            "[0/15][118/469] Loss_D: 0.6714 Loss_G: 2.5216 D(x): 0.7863 D(G(z)): 0.2819 / 0.1066\n",
            "[0/15][119/469] Loss_D: 0.5806 Loss_G: 2.9871 D(x): 0.8085 D(G(z)): 0.2503 / 0.0721\n",
            "[0/15][120/469] Loss_D: 0.6750 Loss_G: 2.1189 D(x): 0.6942 D(G(z)): 0.1712 / 0.1618\n",
            "[0/15][121/469] Loss_D: 0.7293 Loss_G: 3.2829 D(x): 0.8525 D(G(z)): 0.3628 / 0.0543\n",
            "[0/15][122/469] Loss_D: 0.5963 Loss_G: 2.8165 D(x): 0.7246 D(G(z)): 0.1403 / 0.0860\n",
            "[0/15][123/469] Loss_D: 0.6378 Loss_G: 2.4670 D(x): 0.7790 D(G(z)): 0.2087 / 0.1178\n",
            "[0/15][124/469] Loss_D: 0.5682 Loss_G: 3.1416 D(x): 0.8202 D(G(z)): 0.2518 / 0.0624\n",
            "[0/15][125/469] Loss_D: 0.5050 Loss_G: 3.0488 D(x): 0.7955 D(G(z)): 0.1727 / 0.0701\n",
            "[0/15][126/469] Loss_D: 0.4267 Loss_G: 2.5696 D(x): 0.8041 D(G(z)): 0.1285 / 0.0966\n",
            "[0/15][127/469] Loss_D: 0.5281 Loss_G: 2.6324 D(x): 0.8045 D(G(z)): 0.2119 / 0.0927\n",
            "[0/15][128/469] Loss_D: 0.4313 Loss_G: 3.0620 D(x): 0.8371 D(G(z)): 0.1724 / 0.0578\n",
            "[0/15][129/469] Loss_D: 0.3738 Loss_G: 2.9517 D(x): 0.8385 D(G(z)): 0.1485 / 0.0649\n",
            "[0/15][130/469] Loss_D: 0.4373 Loss_G: 1.9313 D(x): 0.7660 D(G(z)): 0.1064 / 0.1746\n",
            "[0/15][131/469] Loss_D: 0.6094 Loss_G: 3.6139 D(x): 0.8718 D(G(z)): 0.3314 / 0.0401\n",
            "[0/15][132/469] Loss_D: 0.4880 Loss_G: 2.2030 D(x): 0.7196 D(G(z)): 0.0817 / 0.1408\n",
            "[0/15][133/469] Loss_D: 0.5240 Loss_G: 2.6202 D(x): 0.8335 D(G(z)): 0.2511 / 0.0940\n",
            "[0/15][134/469] Loss_D: 0.5200 Loss_G: 2.4972 D(x): 0.7772 D(G(z)): 0.1703 / 0.1115\n",
            "[0/15][135/469] Loss_D: 0.5703 Loss_G: 2.3346 D(x): 0.7840 D(G(z)): 0.1958 / 0.1304\n",
            "[0/15][136/469] Loss_D: 0.4508 Loss_G: 3.4280 D(x): 0.8777 D(G(z)): 0.2408 / 0.0455\n",
            "[0/15][137/469] Loss_D: 0.7591 Loss_G: 1.2375 D(x): 0.6243 D(G(z)): 0.0924 / 0.3350\n",
            "[0/15][138/469] Loss_D: 0.8904 Loss_G: 4.5481 D(x): 0.9144 D(G(z)): 0.4967 / 0.0212\n",
            "[0/15][139/469] Loss_D: 0.5948 Loss_G: 2.7160 D(x): 0.6234 D(G(z)): 0.0456 / 0.1047\n",
            "[0/15][140/469] Loss_D: 0.3526 Loss_G: 2.0962 D(x): 0.8452 D(G(z)): 0.1417 / 0.1665\n",
            "[0/15][141/469] Loss_D: 0.5026 Loss_G: 3.6732 D(x): 0.9349 D(G(z)): 0.3025 / 0.0481\n",
            "[0/15][142/469] Loss_D: 0.4584 Loss_G: 3.0795 D(x): 0.7546 D(G(z)): 0.0924 / 0.0801\n",
            "[0/15][143/469] Loss_D: 0.3202 Loss_G: 2.6456 D(x): 0.8617 D(G(z)): 0.1296 / 0.1114\n",
            "[0/15][144/469] Loss_D: 0.3950 Loss_G: 2.8177 D(x): 0.8537 D(G(z)): 0.1784 / 0.0938\n",
            "[0/15][145/469] Loss_D: 0.4167 Loss_G: 2.7783 D(x): 0.8261 D(G(z)): 0.1572 / 0.0923\n",
            "[0/15][146/469] Loss_D: 0.4794 Loss_G: 2.8390 D(x): 0.8235 D(G(z)): 0.1855 / 0.0845\n",
            "[0/15][147/469] Loss_D: 0.7689 Loss_G: 1.2095 D(x): 0.6239 D(G(z)): 0.1525 / 0.3381\n",
            "[0/15][148/469] Loss_D: 0.8038 Loss_G: 4.7869 D(x): 0.9146 D(G(z)): 0.4575 / 0.0146\n",
            "[0/15][149/469] Loss_D: 0.9158 Loss_G: 1.2804 D(x): 0.5063 D(G(z)): 0.0299 / 0.3110\n",
            "[0/15][150/469] Loss_D: 0.7594 Loss_G: 3.6950 D(x): 0.9228 D(G(z)): 0.4535 / 0.0349\n",
            "[0/15][151/469] Loss_D: 0.4727 Loss_G: 2.9177 D(x): 0.7359 D(G(z)): 0.0887 / 0.0743\n",
            "[0/15][152/469] Loss_D: 0.5625 Loss_G: 1.7487 D(x): 0.7369 D(G(z)): 0.1757 / 0.2155\n",
            "[0/15][153/469] Loss_D: 0.7494 Loss_G: 4.5692 D(x): 0.8922 D(G(z)): 0.4247 / 0.0191\n",
            "[0/15][154/469] Loss_D: 0.7493 Loss_G: 2.3162 D(x): 0.5807 D(G(z)): 0.0499 / 0.1551\n",
            "[0/15][155/469] Loss_D: 0.4818 Loss_G: 2.8355 D(x): 0.8739 D(G(z)): 0.2559 / 0.0847\n",
            "[0/15][156/469] Loss_D: 0.3506 Loss_G: 3.6430 D(x): 0.8817 D(G(z)): 0.1678 / 0.0426\n",
            "[0/15][157/469] Loss_D: 0.3507 Loss_G: 3.3398 D(x): 0.8339 D(G(z)): 0.0988 / 0.0656\n",
            "[0/15][158/469] Loss_D: 0.3667 Loss_G: 3.2594 D(x): 0.8809 D(G(z)): 0.1759 / 0.0717\n",
            "[0/15][159/469] Loss_D: 0.4648 Loss_G: 2.7632 D(x): 0.8041 D(G(z)): 0.1632 / 0.1096\n",
            "[0/15][160/469] Loss_D: 0.4462 Loss_G: 2.4435 D(x): 0.8056 D(G(z)): 0.1604 / 0.1259\n",
            "[0/15][161/469] Loss_D: 0.5034 Loss_G: 2.8476 D(x): 0.8371 D(G(z)): 0.2265 / 0.0918\n",
            "[0/15][162/469] Loss_D: 0.5922 Loss_G: 2.0183 D(x): 0.7270 D(G(z)): 0.1591 / 0.1816\n",
            "[0/15][163/469] Loss_D: 0.5041 Loss_G: 3.6578 D(x): 0.9082 D(G(z)): 0.2876 / 0.0461\n",
            "[0/15][164/469] Loss_D: 0.5305 Loss_G: 1.7050 D(x): 0.6813 D(G(z)): 0.0744 / 0.2228\n",
            "[0/15][165/469] Loss_D: 0.6121 Loss_G: 3.4729 D(x): 0.8977 D(G(z)): 0.3601 / 0.0483\n",
            "[0/15][166/469] Loss_D: 0.6566 Loss_G: 1.7576 D(x): 0.6534 D(G(z)): 0.1048 / 0.2192\n",
            "[0/15][167/469] Loss_D: 0.5773 Loss_G: 3.6799 D(x): 0.9085 D(G(z)): 0.3327 / 0.0463\n",
            "[0/15][168/469] Loss_D: 0.6010 Loss_G: 2.0071 D(x): 0.6657 D(G(z)): 0.0805 / 0.1826\n",
            "[0/15][169/469] Loss_D: 0.5280 Loss_G: 3.1450 D(x): 0.8836 D(G(z)): 0.2894 / 0.0764\n",
            "[0/15][170/469] Loss_D: 0.4836 Loss_G: 2.9336 D(x): 0.8021 D(G(z)): 0.1604 / 0.0880\n",
            "[0/15][171/469] Loss_D: 0.6333 Loss_G: 1.4788 D(x): 0.6788 D(G(z)): 0.1344 / 0.2862\n",
            "[0/15][172/469] Loss_D: 0.6431 Loss_G: 4.5045 D(x): 0.9412 D(G(z)): 0.4001 / 0.0207\n",
            "[0/15][173/469] Loss_D: 0.8302 Loss_G: 1.9588 D(x): 0.5642 D(G(z)): 0.0461 / 0.1926\n",
            "[0/15][174/469] Loss_D: 0.4938 Loss_G: 2.9771 D(x): 0.9136 D(G(z)): 0.2900 / 0.0837\n",
            "[0/15][175/469] Loss_D: 0.3738 Loss_G: 3.4471 D(x): 0.8603 D(G(z)): 0.1591 / 0.0542\n",
            "[0/15][176/469] Loss_D: 0.4181 Loss_G: 2.8802 D(x): 0.8038 D(G(z)): 0.1185 / 0.1067\n",
            "[0/15][177/469] Loss_D: 0.5162 Loss_G: 2.7589 D(x): 0.8555 D(G(z)): 0.2275 / 0.1180\n",
            "[0/15][178/469] Loss_D: 0.5191 Loss_G: 2.8842 D(x): 0.8129 D(G(z)): 0.2146 / 0.1005\n",
            "[0/15][179/469] Loss_D: 0.5169 Loss_G: 2.7733 D(x): 0.8078 D(G(z)): 0.1924 / 0.1080\n",
            "[0/15][180/469] Loss_D: 0.4171 Loss_G: 2.9465 D(x): 0.8418 D(G(z)): 0.1792 / 0.0921\n",
            "[0/15][181/469] Loss_D: 0.4919 Loss_G: 2.3394 D(x): 0.7876 D(G(z)): 0.1660 / 0.1570\n",
            "[0/15][182/469] Loss_D: 0.5330 Loss_G: 2.9652 D(x): 0.8523 D(G(z)): 0.2570 / 0.0814\n",
            "[0/15][183/469] Loss_D: 0.5154 Loss_G: 1.8555 D(x): 0.7395 D(G(z)): 0.1352 / 0.1966\n",
            "[0/15][184/469] Loss_D: 0.6749 Loss_G: 3.9828 D(x): 0.8867 D(G(z)): 0.3614 / 0.0344\n",
            "[0/15][185/469] Loss_D: 0.8566 Loss_G: 0.7651 D(x): 0.5284 D(G(z)): 0.0587 / 0.5263\n",
            "[0/15][186/469] Loss_D: 1.1571 Loss_G: 4.5832 D(x): 0.9518 D(G(z)): 0.5836 / 0.0256\n",
            "[0/15][187/469] Loss_D: 0.6820 Loss_G: 2.7736 D(x): 0.6150 D(G(z)): 0.0422 / 0.1029\n",
            "[0/15][188/469] Loss_D: 0.4497 Loss_G: 1.7628 D(x): 0.7936 D(G(z)): 0.1635 / 0.2351\n",
            "[0/15][189/469] Loss_D: 0.6497 Loss_G: 4.1493 D(x): 0.9132 D(G(z)): 0.3686 / 0.0350\n",
            "[0/15][190/469] Loss_D: 0.6596 Loss_G: 1.8400 D(x): 0.6148 D(G(z)): 0.0605 / 0.2161\n",
            "[0/15][191/469] Loss_D: 0.5360 Loss_G: 3.0547 D(x): 0.8910 D(G(z)): 0.3017 / 0.0697\n",
            "[0/15][192/469] Loss_D: 0.3481 Loss_G: 3.2047 D(x): 0.8387 D(G(z)): 0.1209 / 0.0589\n",
            "[0/15][193/469] Loss_D: 0.5609 Loss_G: 1.4015 D(x): 0.6940 D(G(z)): 0.1108 / 0.2925\n",
            "[0/15][194/469] Loss_D: 0.6464 Loss_G: 4.1406 D(x): 0.9341 D(G(z)): 0.3989 / 0.0277\n",
            "[0/15][195/469] Loss_D: 0.7386 Loss_G: 1.5630 D(x): 0.5699 D(G(z)): 0.0506 / 0.2658\n",
            "[0/15][196/469] Loss_D: 0.5479 Loss_G: 2.7957 D(x): 0.9256 D(G(z)): 0.3354 / 0.0934\n",
            "[0/15][197/469] Loss_D: 0.4096 Loss_G: 3.4331 D(x): 0.8544 D(G(z)): 0.1815 / 0.0514\n",
            "[0/15][198/469] Loss_D: 0.5336 Loss_G: 1.8103 D(x): 0.6915 D(G(z)): 0.0885 / 0.2232\n",
            "[0/15][199/469] Loss_D: 0.5386 Loss_G: 2.6416 D(x): 0.8719 D(G(z)): 0.2877 / 0.1044\n",
            "[0/15][200/469] Loss_D: 0.4143 Loss_G: 3.1348 D(x): 0.8486 D(G(z)): 0.1828 / 0.0682\n",
            "saving the output\n",
            "[0/15][201/469] Loss_D: 0.5485 Loss_G: 1.5512 D(x): 0.7057 D(G(z)): 0.1186 / 0.2589\n",
            "[0/15][202/469] Loss_D: 0.5827 Loss_G: 3.3651 D(x): 0.9227 D(G(z)): 0.3506 / 0.0549\n",
            "[0/15][203/469] Loss_D: 0.5000 Loss_G: 2.1658 D(x): 0.7155 D(G(z)): 0.0967 / 0.1618\n",
            "[0/15][204/469] Loss_D: 0.5308 Loss_G: 1.9800 D(x): 0.8092 D(G(z)): 0.2282 / 0.1896\n",
            "[0/15][205/469] Loss_D: 0.4100 Loss_G: 3.4924 D(x): 0.9116 D(G(z)): 0.2428 / 0.0477\n",
            "[0/15][206/469] Loss_D: 0.4629 Loss_G: 2.2336 D(x): 0.7338 D(G(z)): 0.0868 / 0.1495\n",
            "[0/15][207/469] Loss_D: 0.4292 Loss_G: 2.7068 D(x): 0.8630 D(G(z)): 0.2168 / 0.0901\n",
            "[0/15][208/469] Loss_D: 0.4060 Loss_G: 2.5681 D(x): 0.8251 D(G(z)): 0.1460 / 0.1063\n",
            "[0/15][209/469] Loss_D: 0.4693 Loss_G: 2.0891 D(x): 0.7910 D(G(z)): 0.1364 / 0.1587\n",
            "[0/15][210/469] Loss_D: 0.4674 Loss_G: 3.4313 D(x): 0.8773 D(G(z)): 0.2562 / 0.0437\n",
            "[0/15][211/469] Loss_D: 0.5058 Loss_G: 1.6139 D(x): 0.6978 D(G(z)): 0.0755 / 0.2365\n",
            "[0/15][212/469] Loss_D: 0.5904 Loss_G: 2.8091 D(x): 0.8435 D(G(z)): 0.3024 / 0.0841\n",
            "[0/15][213/469] Loss_D: 0.5394 Loss_G: 1.8149 D(x): 0.7202 D(G(z)): 0.1228 / 0.2096\n",
            "[0/15][214/469] Loss_D: 0.5610 Loss_G: 3.2627 D(x): 0.9083 D(G(z)): 0.3321 / 0.0544\n",
            "[0/15][215/469] Loss_D: 0.6552 Loss_G: 1.8112 D(x): 0.6594 D(G(z)): 0.1082 / 0.2171\n",
            "[0/15][216/469] Loss_D: 0.4248 Loss_G: 2.8713 D(x): 0.8975 D(G(z)): 0.2404 / 0.0866\n",
            "[0/15][217/469] Loss_D: 0.5360 Loss_G: 2.2005 D(x): 0.7716 D(G(z)): 0.1644 / 0.1414\n",
            "[0/15][218/469] Loss_D: 0.4615 Loss_G: 3.5875 D(x): 0.8977 D(G(z)): 0.2693 / 0.0405\n",
            "[0/15][219/469] Loss_D: 0.6800 Loss_G: 1.1449 D(x): 0.6168 D(G(z)): 0.0732 / 0.3607\n",
            "[0/15][220/469] Loss_D: 0.7286 Loss_G: 4.0634 D(x): 0.9386 D(G(z)): 0.4418 / 0.0279\n",
            "[0/15][221/469] Loss_D: 0.8296 Loss_G: 1.4179 D(x): 0.5628 D(G(z)): 0.0684 / 0.3084\n",
            "[0/15][222/469] Loss_D: 0.6869 Loss_G: 3.0763 D(x): 0.8977 D(G(z)): 0.3845 / 0.0655\n",
            "[0/15][223/469] Loss_D: 0.5189 Loss_G: 2.2515 D(x): 0.7165 D(G(z)): 0.1250 / 0.1361\n",
            "[0/15][224/469] Loss_D: 0.5528 Loss_G: 2.0358 D(x): 0.7872 D(G(z)): 0.2183 / 0.1671\n",
            "[0/15][225/469] Loss_D: 0.4517 Loss_G: 3.6912 D(x): 0.9085 D(G(z)): 0.2647 / 0.0357\n",
            "[0/15][226/469] Loss_D: 0.7432 Loss_G: 1.2375 D(x): 0.5819 D(G(z)): 0.0539 / 0.3447\n",
            "[0/15][227/469] Loss_D: 0.7327 Loss_G: 3.5750 D(x): 0.9310 D(G(z)): 0.4367 / 0.0405\n",
            "[0/15][228/469] Loss_D: 0.5556 Loss_G: 2.2833 D(x): 0.6857 D(G(z)): 0.0734 / 0.1346\n",
            "[0/15][229/469] Loss_D: 0.4099 Loss_G: 2.0975 D(x): 0.8401 D(G(z)): 0.1830 / 0.1567\n",
            "[0/15][230/469] Loss_D: 0.4218 Loss_G: 3.0418 D(x): 0.8802 D(G(z)): 0.2198 / 0.0683\n",
            "[0/15][231/469] Loss_D: 0.4246 Loss_G: 2.3312 D(x): 0.7810 D(G(z)): 0.1235 / 0.1292\n",
            "[0/15][232/469] Loss_D: 0.4800 Loss_G: 2.3038 D(x): 0.8228 D(G(z)): 0.2101 / 0.1252\n",
            "[0/15][233/469] Loss_D: 0.4766 Loss_G: 2.4920 D(x): 0.8092 D(G(z)): 0.1981 / 0.1085\n",
            "[0/15][234/469] Loss_D: 0.4674 Loss_G: 2.1798 D(x): 0.7908 D(G(z)): 0.1730 / 0.1388\n",
            "[0/15][235/469] Loss_D: 0.4653 Loss_G: 3.0298 D(x): 0.8596 D(G(z)): 0.2411 / 0.0647\n",
            "[0/15][236/469] Loss_D: 0.8347 Loss_G: 0.6923 D(x): 0.5659 D(G(z)): 0.1253 / 0.5542\n",
            "[0/15][237/469] Loss_D: 1.1383 Loss_G: 5.2875 D(x): 0.9784 D(G(z)): 0.6210 / 0.0089\n",
            "[0/15][238/469] Loss_D: 1.4271 Loss_G: 1.1225 D(x): 0.3454 D(G(z)): 0.0213 / 0.3772\n",
            "[0/15][239/469] Loss_D: 0.7943 Loss_G: 3.2893 D(x): 0.9129 D(G(z)): 0.4430 / 0.0566\n",
            "[0/15][240/469] Loss_D: 0.7685 Loss_G: 1.9569 D(x): 0.6396 D(G(z)): 0.1476 / 0.1968\n",
            "[0/15][241/469] Loss_D: 0.5566 Loss_G: 3.3047 D(x): 0.8746 D(G(z)): 0.3101 / 0.0526\n",
            "[0/15][242/469] Loss_D: 0.7440 Loss_G: 1.2714 D(x): 0.6044 D(G(z)): 0.1269 / 0.3242\n",
            "[0/15][243/469] Loss_D: 0.7711 Loss_G: 4.6168 D(x): 0.9420 D(G(z)): 0.4632 / 0.0169\n",
            "[0/15][244/469] Loss_D: 1.1293 Loss_G: 0.9873 D(x): 0.4233 D(G(z)): 0.0420 / 0.4250\n",
            "[0/15][245/469] Loss_D: 0.9780 Loss_G: 4.2615 D(x): 0.9377 D(G(z)): 0.5548 / 0.0191\n",
            "[0/15][246/469] Loss_D: 0.7420 Loss_G: 2.0627 D(x): 0.5684 D(G(z)): 0.0398 / 0.1608\n",
            "[0/15][247/469] Loss_D: 0.5923 Loss_G: 2.7110 D(x): 0.8781 D(G(z)): 0.3262 / 0.0943\n",
            "[0/15][248/469] Loss_D: 0.4731 Loss_G: 2.9921 D(x): 0.8053 D(G(z)): 0.1892 / 0.0693\n",
            "[0/15][249/469] Loss_D: 0.5437 Loss_G: 1.8807 D(x): 0.7175 D(G(z)): 0.1335 / 0.2020\n",
            "[0/15][250/469] Loss_D: 0.5999 Loss_G: 2.8209 D(x): 0.8500 D(G(z)): 0.2943 / 0.0823\n",
            "[0/15][251/469] Loss_D: 0.6118 Loss_G: 2.4203 D(x): 0.7358 D(G(z)): 0.2079 / 0.1143\n",
            "[0/15][252/469] Loss_D: 0.4442 Loss_G: 2.2214 D(x): 0.7908 D(G(z)): 0.1567 / 0.1411\n",
            "[0/15][253/469] Loss_D: 0.4840 Loss_G: 2.7701 D(x): 0.8391 D(G(z)): 0.2241 / 0.0874\n",
            "[0/15][254/469] Loss_D: 0.5152 Loss_G: 1.9702 D(x): 0.7367 D(G(z)): 0.1352 / 0.1783\n",
            "[0/15][255/469] Loss_D: 0.5561 Loss_G: 3.0258 D(x): 0.8551 D(G(z)): 0.2843 / 0.0729\n",
            "[0/15][256/469] Loss_D: 0.4970 Loss_G: 2.2235 D(x): 0.7658 D(G(z)): 0.1454 / 0.1513\n",
            "[0/15][257/469] Loss_D: 0.5213 Loss_G: 2.8547 D(x): 0.8485 D(G(z)): 0.2690 / 0.0815\n",
            "[0/15][258/469] Loss_D: 0.5857 Loss_G: 1.6182 D(x): 0.6940 D(G(z)): 0.1250 / 0.2615\n",
            "[0/15][259/469] Loss_D: 0.7728 Loss_G: 3.8744 D(x): 0.8916 D(G(z)): 0.4335 / 0.0391\n",
            "[0/15][260/469] Loss_D: 1.1220 Loss_G: 0.6449 D(x): 0.4284 D(G(z)): 0.0625 / 0.5717\n",
            "[0/15][261/469] Loss_D: 1.0793 Loss_G: 3.6163 D(x): 0.9596 D(G(z)): 0.5870 / 0.0448\n",
            "[0/15][262/469] Loss_D: 0.5770 Loss_G: 2.4503 D(x): 0.6718 D(G(z)): 0.1118 / 0.1250\n",
            "[0/15][263/469] Loss_D: 0.5316 Loss_G: 2.1100 D(x): 0.7916 D(G(z)): 0.2090 / 0.1555\n",
            "[0/15][264/469] Loss_D: 0.6892 Loss_G: 2.4695 D(x): 0.7582 D(G(z)): 0.2825 / 0.1250\n",
            "[0/15][265/469] Loss_D: 0.6857 Loss_G: 2.5071 D(x): 0.7189 D(G(z)): 0.2373 / 0.1223\n",
            "[0/15][266/469] Loss_D: 0.6130 Loss_G: 2.2778 D(x): 0.7484 D(G(z)): 0.2354 / 0.1321\n",
            "[0/15][267/469] Loss_D: 0.4955 Loss_G: 2.7175 D(x): 0.8077 D(G(z)): 0.2155 / 0.0877\n",
            "[0/15][268/469] Loss_D: 0.6155 Loss_G: 2.0271 D(x): 0.7106 D(G(z)): 0.1784 / 0.1771\n",
            "[0/15][269/469] Loss_D: 0.6102 Loss_G: 2.9369 D(x): 0.8121 D(G(z)): 0.2865 / 0.0831\n",
            "[0/15][270/469] Loss_D: 0.5400 Loss_G: 1.6509 D(x): 0.7002 D(G(z)): 0.1137 / 0.2245\n",
            "[0/15][271/469] Loss_D: 0.5985 Loss_G: 3.6266 D(x): 0.8973 D(G(z)): 0.3545 / 0.0420\n",
            "[0/15][272/469] Loss_D: 0.5863 Loss_G: 1.7648 D(x): 0.6511 D(G(z)): 0.0639 / 0.2191\n",
            "[0/15][273/469] Loss_D: 0.6385 Loss_G: 3.2932 D(x): 0.9116 D(G(z)): 0.3693 / 0.0746\n",
            "[0/15][274/469] Loss_D: 0.7662 Loss_G: 1.4121 D(x): 0.5946 D(G(z)): 0.1128 / 0.3053\n",
            "[0/15][275/469] Loss_D: 0.7956 Loss_G: 3.9914 D(x): 0.9102 D(G(z)): 0.4626 / 0.0334\n",
            "[0/15][276/469] Loss_D: 1.0712 Loss_G: 0.7817 D(x): 0.4532 D(G(z)): 0.0606 / 0.5010\n",
            "[0/15][277/469] Loss_D: 1.1483 Loss_G: 4.8236 D(x): 0.9540 D(G(z)): 0.6222 / 0.0223\n",
            "[0/15][278/469] Loss_D: 1.1911 Loss_G: 1.4225 D(x): 0.4297 D(G(z)): 0.0440 / 0.2922\n",
            "[0/15][279/469] Loss_D: 0.8387 Loss_G: 2.7111 D(x): 0.8758 D(G(z)): 0.4378 / 0.1030\n",
            "[0/15][280/469] Loss_D: 0.5802 Loss_G: 2.9045 D(x): 0.7599 D(G(z)): 0.1941 / 0.0826\n",
            "[0/15][281/469] Loss_D: 0.7506 Loss_G: 1.2432 D(x): 0.6463 D(G(z)): 0.1636 / 0.3436\n",
            "[0/15][282/469] Loss_D: 0.7907 Loss_G: 3.4259 D(x): 0.8792 D(G(z)): 0.4323 / 0.0549\n",
            "[0/15][283/469] Loss_D: 0.9875 Loss_G: 1.1829 D(x): 0.5132 D(G(z)): 0.1147 / 0.3639\n",
            "[0/15][284/469] Loss_D: 0.8600 Loss_G: 3.1165 D(x): 0.8785 D(G(z)): 0.4606 / 0.0794\n",
            "[0/15][285/469] Loss_D: 0.7396 Loss_G: 1.8917 D(x): 0.6446 D(G(z)): 0.1736 / 0.2025\n",
            "[0/15][286/469] Loss_D: 0.7057 Loss_G: 1.9643 D(x): 0.7715 D(G(z)): 0.3008 / 0.1863\n",
            "[0/15][287/469] Loss_D: 0.6604 Loss_G: 2.1876 D(x): 0.7418 D(G(z)): 0.2283 / 0.1479\n",
            "[0/15][288/469] Loss_D: 0.6611 Loss_G: 2.1836 D(x): 0.7564 D(G(z)): 0.2646 / 0.1504\n",
            "[0/15][289/469] Loss_D: 0.6409 Loss_G: 2.2563 D(x): 0.7608 D(G(z)): 0.2544 / 0.1503\n",
            "[0/15][290/469] Loss_D: 0.6703 Loss_G: 1.5586 D(x): 0.6912 D(G(z)): 0.1930 / 0.2645\n",
            "[0/15][291/469] Loss_D: 0.6196 Loss_G: 3.3027 D(x): 0.8816 D(G(z)): 0.3547 / 0.0563\n",
            "[0/15][292/469] Loss_D: 0.8309 Loss_G: 0.9971 D(x): 0.5535 D(G(z)): 0.1197 / 0.4257\n",
            "[0/15][293/469] Loss_D: 0.9166 Loss_G: 3.4552 D(x): 0.9089 D(G(z)): 0.5129 / 0.0578\n",
            "[0/15][294/469] Loss_D: 1.2338 Loss_G: 0.8617 D(x): 0.4094 D(G(z)): 0.1024 / 0.4726\n",
            "[0/15][295/469] Loss_D: 1.0367 Loss_G: 3.3882 D(x): 0.9106 D(G(z)): 0.5466 / 0.0572\n",
            "[0/15][296/469] Loss_D: 1.0392 Loss_G: 0.8602 D(x): 0.4702 D(G(z)): 0.1031 / 0.4664\n",
            "[0/15][297/469] Loss_D: 1.0497 Loss_G: 3.4602 D(x): 0.9211 D(G(z)): 0.5669 / 0.0503\n",
            "[0/15][298/469] Loss_D: 0.9698 Loss_G: 1.5153 D(x): 0.5057 D(G(z)): 0.1032 / 0.2722\n",
            "[0/15][299/469] Loss_D: 0.7034 Loss_G: 2.2412 D(x): 0.8381 D(G(z)): 0.3673 / 0.1377\n",
            "[0/15][300/469] Loss_D: 0.6368 Loss_G: 2.5673 D(x): 0.7654 D(G(z)): 0.2561 / 0.1105\n",
            "saving the output\n",
            "[0/15][301/469] Loss_D: 0.8063 Loss_G: 1.1771 D(x): 0.6166 D(G(z)): 0.1873 / 0.3555\n",
            "[0/15][302/469] Loss_D: 0.8760 Loss_G: 2.6355 D(x): 0.8080 D(G(z)): 0.4285 / 0.0964\n",
            "[0/15][303/469] Loss_D: 0.7975 Loss_G: 1.4275 D(x): 0.5999 D(G(z)): 0.1399 / 0.2915\n",
            "[0/15][304/469] Loss_D: 0.7384 Loss_G: 2.9266 D(x): 0.8859 D(G(z)): 0.4215 / 0.0829\n",
            "[0/15][305/469] Loss_D: 0.8495 Loss_G: 1.0221 D(x): 0.5405 D(G(z)): 0.1032 / 0.3977\n",
            "[0/15][306/469] Loss_D: 1.0212 Loss_G: 3.3089 D(x): 0.8861 D(G(z)): 0.5364 / 0.0556\n",
            "[0/15][307/469] Loss_D: 0.7809 Loss_G: 1.5196 D(x): 0.5706 D(G(z)): 0.1009 / 0.2585\n",
            "[0/15][308/469] Loss_D: 0.8056 Loss_G: 2.2364 D(x): 0.8002 D(G(z)): 0.3927 / 0.1415\n",
            "[0/15][309/469] Loss_D: 0.7946 Loss_G: 1.4063 D(x): 0.6264 D(G(z)): 0.1921 / 0.2884\n",
            "[0/15][310/469] Loss_D: 0.6722 Loss_G: 2.1355 D(x): 0.7957 D(G(z)): 0.3174 / 0.1544\n",
            "[0/15][311/469] Loss_D: 0.5867 Loss_G: 2.0791 D(x): 0.7550 D(G(z)): 0.2314 / 0.1568\n",
            "[0/15][312/469] Loss_D: 0.6706 Loss_G: 1.8434 D(x): 0.7339 D(G(z)): 0.2550 / 0.1880\n",
            "[0/15][313/469] Loss_D: 0.5581 Loss_G: 1.8603 D(x): 0.7575 D(G(z)): 0.2175 / 0.1797\n",
            "[0/15][314/469] Loss_D: 0.8120 Loss_G: 1.5461 D(x): 0.6850 D(G(z)): 0.2916 / 0.2582\n",
            "[0/15][315/469] Loss_D: 0.8152 Loss_G: 2.4251 D(x): 0.7461 D(G(z)): 0.3428 / 0.1276\n",
            "[0/15][316/469] Loss_D: 0.6526 Loss_G: 1.4802 D(x): 0.6802 D(G(z)): 0.1795 / 0.2700\n",
            "[0/15][317/469] Loss_D: 0.6921 Loss_G: 2.1117 D(x): 0.7859 D(G(z)): 0.3241 / 0.1483\n",
            "[0/15][318/469] Loss_D: 0.6815 Loss_G: 1.5211 D(x): 0.7005 D(G(z)): 0.2203 / 0.2541\n",
            "[0/15][319/469] Loss_D: 0.7183 Loss_G: 1.5418 D(x): 0.7161 D(G(z)): 0.2600 / 0.2571\n",
            "[0/15][320/469] Loss_D: 0.6817 Loss_G: 3.1711 D(x): 0.8299 D(G(z)): 0.3446 / 0.0622\n",
            "[0/15][321/469] Loss_D: 1.0866 Loss_G: 0.4426 D(x): 0.4536 D(G(z)): 0.1308 / 0.6720\n",
            "[0/15][322/469] Loss_D: 1.2750 Loss_G: 4.5498 D(x): 0.9386 D(G(z)): 0.6437 / 0.0177\n",
            "[0/15][323/469] Loss_D: 1.4617 Loss_G: 0.7528 D(x): 0.3276 D(G(z)): 0.0404 / 0.5056\n",
            "[0/15][324/469] Loss_D: 1.0869 Loss_G: 4.1056 D(x): 0.9336 D(G(z)): 0.5858 / 0.0329\n",
            "[0/15][325/469] Loss_D: 1.5712 Loss_G: 0.3642 D(x): 0.3270 D(G(z)): 0.0729 / 0.7165\n",
            "[0/15][326/469] Loss_D: 1.5944 Loss_G: 3.7813 D(x): 0.9539 D(G(z)): 0.7332 / 0.0446\n",
            "[0/15][327/469] Loss_D: 1.4946 Loss_G: 0.7415 D(x): 0.3248 D(G(z)): 0.0725 / 0.5268\n",
            "[0/15][328/469] Loss_D: 1.1082 Loss_G: 3.0910 D(x): 0.9056 D(G(z)): 0.5677 / 0.0706\n",
            "[0/15][329/469] Loss_D: 0.8783 Loss_G: 1.2870 D(x): 0.5541 D(G(z)): 0.1333 / 0.3203\n",
            "[0/15][330/469] Loss_D: 0.7856 Loss_G: 1.9621 D(x): 0.7973 D(G(z)): 0.3758 / 0.1868\n",
            "[0/15][331/469] Loss_D: 0.7078 Loss_G: 1.7412 D(x): 0.7123 D(G(z)): 0.2590 / 0.2155\n",
            "[0/15][332/469] Loss_D: 0.7158 Loss_G: 1.7040 D(x): 0.7297 D(G(z)): 0.2779 / 0.2124\n",
            "[0/15][333/469] Loss_D: 0.6982 Loss_G: 1.7571 D(x): 0.7251 D(G(z)): 0.2650 / 0.2048\n",
            "[0/15][334/469] Loss_D: 0.6839 Loss_G: 1.9215 D(x): 0.7295 D(G(z)): 0.2501 / 0.1741\n",
            "[0/15][335/469] Loss_D: 0.6342 Loss_G: 1.9980 D(x): 0.7608 D(G(z)): 0.2550 / 0.1763\n",
            "[0/15][336/469] Loss_D: 0.7118 Loss_G: 1.9833 D(x): 0.7492 D(G(z)): 0.2860 / 0.1711\n",
            "[0/15][337/469] Loss_D: 0.8991 Loss_G: 0.8220 D(x): 0.5767 D(G(z)): 0.2085 / 0.4755\n",
            "[0/15][338/469] Loss_D: 0.8765 Loss_G: 2.9938 D(x): 0.8753 D(G(z)): 0.4889 / 0.0674\n",
            "[0/15][339/469] Loss_D: 0.7812 Loss_G: 1.2134 D(x): 0.5485 D(G(z)): 0.0819 / 0.3369\n",
            "[0/15][340/469] Loss_D: 0.8429 Loss_G: 2.1446 D(x): 0.8170 D(G(z)): 0.4273 / 0.1470\n",
            "[0/15][341/469] Loss_D: 0.5622 Loss_G: 2.4556 D(x): 0.7728 D(G(z)): 0.2229 / 0.1114\n",
            "[0/15][342/469] Loss_D: 0.8196 Loss_G: 0.8786 D(x): 0.5814 D(G(z)): 0.1612 / 0.4502\n",
            "[0/15][343/469] Loss_D: 0.9784 Loss_G: 3.2175 D(x): 0.8957 D(G(z)): 0.5425 / 0.0557\n",
            "[0/15][344/469] Loss_D: 1.0659 Loss_G: 0.8719 D(x): 0.4338 D(G(z)): 0.0809 / 0.4644\n",
            "[0/15][345/469] Loss_D: 0.9410 Loss_G: 2.5517 D(x): 0.9066 D(G(z)): 0.5329 / 0.1100\n",
            "[0/15][346/469] Loss_D: 0.6956 Loss_G: 1.9006 D(x): 0.6567 D(G(z)): 0.1673 / 0.1830\n",
            "[0/15][347/469] Loss_D: 0.7497 Loss_G: 1.4592 D(x): 0.7119 D(G(z)): 0.2803 / 0.2680\n",
            "[0/15][348/469] Loss_D: 0.8368 Loss_G: 2.4473 D(x): 0.7694 D(G(z)): 0.3824 / 0.1120\n",
            "[0/15][349/469] Loss_D: 1.1290 Loss_G: 0.6399 D(x): 0.4616 D(G(z)): 0.1866 / 0.5596\n",
            "[0/15][350/469] Loss_D: 1.1752 Loss_G: 3.4718 D(x): 0.8875 D(G(z)): 0.6104 / 0.0483\n",
            "[0/15][351/469] Loss_D: 1.0629 Loss_G: 1.2029 D(x): 0.4347 D(G(z)): 0.0687 / 0.3458\n",
            "[0/15][352/469] Loss_D: 0.9268 Loss_G: 1.7999 D(x): 0.7875 D(G(z)): 0.4421 / 0.2012\n",
            "[0/15][353/469] Loss_D: 0.7597 Loss_G: 2.1396 D(x): 0.7132 D(G(z)): 0.2975 / 0.1466\n",
            "[0/15][354/469] Loss_D: 0.8657 Loss_G: 0.7843 D(x): 0.5548 D(G(z)): 0.1619 / 0.4885\n",
            "[0/15][355/469] Loss_D: 1.0092 Loss_G: 2.9003 D(x): 0.8749 D(G(z)): 0.5422 / 0.0812\n",
            "[0/15][356/469] Loss_D: 0.7647 Loss_G: 1.4346 D(x): 0.5945 D(G(z)): 0.1459 / 0.2734\n",
            "[0/15][357/469] Loss_D: 0.6786 Loss_G: 1.5358 D(x): 0.7639 D(G(z)): 0.2972 / 0.2397\n",
            "[0/15][358/469] Loss_D: 0.7421 Loss_G: 1.7424 D(x): 0.7152 D(G(z)): 0.2769 / 0.2134\n",
            "[0/15][359/469] Loss_D: 0.6605 Loss_G: 2.5853 D(x): 0.8136 D(G(z)): 0.3325 / 0.1004\n",
            "[0/15][360/469] Loss_D: 0.7460 Loss_G: 0.9988 D(x): 0.5726 D(G(z)): 0.1227 / 0.3998\n",
            "[0/15][361/469] Loss_D: 0.8584 Loss_G: 2.3165 D(x): 0.8404 D(G(z)): 0.4475 / 0.1278\n",
            "[0/15][362/469] Loss_D: 0.6990 Loss_G: 1.4007 D(x): 0.6323 D(G(z)): 0.1525 / 0.2919\n",
            "[0/15][363/469] Loss_D: 0.7252 Loss_G: 2.2474 D(x): 0.8331 D(G(z)): 0.3848 / 0.1305\n",
            "[0/15][364/469] Loss_D: 0.6382 Loss_G: 1.7353 D(x): 0.6843 D(G(z)): 0.1782 / 0.2125\n",
            "[0/15][365/469] Loss_D: 0.6181 Loss_G: 1.7745 D(x): 0.7643 D(G(z)): 0.2565 / 0.2021\n",
            "[0/15][366/469] Loss_D: 0.6691 Loss_G: 2.1207 D(x): 0.7624 D(G(z)): 0.2842 / 0.1555\n",
            "[0/15][367/469] Loss_D: 0.6587 Loss_G: 1.3884 D(x): 0.6812 D(G(z)): 0.1723 / 0.2875\n",
            "[0/15][368/469] Loss_D: 0.7699 Loss_G: 2.4247 D(x): 0.8087 D(G(z)): 0.3814 / 0.1195\n",
            "[0/15][369/469] Loss_D: 0.6508 Loss_G: 1.3912 D(x): 0.6599 D(G(z)): 0.1591 / 0.2855\n",
            "[0/15][370/469] Loss_D: 0.7074 Loss_G: 2.0537 D(x): 0.7886 D(G(z)): 0.3366 / 0.1583\n",
            "[0/15][371/469] Loss_D: 0.6661 Loss_G: 1.7956 D(x): 0.7094 D(G(z)): 0.2296 / 0.1917\n",
            "[0/15][372/469] Loss_D: 0.6595 Loss_G: 1.5179 D(x): 0.7131 D(G(z)): 0.2327 / 0.2532\n",
            "[0/15][373/469] Loss_D: 0.7407 Loss_G: 1.8111 D(x): 0.7423 D(G(z)): 0.3149 / 0.1957\n",
            "[0/15][374/469] Loss_D: 0.5572 Loss_G: 2.3162 D(x): 0.7973 D(G(z)): 0.2522 / 0.1268\n",
            "[0/15][375/469] Loss_D: 0.8920 Loss_G: 0.7105 D(x): 0.5508 D(G(z)): 0.1742 / 0.5202\n",
            "[0/15][376/469] Loss_D: 0.7866 Loss_G: 3.6343 D(x): 0.9118 D(G(z)): 0.4622 / 0.0366\n",
            "[0/15][377/469] Loss_D: 1.2682 Loss_G: 0.3648 D(x): 0.3706 D(G(z)): 0.0898 / 0.7112\n",
            "[0/15][378/469] Loss_D: 1.4663 Loss_G: 4.7315 D(x): 0.9487 D(G(z)): 0.7134 / 0.0136\n",
            "[0/15][379/469] Loss_D: 1.6763 Loss_G: 0.4182 D(x): 0.2600 D(G(z)): 0.0387 / 0.6850\n",
            "[0/15][380/469] Loss_D: 1.3485 Loss_G: 3.6586 D(x): 0.9486 D(G(z)): 0.6744 / 0.0350\n",
            "[0/15][381/469] Loss_D: 1.4127 Loss_G: 0.4945 D(x): 0.3252 D(G(z)): 0.0945 / 0.6545\n",
            "[0/15][382/469] Loss_D: 1.4752 Loss_G: 2.9553 D(x): 0.9302 D(G(z)): 0.6989 / 0.0825\n",
            "[0/15][383/469] Loss_D: 1.0729 Loss_G: 1.3692 D(x): 0.4754 D(G(z)): 0.1655 / 0.2969\n",
            "[0/15][384/469] Loss_D: 0.6902 Loss_G: 2.0803 D(x): 0.8062 D(G(z)): 0.3374 / 0.1535\n",
            "[0/15][385/469] Loss_D: 0.6704 Loss_G: 1.7966 D(x): 0.7036 D(G(z)): 0.2215 / 0.1944\n",
            "[0/15][386/469] Loss_D: 0.7690 Loss_G: 1.5401 D(x): 0.7031 D(G(z)): 0.2997 / 0.2468\n",
            "[0/15][387/469] Loss_D: 0.7491 Loss_G: 2.2288 D(x): 0.7634 D(G(z)): 0.3450 / 0.1358\n",
            "[0/15][388/469] Loss_D: 0.8459 Loss_G: 1.0792 D(x): 0.5766 D(G(z)): 0.1814 / 0.3799\n",
            "[0/15][389/469] Loss_D: 0.8975 Loss_G: 2.0363 D(x): 0.7821 D(G(z)): 0.4302 / 0.1686\n",
            "[0/15][390/469] Loss_D: 0.6683 Loss_G: 2.0995 D(x): 0.7148 D(G(z)): 0.2453 / 0.1694\n",
            "[0/15][391/469] Loss_D: 0.7432 Loss_G: 1.2707 D(x): 0.6450 D(G(z)): 0.2044 / 0.3281\n",
            "[0/15][392/469] Loss_D: 0.8057 Loss_G: 2.9496 D(x): 0.8772 D(G(z)): 0.4485 / 0.0778\n",
            "[0/15][393/469] Loss_D: 1.1234 Loss_G: 0.6969 D(x): 0.4127 D(G(z)): 0.0955 / 0.5231\n",
            "[0/15][394/469] Loss_D: 1.0629 Loss_G: 3.7500 D(x): 0.9133 D(G(z)): 0.5894 / 0.0395\n",
            "[0/15][395/469] Loss_D: 1.2383 Loss_G: 0.5354 D(x): 0.3821 D(G(z)): 0.0973 / 0.6152\n",
            "[0/15][396/469] Loss_D: 1.1579 Loss_G: 3.0431 D(x): 0.9478 D(G(z)): 0.6315 / 0.0656\n",
            "[0/15][397/469] Loss_D: 1.1979 Loss_G: 0.7987 D(x): 0.3961 D(G(z)): 0.0884 / 0.4761\n",
            "[0/15][398/469] Loss_D: 1.0221 Loss_G: 2.7572 D(x): 0.8882 D(G(z)): 0.5580 / 0.0898\n",
            "[0/15][399/469] Loss_D: 0.7545 Loss_G: 1.7469 D(x): 0.6165 D(G(z)): 0.1623 / 0.2029\n",
            "[0/15][400/469] Loss_D: 0.7987 Loss_G: 1.3080 D(x): 0.6823 D(G(z)): 0.2941 / 0.2995\n",
            "saving the output\n",
            "[0/15][401/469] Loss_D: 0.8026 Loss_G: 3.2533 D(x): 0.8567 D(G(z)): 0.4466 / 0.0535\n",
            "[0/15][402/469] Loss_D: 1.1749 Loss_G: 1.0280 D(x): 0.4129 D(G(z)): 0.0862 / 0.3965\n",
            "[0/15][403/469] Loss_D: 0.9023 Loss_G: 2.4321 D(x): 0.8685 D(G(z)): 0.4937 / 0.1092\n",
            "[0/15][404/469] Loss_D: 0.7763 Loss_G: 1.8566 D(x): 0.6556 D(G(z)): 0.2301 / 0.1921\n",
            "[0/15][405/469] Loss_D: 0.6648 Loss_G: 1.2671 D(x): 0.6776 D(G(z)): 0.1876 / 0.3276\n",
            "[0/15][406/469] Loss_D: 0.9497 Loss_G: 2.7922 D(x): 0.8303 D(G(z)): 0.4883 / 0.0807\n",
            "[0/15][407/469] Loss_D: 0.8656 Loss_G: 1.1376 D(x): 0.5111 D(G(z)): 0.1091 / 0.3580\n",
            "[0/15][408/469] Loss_D: 0.8516 Loss_G: 1.9851 D(x): 0.8241 D(G(z)): 0.4381 / 0.1664\n",
            "[0/15][409/469] Loss_D: 0.6968 Loss_G: 2.0042 D(x): 0.7003 D(G(z)): 0.2484 / 0.1653\n",
            "[0/15][410/469] Loss_D: 0.6942 Loss_G: 1.8082 D(x): 0.7152 D(G(z)): 0.2563 / 0.1894\n",
            "[0/15][411/469] Loss_D: 0.7182 Loss_G: 1.9277 D(x): 0.7100 D(G(z)): 0.2738 / 0.1723\n",
            "[0/15][412/469] Loss_D: 0.7052 Loss_G: 1.5758 D(x): 0.6973 D(G(z)): 0.2440 / 0.2415\n",
            "[0/15][413/469] Loss_D: 0.7220 Loss_G: 2.1119 D(x): 0.7856 D(G(z)): 0.3420 / 0.1430\n",
            "[0/15][414/469] Loss_D: 0.7283 Loss_G: 1.3377 D(x): 0.6518 D(G(z)): 0.2027 / 0.2917\n",
            "[0/15][415/469] Loss_D: 0.6651 Loss_G: 2.4894 D(x): 0.8616 D(G(z)): 0.3739 / 0.1021\n",
            "[0/15][416/469] Loss_D: 0.8031 Loss_G: 1.3729 D(x): 0.6042 D(G(z)): 0.1854 / 0.2864\n",
            "[0/15][417/469] Loss_D: 0.8198 Loss_G: 2.4653 D(x): 0.8171 D(G(z)): 0.4268 / 0.1073\n",
            "[0/15][418/469] Loss_D: 0.8751 Loss_G: 1.1324 D(x): 0.5434 D(G(z)): 0.1524 / 0.3643\n",
            "[0/15][419/469] Loss_D: 0.8853 Loss_G: 2.2587 D(x): 0.8084 D(G(z)): 0.4428 / 0.1325\n",
            "[0/15][420/469] Loss_D: 0.7533 Loss_G: 1.6961 D(x): 0.6421 D(G(z)): 0.2093 / 0.2209\n",
            "[0/15][421/469] Loss_D: 0.6647 Loss_G: 2.0234 D(x): 0.7611 D(G(z)): 0.2930 / 0.1532\n",
            "[0/15][422/469] Loss_D: 0.5729 Loss_G: 1.8068 D(x): 0.7351 D(G(z)): 0.2021 / 0.2063\n",
            "[0/15][423/469] Loss_D: 0.7202 Loss_G: 1.5949 D(x): 0.7186 D(G(z)): 0.2590 / 0.2423\n",
            "[0/15][424/469] Loss_D: 0.6475 Loss_G: 1.8983 D(x): 0.7489 D(G(z)): 0.2630 / 0.1807\n",
            "[0/15][425/469] Loss_D: 0.6389 Loss_G: 2.5411 D(x): 0.7857 D(G(z)): 0.2942 / 0.0975\n",
            "[0/15][426/469] Loss_D: 0.8718 Loss_G: 0.7872 D(x): 0.5442 D(G(z)): 0.1542 / 0.4987\n",
            "[0/15][427/469] Loss_D: 1.0866 Loss_G: 3.7614 D(x): 0.9274 D(G(z)): 0.5910 / 0.0357\n",
            "[0/15][428/469] Loss_D: 1.3139 Loss_G: 0.7854 D(x): 0.3515 D(G(z)): 0.0753 / 0.4988\n",
            "[0/15][429/469] Loss_D: 0.8999 Loss_G: 2.8059 D(x): 0.9166 D(G(z)): 0.5237 / 0.0767\n",
            "[0/15][430/469] Loss_D: 0.8049 Loss_G: 1.4318 D(x): 0.5683 D(G(z)): 0.1465 / 0.2723\n",
            "[0/15][431/469] Loss_D: 0.7594 Loss_G: 2.4634 D(x): 0.8425 D(G(z)): 0.4005 / 0.1139\n",
            "[0/15][432/469] Loss_D: 0.7399 Loss_G: 1.5888 D(x): 0.6403 D(G(z)): 0.1905 / 0.2361\n",
            "[0/15][433/469] Loss_D: 0.8438 Loss_G: 1.8255 D(x): 0.7199 D(G(z)): 0.3498 / 0.1990\n",
            "[0/15][434/469] Loss_D: 0.6964 Loss_G: 1.7823 D(x): 0.7074 D(G(z)): 0.2472 / 0.1983\n",
            "[0/15][435/469] Loss_D: 0.7946 Loss_G: 1.8690 D(x): 0.7103 D(G(z)): 0.3166 / 0.1809\n",
            "[0/15][436/469] Loss_D: 0.7101 Loss_G: 1.4716 D(x): 0.6781 D(G(z)): 0.2316 / 0.2601\n",
            "[0/15][437/469] Loss_D: 0.7465 Loss_G: 2.4656 D(x): 0.7974 D(G(z)): 0.3719 / 0.1098\n",
            "[0/15][438/469] Loss_D: 0.6941 Loss_G: 1.2885 D(x): 0.6338 D(G(z)): 0.1541 / 0.3055\n",
            "[0/15][439/469] Loss_D: 0.8508 Loss_G: 2.2723 D(x): 0.7848 D(G(z)): 0.4085 / 0.1261\n",
            "[0/15][440/469] Loss_D: 0.6814 Loss_G: 1.7990 D(x): 0.6927 D(G(z)): 0.2262 / 0.1892\n",
            "[0/15][441/469] Loss_D: 0.6842 Loss_G: 1.1552 D(x): 0.6696 D(G(z)): 0.1869 / 0.3547\n",
            "[0/15][442/469] Loss_D: 0.8910 Loss_G: 3.4812 D(x): 0.9029 D(G(z)): 0.5111 / 0.0430\n",
            "[0/15][443/469] Loss_D: 1.2267 Loss_G: 0.6165 D(x): 0.3761 D(G(z)): 0.0771 / 0.5752\n",
            "[0/15][444/469] Loss_D: 1.3264 Loss_G: 3.7806 D(x): 0.9208 D(G(z)): 0.6685 / 0.0310\n",
            "[0/15][445/469] Loss_D: 1.4201 Loss_G: 0.5083 D(x): 0.3259 D(G(z)): 0.0663 / 0.6301\n",
            "[0/15][446/469] Loss_D: 1.3547 Loss_G: 4.1462 D(x): 0.9324 D(G(z)): 0.6704 / 0.0232\n",
            "[0/15][447/469] Loss_D: 1.7719 Loss_G: 0.5055 D(x): 0.2570 D(G(z)): 0.0483 / 0.6249\n",
            "[0/15][448/469] Loss_D: 1.4222 Loss_G: 3.1805 D(x): 0.9410 D(G(z)): 0.7051 / 0.0735\n",
            "[0/15][449/469] Loss_D: 1.1120 Loss_G: 1.1619 D(x): 0.4456 D(G(z)): 0.1280 / 0.3716\n",
            "[0/15][450/469] Loss_D: 0.9416 Loss_G: 1.9281 D(x): 0.7921 D(G(z)): 0.4512 / 0.1776\n",
            "[0/15][451/469] Loss_D: 0.8574 Loss_G: 1.7142 D(x): 0.6385 D(G(z)): 0.2540 / 0.2190\n",
            "[0/15][452/469] Loss_D: 0.8865 Loss_G: 1.9395 D(x): 0.7085 D(G(z)): 0.3623 / 0.1740\n",
            "[0/15][453/469] Loss_D: 0.8508 Loss_G: 1.3052 D(x): 0.6218 D(G(z)): 0.2426 / 0.3081\n",
            "[0/15][454/469] Loss_D: 0.8191 Loss_G: 1.7189 D(x): 0.7284 D(G(z)): 0.3466 / 0.2135\n",
            "[0/15][455/469] Loss_D: 0.8213 Loss_G: 1.9925 D(x): 0.7106 D(G(z)): 0.3246 / 0.1683\n",
            "[0/15][456/469] Loss_D: 0.7413 Loss_G: 1.4699 D(x): 0.6694 D(G(z)): 0.2377 / 0.2597\n",
            "[0/15][457/469] Loss_D: 0.9140 Loss_G: 1.6344 D(x): 0.6881 D(G(z)): 0.3537 / 0.2270\n",
            "[0/15][458/469] Loss_D: 0.8526 Loss_G: 1.7230 D(x): 0.6822 D(G(z)): 0.3027 / 0.2087\n",
            "[0/15][459/469] Loss_D: 0.8334 Loss_G: 1.3722 D(x): 0.6510 D(G(z)): 0.2725 / 0.2847\n",
            "[0/15][460/469] Loss_D: 0.7138 Loss_G: 2.1656 D(x): 0.7850 D(G(z)): 0.3247 / 0.1476\n",
            "[0/15][461/469] Loss_D: 0.8650 Loss_G: 1.1260 D(x): 0.5941 D(G(z)): 0.2082 / 0.3603\n",
            "[0/15][462/469] Loss_D: 0.8793 Loss_G: 2.7844 D(x): 0.8384 D(G(z)): 0.4617 / 0.0806\n",
            "[0/15][463/469] Loss_D: 0.8968 Loss_G: 1.0861 D(x): 0.5297 D(G(z)): 0.1259 / 0.3903\n",
            "[0/15][464/469] Loss_D: 0.8325 Loss_G: 2.1868 D(x): 0.8219 D(G(z)): 0.4301 / 0.1386\n",
            "[0/15][465/469] Loss_D: 0.6960 Loss_G: 1.7544 D(x): 0.6661 D(G(z)): 0.2114 / 0.2018\n",
            "[0/15][466/469] Loss_D: 0.5881 Loss_G: 1.5275 D(x): 0.7379 D(G(z)): 0.2098 / 0.2523\n",
            "[0/15][467/469] Loss_D: 0.7155 Loss_G: 2.2035 D(x): 0.7968 D(G(z)): 0.3473 / 0.1299\n",
            "[0/15][468/469] Loss_D: 0.7123 Loss_G: 1.2797 D(x): 0.6386 D(G(z)): 0.1764 / 0.3169\n",
            "[1/15][0/469] Loss_D: 0.8392 Loss_G: 2.0112 D(x): 0.7808 D(G(z)): 0.3977 / 0.1660\n",
            "saving the output\n",
            "[1/15][1/469] Loss_D: 0.8643 Loss_G: 1.2941 D(x): 0.6183 D(G(z)): 0.2535 / 0.3127\n",
            "[1/15][2/469] Loss_D: 0.8757 Loss_G: 2.2474 D(x): 0.7768 D(G(z)): 0.4237 / 0.1341\n",
            "[1/15][3/469] Loss_D: 1.0352 Loss_G: 0.7323 D(x): 0.4903 D(G(z)): 0.1661 / 0.5088\n",
            "[1/15][4/469] Loss_D: 1.1054 Loss_G: 3.2430 D(x): 0.8936 D(G(z)): 0.5909 / 0.0564\n",
            "[1/15][5/469] Loss_D: 1.0083 Loss_G: 1.0406 D(x): 0.4566 D(G(z)): 0.1007 / 0.4037\n",
            "[1/15][6/469] Loss_D: 0.8058 Loss_G: 2.0430 D(x): 0.8366 D(G(z)): 0.4251 / 0.1611\n",
            "[1/15][7/469] Loss_D: 0.7224 Loss_G: 2.2102 D(x): 0.7275 D(G(z)): 0.2731 / 0.1442\n",
            "[1/15][8/469] Loss_D: 0.8084 Loss_G: 1.1215 D(x): 0.6210 D(G(z)): 0.2149 / 0.3644\n",
            "[1/15][9/469] Loss_D: 0.8687 Loss_G: 2.3370 D(x): 0.8131 D(G(z)): 0.4391 / 0.1179\n",
            "[1/15][10/469] Loss_D: 0.9573 Loss_G: 0.9862 D(x): 0.5255 D(G(z)): 0.1737 / 0.4109\n",
            "[1/15][11/469] Loss_D: 0.8630 Loss_G: 2.7456 D(x): 0.8748 D(G(z)): 0.4862 / 0.0840\n",
            "[1/15][12/469] Loss_D: 0.8671 Loss_G: 1.1706 D(x): 0.5169 D(G(z)): 0.1031 / 0.3458\n",
            "[1/15][13/469] Loss_D: 0.8217 Loss_G: 2.0986 D(x): 0.8357 D(G(z)): 0.4414 / 0.1491\n",
            "[1/15][14/469] Loss_D: 0.7228 Loss_G: 1.9200 D(x): 0.6798 D(G(z)): 0.2362 / 0.1867\n",
            "[1/15][15/469] Loss_D: 0.6065 Loss_G: 1.5672 D(x): 0.7278 D(G(z)): 0.2126 / 0.2468\n",
            "[1/15][16/469] Loss_D: 0.7567 Loss_G: 2.0541 D(x): 0.7802 D(G(z)): 0.3555 / 0.1542\n",
            "[1/15][17/469] Loss_D: 0.7662 Loss_G: 1.2734 D(x): 0.6381 D(G(z)): 0.2035 / 0.3114\n",
            "[1/15][18/469] Loss_D: 0.8364 Loss_G: 2.2322 D(x): 0.7810 D(G(z)): 0.4031 / 0.1303\n",
            "[1/15][19/469] Loss_D: 0.8424 Loss_G: 0.9251 D(x): 0.5680 D(G(z)): 0.1730 / 0.4183\n",
            "[1/15][20/469] Loss_D: 0.9972 Loss_G: 2.7133 D(x): 0.8246 D(G(z)): 0.5122 / 0.0839\n",
            "[1/15][21/469] Loss_D: 1.1327 Loss_G: 0.7395 D(x): 0.4391 D(G(z)): 0.1325 / 0.5112\n",
            "[1/15][22/469] Loss_D: 1.0719 Loss_G: 2.7233 D(x): 0.8557 D(G(z)): 0.5527 / 0.0851\n",
            "[1/15][23/469] Loss_D: 0.9194 Loss_G: 1.0468 D(x): 0.5162 D(G(z)): 0.1346 / 0.3787\n",
            "[1/15][24/469] Loss_D: 0.7705 Loss_G: 2.2738 D(x): 0.8346 D(G(z)): 0.4137 / 0.1293\n",
            "[1/15][25/469] Loss_D: 0.7322 Loss_G: 1.8085 D(x): 0.6588 D(G(z)): 0.2145 / 0.2041\n",
            "[1/15][26/469] Loss_D: 0.7281 Loss_G: 1.4921 D(x): 0.7182 D(G(z)): 0.2791 / 0.2564\n",
            "[1/15][27/469] Loss_D: 0.7547 Loss_G: 1.7113 D(x): 0.7270 D(G(z)): 0.3072 / 0.2157\n",
            "[1/15][28/469] Loss_D: 0.6244 Loss_G: 2.4228 D(x): 0.7825 D(G(z)): 0.2793 / 0.1092\n",
            "[1/15][29/469] Loss_D: 0.9959 Loss_G: 0.5563 D(x): 0.5117 D(G(z)): 0.1692 / 0.5943\n",
            "[1/15][30/469] Loss_D: 1.3046 Loss_G: 3.4852 D(x): 0.8788 D(G(z)): 0.6492 / 0.0418\n",
            "[1/15][31/469] Loss_D: 1.5398 Loss_G: 0.5459 D(x): 0.2880 D(G(z)): 0.0570 / 0.6123\n",
            "[1/15][32/469] Loss_D: 1.3065 Loss_G: 3.3157 D(x): 0.9448 D(G(z)): 0.6680 / 0.0502\n",
            "[1/15][33/469] Loss_D: 1.1902 Loss_G: 0.7062 D(x): 0.3877 D(G(z)): 0.0930 / 0.5280\n",
            "[1/15][34/469] Loss_D: 1.0577 Loss_G: 2.7850 D(x): 0.8910 D(G(z)): 0.5644 / 0.0841\n",
            "[1/15][35/469] Loss_D: 0.8356 Loss_G: 1.4795 D(x): 0.5712 D(G(z)): 0.1611 / 0.2623\n",
            "[1/15][36/469] Loss_D: 0.7189 Loss_G: 1.4566 D(x): 0.7363 D(G(z)): 0.2910 / 0.2706\n",
            "[1/15][37/469] Loss_D: 0.8241 Loss_G: 2.2399 D(x): 0.7803 D(G(z)): 0.3906 / 0.1383\n",
            "[1/15][38/469] Loss_D: 0.7869 Loss_G: 1.0804 D(x): 0.5722 D(G(z)): 0.1470 / 0.3741\n",
            "[1/15][39/469] Loss_D: 0.9452 Loss_G: 2.6054 D(x): 0.8674 D(G(z)): 0.5202 / 0.0955\n",
            "[1/15][40/469] Loss_D: 1.1487 Loss_G: 0.8419 D(x): 0.4241 D(G(z)): 0.1295 / 0.4638\n",
            "[1/15][41/469] Loss_D: 0.9841 Loss_G: 2.2559 D(x): 0.8449 D(G(z)): 0.5163 / 0.1299\n",
            "[1/15][42/469] Loss_D: 0.7695 Loss_G: 1.6593 D(x): 0.6266 D(G(z)): 0.2010 / 0.2319\n",
            "[1/15][43/469] Loss_D: 0.7624 Loss_G: 1.6349 D(x): 0.7347 D(G(z)): 0.3144 / 0.2332\n",
            "[1/15][44/469] Loss_D: 0.7992 Loss_G: 1.7038 D(x): 0.7019 D(G(z)): 0.3045 / 0.2221\n",
            "[1/15][45/469] Loss_D: 0.7833 Loss_G: 1.8086 D(x): 0.7276 D(G(z)): 0.3234 / 0.1906\n",
            "[1/15][46/469] Loss_D: 0.8021 Loss_G: 1.3952 D(x): 0.6556 D(G(z)): 0.2733 / 0.2828\n",
            "[1/15][47/469] Loss_D: 0.7759 Loss_G: 1.8103 D(x): 0.7250 D(G(z)): 0.3297 / 0.1975\n",
            "[1/15][48/469] Loss_D: 0.6847 Loss_G: 1.8067 D(x): 0.7226 D(G(z)): 0.2604 / 0.1984\n",
            "[1/15][49/469] Loss_D: 0.7537 Loss_G: 1.5212 D(x): 0.6860 D(G(z)): 0.2565 / 0.2566\n",
            "[1/15][50/469] Loss_D: 0.6546 Loss_G: 1.9585 D(x): 0.7610 D(G(z)): 0.2849 / 0.1692\n",
            "[1/15][51/469] Loss_D: 0.8301 Loss_G: 1.5632 D(x): 0.6747 D(G(z)): 0.2886 / 0.2414\n",
            "[1/15][52/469] Loss_D: 0.8340 Loss_G: 1.7082 D(x): 0.6865 D(G(z)): 0.3022 / 0.2279\n",
            "[1/15][53/469] Loss_D: 0.6687 Loss_G: 2.1401 D(x): 0.7631 D(G(z)): 0.2931 / 0.1418\n",
            "[1/15][54/469] Loss_D: 0.6404 Loss_G: 1.3215 D(x): 0.6767 D(G(z)): 0.1686 / 0.2987\n",
            "[1/15][55/469] Loss_D: 0.8320 Loss_G: 1.9397 D(x): 0.7474 D(G(z)): 0.3704 / 0.1702\n",
            "[1/15][56/469] Loss_D: 0.7925 Loss_G: 1.5827 D(x): 0.6678 D(G(z)): 0.2532 / 0.2409\n",
            "[1/15][57/469] Loss_D: 0.6963 Loss_G: 1.8473 D(x): 0.7411 D(G(z)): 0.2907 / 0.1915\n",
            "[1/15][58/469] Loss_D: 0.6766 Loss_G: 1.6747 D(x): 0.7245 D(G(z)): 0.2524 / 0.2108\n",
            "[1/15][59/469] Loss_D: 0.6440 Loss_G: 2.1193 D(x): 0.7661 D(G(z)): 0.2814 / 0.1355\n",
            "[1/15][60/469] Loss_D: 0.8439 Loss_G: 0.8177 D(x): 0.5993 D(G(z)): 0.2082 / 0.4705\n",
            "[1/15][61/469] Loss_D: 1.0893 Loss_G: 2.9453 D(x): 0.8224 D(G(z)): 0.5280 / 0.0714\n",
            "[1/15][62/469] Loss_D: 1.1769 Loss_G: 0.5317 D(x): 0.3997 D(G(z)): 0.0946 / 0.6244\n",
            "[1/15][63/469] Loss_D: 1.5251 Loss_G: 3.4224 D(x): 0.9336 D(G(z)): 0.7014 / 0.0464\n",
            "[1/15][64/469] Loss_D: 1.1929 Loss_G: 0.8835 D(x): 0.3769 D(G(z)): 0.0563 / 0.4487\n",
            "[1/15][65/469] Loss_D: 1.0543 Loss_G: 2.6900 D(x): 0.8577 D(G(z)): 0.5482 / 0.0949\n",
            "[1/15][66/469] Loss_D: 0.9901 Loss_G: 1.0477 D(x): 0.5008 D(G(z)): 0.1597 / 0.3935\n",
            "[1/15][67/469] Loss_D: 1.0109 Loss_G: 2.7789 D(x): 0.8499 D(G(z)): 0.5228 / 0.0841\n",
            "[1/15][68/469] Loss_D: 1.1706 Loss_G: 0.8266 D(x): 0.4231 D(G(z)): 0.1353 / 0.4786\n",
            "[1/15][69/469] Loss_D: 0.9475 Loss_G: 2.5257 D(x): 0.8788 D(G(z)): 0.5185 / 0.1030\n",
            "[1/15][70/469] Loss_D: 0.8092 Loss_G: 1.4217 D(x): 0.5841 D(G(z)): 0.1622 / 0.2697\n",
            "[1/15][71/469] Loss_D: 0.7708 Loss_G: 1.7094 D(x): 0.7487 D(G(z)): 0.3402 / 0.2126\n",
            "[1/15][72/469] Loss_D: 0.7580 Loss_G: 2.1989 D(x): 0.7421 D(G(z)): 0.3180 / 0.1345\n",
            "[1/15][73/469] Loss_D: 0.7439 Loss_G: 1.4161 D(x): 0.6436 D(G(z)): 0.2081 / 0.2791\n",
            "[1/15][74/469] Loss_D: 0.8753 Loss_G: 2.3644 D(x): 0.7638 D(G(z)): 0.4136 / 0.1198\n",
            "[1/15][75/469] Loss_D: 0.7046 Loss_G: 1.4406 D(x): 0.6284 D(G(z)): 0.1700 / 0.2772\n",
            "[1/15][76/469] Loss_D: 0.7305 Loss_G: 2.3864 D(x): 0.8096 D(G(z)): 0.3661 / 0.1293\n",
            "[1/15][77/469] Loss_D: 0.6596 Loss_G: 2.0113 D(x): 0.7165 D(G(z)): 0.2341 / 0.1640\n",
            "[1/15][78/469] Loss_D: 0.6685 Loss_G: 1.2198 D(x): 0.6709 D(G(z)): 0.1967 / 0.3326\n",
            "[1/15][79/469] Loss_D: 0.8184 Loss_G: 2.9432 D(x): 0.8621 D(G(z)): 0.4581 / 0.0653\n",
            "[1/15][80/469] Loss_D: 0.7262 Loss_G: 1.3206 D(x): 0.5815 D(G(z)): 0.1086 / 0.2996\n",
            "[1/15][81/469] Loss_D: 0.6818 Loss_G: 2.0978 D(x): 0.8383 D(G(z)): 0.3668 / 0.1446\n",
            "[1/15][82/469] Loss_D: 0.7438 Loss_G: 1.6662 D(x): 0.6679 D(G(z)): 0.2441 / 0.2162\n",
            "[1/15][83/469] Loss_D: 0.7345 Loss_G: 1.7078 D(x): 0.7120 D(G(z)): 0.2889 / 0.2046\n",
            "[1/15][84/469] Loss_D: 0.8492 Loss_G: 2.1754 D(x): 0.7061 D(G(z)): 0.3432 / 0.1451\n",
            "[1/15][85/469] Loss_D: 0.8306 Loss_G: 0.9586 D(x): 0.5976 D(G(z)): 0.2027 / 0.4196\n",
            "[1/15][86/469] Loss_D: 0.8832 Loss_G: 3.3452 D(x): 0.8788 D(G(z)): 0.5024 / 0.0464\n",
            "[1/15][87/469] Loss_D: 1.0424 Loss_G: 0.7382 D(x): 0.4295 D(G(z)): 0.0798 / 0.5234\n",
            "[1/15][88/469] Loss_D: 1.2124 Loss_G: 3.2986 D(x): 0.9203 D(G(z)): 0.6350 / 0.0521\n",
            "[1/15][89/469] Loss_D: 1.0989 Loss_G: 1.1179 D(x): 0.4369 D(G(z)): 0.0874 / 0.3628\n",
            "[1/15][90/469] Loss_D: 0.9153 Loss_G: 2.4292 D(x): 0.8062 D(G(z)): 0.4669 / 0.1118\n",
            "[1/15][91/469] Loss_D: 0.8690 Loss_G: 1.2146 D(x): 0.5902 D(G(z)): 0.1982 / 0.3409\n",
            "[1/15][92/469] Loss_D: 0.8224 Loss_G: 2.4277 D(x): 0.8048 D(G(z)): 0.4116 / 0.1119\n",
            "[1/15][93/469] Loss_D: 0.9186 Loss_G: 1.2193 D(x): 0.5661 D(G(z)): 0.1932 / 0.3410\n",
            "[1/15][94/469] Loss_D: 0.8420 Loss_G: 2.4599 D(x): 0.8213 D(G(z)): 0.4389 / 0.1182\n",
            "[1/15][95/469] Loss_D: 0.8297 Loss_G: 1.3250 D(x): 0.5727 D(G(z)): 0.1601 / 0.3089\n",
            "[1/15][96/469] Loss_D: 0.7995 Loss_G: 1.9958 D(x): 0.7954 D(G(z)): 0.3895 / 0.1689\n",
            "[1/15][97/469] Loss_D: 0.8122 Loss_G: 1.7557 D(x): 0.6704 D(G(z)): 0.2720 / 0.2031\n",
            "[1/15][98/469] Loss_D: 0.7566 Loss_G: 1.1545 D(x): 0.6524 D(G(z)): 0.2302 / 0.3463\n",
            "[1/15][99/469] Loss_D: 0.9147 Loss_G: 2.4563 D(x): 0.7879 D(G(z)): 0.4374 / 0.1076\n",
            "[1/15][100/469] Loss_D: 0.7147 Loss_G: 1.3211 D(x): 0.6120 D(G(z)): 0.1518 / 0.3040\n",
            "saving the output\n",
            "[1/15][101/469] Loss_D: 0.8831 Loss_G: 2.0277 D(x): 0.7558 D(G(z)): 0.4165 / 0.1578\n",
            "[1/15][102/469] Loss_D: 0.7453 Loss_G: 1.4461 D(x): 0.6428 D(G(z)): 0.2063 / 0.2687\n",
            "[1/15][103/469] Loss_D: 0.7713 Loss_G: 2.1527 D(x): 0.7722 D(G(z)): 0.3628 / 0.1490\n",
            "[1/15][104/469] Loss_D: 0.7749 Loss_G: 1.0393 D(x): 0.6125 D(G(z)): 0.1849 / 0.3906\n",
            "[1/15][105/469] Loss_D: 0.9503 Loss_G: 2.6682 D(x): 0.8185 D(G(z)): 0.4835 / 0.0890\n",
            "[1/15][106/469] Loss_D: 0.9261 Loss_G: 0.8287 D(x): 0.4983 D(G(z)): 0.1332 / 0.4862\n",
            "[1/15][107/469] Loss_D: 0.9504 Loss_G: 3.0054 D(x): 0.8835 D(G(z)): 0.5120 / 0.0692\n",
            "[1/15][108/469] Loss_D: 1.0194 Loss_G: 0.9437 D(x): 0.4707 D(G(z)): 0.1234 / 0.4319\n",
            "[1/15][109/469] Loss_D: 0.9321 Loss_G: 2.8083 D(x): 0.8712 D(G(z)): 0.4892 / 0.0831\n",
            "[1/15][110/469] Loss_D: 0.7529 Loss_G: 1.2312 D(x): 0.5949 D(G(z)): 0.1226 / 0.3257\n",
            "[1/15][111/469] Loss_D: 0.7195 Loss_G: 2.7327 D(x): 0.8770 D(G(z)): 0.4062 / 0.0913\n",
            "[1/15][112/469] Loss_D: 0.7463 Loss_G: 1.2865 D(x): 0.6055 D(G(z)): 0.1413 / 0.3293\n",
            "[1/15][113/469] Loss_D: 0.7683 Loss_G: 1.9691 D(x): 0.7930 D(G(z)): 0.3729 / 0.1722\n",
            "[1/15][114/469] Loss_D: 0.7780 Loss_G: 1.7651 D(x): 0.6825 D(G(z)): 0.2830 / 0.1946\n",
            "[1/15][115/469] Loss_D: 0.7755 Loss_G: 1.7434 D(x): 0.6962 D(G(z)): 0.2985 / 0.2066\n",
            "[1/15][116/469] Loss_D: 0.7823 Loss_G: 1.1905 D(x): 0.6580 D(G(z)): 0.2371 / 0.3363\n",
            "[1/15][117/469] Loss_D: 0.8306 Loss_G: 2.6631 D(x): 0.7945 D(G(z)): 0.4178 / 0.0884\n",
            "[1/15][118/469] Loss_D: 0.8178 Loss_G: 0.9187 D(x): 0.5589 D(G(z)): 0.1293 / 0.4392\n",
            "[1/15][119/469] Loss_D: 0.9617 Loss_G: 2.8425 D(x): 0.8312 D(G(z)): 0.5092 / 0.0757\n",
            "[1/15][120/469] Loss_D: 1.0900 Loss_G: 0.7512 D(x): 0.4749 D(G(z)): 0.1500 / 0.5029\n",
            "[1/15][121/469] Loss_D: 0.9487 Loss_G: 2.9817 D(x): 0.8631 D(G(z)): 0.5072 / 0.0623\n",
            "[1/15][122/469] Loss_D: 0.8243 Loss_G: 1.2390 D(x): 0.5547 D(G(z)): 0.1333 / 0.3246\n",
            "[1/15][123/469] Loss_D: 1.0311 Loss_G: 2.6353 D(x): 0.7603 D(G(z)): 0.4855 / 0.0984\n",
            "[1/15][124/469] Loss_D: 0.9929 Loss_G: 0.7663 D(x): 0.5036 D(G(z)): 0.1334 / 0.4970\n",
            "[1/15][125/469] Loss_D: 1.1984 Loss_G: 3.6916 D(x): 0.8859 D(G(z)): 0.6131 / 0.0352\n",
            "[1/15][126/469] Loss_D: 1.2535 Loss_G: 0.8789 D(x): 0.3822 D(G(z)): 0.0677 / 0.4502\n",
            "[1/15][127/469] Loss_D: 0.9146 Loss_G: 2.9419 D(x): 0.9144 D(G(z)): 0.5364 / 0.0647\n",
            "[1/15][128/469] Loss_D: 0.7716 Loss_G: 1.4648 D(x): 0.5792 D(G(z)): 0.1299 / 0.2676\n",
            "[1/15][129/469] Loss_D: 0.7183 Loss_G: 1.7851 D(x): 0.7691 D(G(z)): 0.3292 / 0.1986\n",
            "[1/15][130/469] Loss_D: 0.6196 Loss_G: 2.3840 D(x): 0.7836 D(G(z)): 0.2743 / 0.1180\n",
            "[1/15][131/469] Loss_D: 0.8808 Loss_G: 0.9386 D(x): 0.5828 D(G(z)): 0.2222 / 0.4233\n",
            "[1/15][132/469] Loss_D: 0.8536 Loss_G: 2.6251 D(x): 0.8322 D(G(z)): 0.4470 / 0.0890\n",
            "[1/15][133/469] Loss_D: 1.0307 Loss_G: 0.8963 D(x): 0.5019 D(G(z)): 0.1881 / 0.4547\n",
            "[1/15][134/469] Loss_D: 0.9474 Loss_G: 2.8863 D(x): 0.8713 D(G(z)): 0.4993 / 0.0752\n",
            "[1/15][135/469] Loss_D: 0.7508 Loss_G: 1.4460 D(x): 0.5709 D(G(z)): 0.1155 / 0.2783\n",
            "[1/15][136/469] Loss_D: 0.8311 Loss_G: 1.9188 D(x): 0.7657 D(G(z)): 0.3773 / 0.1837\n",
            "[1/15][137/469] Loss_D: 0.6472 Loss_G: 1.8708 D(x): 0.7251 D(G(z)): 0.2381 / 0.1810\n",
            "[1/15][138/469] Loss_D: 0.7070 Loss_G: 1.9371 D(x): 0.7402 D(G(z)): 0.2947 / 0.1760\n",
            "[1/15][139/469] Loss_D: 0.7020 Loss_G: 1.9628 D(x): 0.7351 D(G(z)): 0.2883 / 0.1701\n",
            "[1/15][140/469] Loss_D: 0.7375 Loss_G: 1.6531 D(x): 0.6990 D(G(z)): 0.2725 / 0.2199\n",
            "[1/15][141/469] Loss_D: 0.7529 Loss_G: 1.9700 D(x): 0.7353 D(G(z)): 0.3223 / 0.1655\n",
            "[1/15][142/469] Loss_D: 0.7215 Loss_G: 1.3509 D(x): 0.6535 D(G(z)): 0.2044 / 0.2934\n",
            "[1/15][143/469] Loss_D: 0.7480 Loss_G: 2.7126 D(x): 0.8025 D(G(z)): 0.3703 / 0.0868\n",
            "[1/15][144/469] Loss_D: 0.7889 Loss_G: 1.0061 D(x): 0.5892 D(G(z)): 0.1553 / 0.3986\n",
            "[1/15][145/469] Loss_D: 0.8878 Loss_G: 3.8334 D(x): 0.8770 D(G(z)): 0.4956 / 0.0376\n",
            "[1/15][146/469] Loss_D: 1.4068 Loss_G: 0.4055 D(x): 0.3376 D(G(z)): 0.0657 / 0.7007\n",
            "[1/15][147/469] Loss_D: 1.6142 Loss_G: 4.0524 D(x): 0.9515 D(G(z)): 0.7323 / 0.0265\n",
            "[1/15][148/469] Loss_D: 1.4048 Loss_G: 0.6451 D(x): 0.3213 D(G(z)): 0.0394 / 0.5503\n",
            "[1/15][149/469] Loss_D: 1.2365 Loss_G: 3.3846 D(x): 0.9182 D(G(z)): 0.6421 / 0.0456\n",
            "[1/15][150/469] Loss_D: 1.1170 Loss_G: 0.9084 D(x): 0.4202 D(G(z)): 0.0891 / 0.4530\n",
            "[1/15][151/469] Loss_D: 1.0749 Loss_G: 3.0324 D(x): 0.8885 D(G(z)): 0.5626 / 0.0678\n",
            "[1/15][152/469] Loss_D: 1.0298 Loss_G: 0.9157 D(x): 0.4658 D(G(z)): 0.1118 / 0.4370\n",
            "[1/15][153/469] Loss_D: 1.0079 Loss_G: 2.8844 D(x): 0.9021 D(G(z)): 0.5394 / 0.0789\n",
            "[1/15][154/469] Loss_D: 0.9572 Loss_G: 1.4927 D(x): 0.5283 D(G(z)): 0.1544 / 0.2882\n",
            "[1/15][155/469] Loss_D: 0.9417 Loss_G: 1.9708 D(x): 0.7387 D(G(z)): 0.4023 / 0.1896\n",
            "[1/15][156/469] Loss_D: 0.7960 Loss_G: 1.9893 D(x): 0.6826 D(G(z)): 0.2801 / 0.1673\n",
            "[1/15][157/469] Loss_D: 0.8451 Loss_G: 1.5386 D(x): 0.6617 D(G(z)): 0.2901 / 0.2588\n",
            "[1/15][158/469] Loss_D: 0.7632 Loss_G: 1.8289 D(x): 0.7236 D(G(z)): 0.3077 / 0.1893\n",
            "[1/15][159/469] Loss_D: 0.7975 Loss_G: 1.5257 D(x): 0.6724 D(G(z)): 0.2853 / 0.2548\n",
            "[1/15][160/469] Loss_D: 0.7358 Loss_G: 2.5160 D(x): 0.7915 D(G(z)): 0.3570 / 0.1010\n",
            "[1/15][161/469] Loss_D: 1.0201 Loss_G: 0.7312 D(x): 0.4916 D(G(z)): 0.1613 / 0.5369\n",
            "[1/15][162/469] Loss_D: 1.1406 Loss_G: 3.1254 D(x): 0.8969 D(G(z)): 0.5968 / 0.0599\n",
            "[1/15][163/469] Loss_D: 0.9563 Loss_G: 1.3738 D(x): 0.4869 D(G(z)): 0.1180 / 0.3085\n",
            "[1/15][164/469] Loss_D: 0.8343 Loss_G: 2.4799 D(x): 0.8256 D(G(z)): 0.4174 / 0.1065\n",
            "[1/15][165/469] Loss_D: 0.8159 Loss_G: 1.2888 D(x): 0.5864 D(G(z)): 0.1749 / 0.3316\n",
            "[1/15][166/469] Loss_D: 0.8869 Loss_G: 2.6453 D(x): 0.8256 D(G(z)): 0.4572 / 0.0985\n",
            "[1/15][167/469] Loss_D: 0.8830 Loss_G: 0.9692 D(x): 0.5353 D(G(z)): 0.1503 / 0.4109\n",
            "[1/15][168/469] Loss_D: 0.9207 Loss_G: 3.1834 D(x): 0.8869 D(G(z)): 0.5193 / 0.0543\n",
            "[1/15][169/469] Loss_D: 0.8059 Loss_G: 1.4181 D(x): 0.5422 D(G(z)): 0.1030 / 0.2836\n",
            "[1/15][170/469] Loss_D: 0.8044 Loss_G: 1.8164 D(x): 0.7770 D(G(z)): 0.3827 / 0.1903\n",
            "[1/15][171/469] Loss_D: 0.7500 Loss_G: 1.9743 D(x): 0.7054 D(G(z)): 0.2825 / 0.1602\n",
            "[1/15][172/469] Loss_D: 0.8147 Loss_G: 1.2298 D(x): 0.6231 D(G(z)): 0.2429 / 0.3155\n",
            "[1/15][173/469] Loss_D: 0.7773 Loss_G: 1.8290 D(x): 0.7348 D(G(z)): 0.3341 / 0.1896\n",
            "[1/15][174/469] Loss_D: 0.8745 Loss_G: 1.3465 D(x): 0.6356 D(G(z)): 0.2925 / 0.3019\n",
            "[1/15][175/469] Loss_D: 0.8308 Loss_G: 2.4598 D(x): 0.7642 D(G(z)): 0.3863 / 0.1108\n",
            "[1/15][176/469] Loss_D: 0.9852 Loss_G: 0.5666 D(x): 0.4922 D(G(z)): 0.1599 / 0.5941\n",
            "[1/15][177/469] Loss_D: 1.2269 Loss_G: 3.7384 D(x): 0.9251 D(G(z)): 0.6473 / 0.0365\n",
            "[1/15][178/469] Loss_D: 1.4840 Loss_G: 0.7740 D(x): 0.3139 D(G(z)): 0.0599 / 0.5177\n",
            "[1/15][179/469] Loss_D: 1.0926 Loss_G: 2.6577 D(x): 0.8935 D(G(z)): 0.5704 / 0.0865\n",
            "[1/15][180/469] Loss_D: 0.8027 Loss_G: 1.4884 D(x): 0.5945 D(G(z)): 0.1756 / 0.2592\n",
            "[1/15][181/469] Loss_D: 0.7751 Loss_G: 1.6467 D(x): 0.7278 D(G(z)): 0.3209 / 0.2263\n",
            "[1/15][182/469] Loss_D: 0.7719 Loss_G: 1.9020 D(x): 0.7130 D(G(z)): 0.3086 / 0.1786\n",
            "[1/15][183/469] Loss_D: 0.6577 Loss_G: 1.5226 D(x): 0.6940 D(G(z)): 0.2115 / 0.2512\n",
            "[1/15][184/469] Loss_D: 0.7550 Loss_G: 2.3498 D(x): 0.7891 D(G(z)): 0.3668 / 0.1151\n",
            "[1/15][185/469] Loss_D: 0.8080 Loss_G: 1.1010 D(x): 0.5936 D(G(z)): 0.1803 / 0.3633\n",
            "[1/15][186/469] Loss_D: 0.7077 Loss_G: 2.2359 D(x): 0.8389 D(G(z)): 0.3876 / 0.1224\n",
            "[1/15][187/469] Loss_D: 0.7491 Loss_G: 1.6332 D(x): 0.6602 D(G(z)): 0.2319 / 0.2246\n",
            "[1/15][188/469] Loss_D: 0.6972 Loss_G: 1.5573 D(x): 0.7223 D(G(z)): 0.2688 / 0.2441\n",
            "[1/15][189/469] Loss_D: 0.8369 Loss_G: 1.7006 D(x): 0.6959 D(G(z)): 0.3339 / 0.2133\n",
            "[1/15][190/469] Loss_D: 0.7771 Loss_G: 1.7652 D(x): 0.7025 D(G(z)): 0.3079 / 0.2072\n",
            "[1/15][191/469] Loss_D: 0.7315 Loss_G: 1.6347 D(x): 0.7054 D(G(z)): 0.2765 / 0.2320\n",
            "[1/15][192/469] Loss_D: 0.7491 Loss_G: 1.6015 D(x): 0.7034 D(G(z)): 0.2850 / 0.2370\n",
            "[1/15][193/469] Loss_D: 0.6870 Loss_G: 2.5685 D(x): 0.7898 D(G(z)): 0.3297 / 0.0937\n",
            "[1/15][194/469] Loss_D: 0.9211 Loss_G: 0.5171 D(x): 0.5150 D(G(z)): 0.1327 / 0.6254\n",
            "[1/15][195/469] Loss_D: 1.4211 Loss_G: 3.9356 D(x): 0.9072 D(G(z)): 0.6858 / 0.0275\n",
            "[1/15][196/469] Loss_D: 1.7366 Loss_G: 0.5328 D(x): 0.2402 D(G(z)): 0.0594 / 0.6172\n",
            "[1/15][197/469] Loss_D: 1.0122 Loss_G: 4.0744 D(x): 0.9169 D(G(z)): 0.5602 / 0.0266\n",
            "[1/15][198/469] Loss_D: 1.4816 Loss_G: 0.2589 D(x): 0.2996 D(G(z)): 0.0614 / 0.7943\n",
            "[1/15][199/469] Loss_D: 1.8947 Loss_G: 3.6172 D(x): 0.9637 D(G(z)): 0.8048 / 0.0503\n",
            "[1/15][200/469] Loss_D: 1.1220 Loss_G: 1.6042 D(x): 0.4533 D(G(z)): 0.1416 / 0.2380\n",
            "saving the output\n",
            "[1/15][201/469] Loss_D: 0.9129 Loss_G: 1.4874 D(x): 0.6894 D(G(z)): 0.3560 / 0.2627\n",
            "[1/15][202/469] Loss_D: 1.0955 Loss_G: 2.6890 D(x): 0.7199 D(G(z)): 0.4858 / 0.0882\n",
            "[1/15][203/469] Loss_D: 1.0566 Loss_G: 1.1418 D(x): 0.4750 D(G(z)): 0.1534 / 0.3662\n",
            "[1/15][204/469] Loss_D: 0.9252 Loss_G: 2.9700 D(x): 0.8501 D(G(z)): 0.4927 / 0.0641\n",
            "[1/15][205/469] Loss_D: 0.7979 Loss_G: 1.8000 D(x): 0.6036 D(G(z)): 0.1692 / 0.2037\n",
            "[1/15][206/469] Loss_D: 0.7211 Loss_G: 1.5147 D(x): 0.7226 D(G(z)): 0.2846 / 0.2520\n",
            "[1/15][207/469] Loss_D: 0.8061 Loss_G: 2.6880 D(x): 0.7976 D(G(z)): 0.3987 / 0.0886\n",
            "[1/15][208/469] Loss_D: 1.0107 Loss_G: 0.7090 D(x): 0.4847 D(G(z)): 0.1396 / 0.5360\n",
            "[1/15][209/469] Loss_D: 1.3945 Loss_G: 3.4820 D(x): 0.9143 D(G(z)): 0.6775 / 0.0508\n",
            "[1/15][210/469] Loss_D: 1.3613 Loss_G: 1.0525 D(x): 0.3653 D(G(z)): 0.0971 / 0.3999\n",
            "[1/15][211/469] Loss_D: 1.0143 Loss_G: 2.1766 D(x): 0.8300 D(G(z)): 0.5082 / 0.1455\n",
            "[1/15][212/469] Loss_D: 0.7863 Loss_G: 2.1379 D(x): 0.6595 D(G(z)): 0.2370 / 0.1557\n",
            "[1/15][213/469] Loss_D: 0.7973 Loss_G: 1.6279 D(x): 0.6650 D(G(z)): 0.2691 / 0.2321\n",
            "[1/15][214/469] Loss_D: 0.8198 Loss_G: 2.0510 D(x): 0.7259 D(G(z)): 0.3584 / 0.1494\n",
            "[1/15][215/469] Loss_D: 0.8214 Loss_G: 1.6611 D(x): 0.6480 D(G(z)): 0.2689 / 0.2204\n",
            "[1/15][216/469] Loss_D: 0.7456 Loss_G: 2.5567 D(x): 0.7866 D(G(z)): 0.3640 / 0.0952\n",
            "[1/15][217/469] Loss_D: 0.8745 Loss_G: 0.9991 D(x): 0.5560 D(G(z)): 0.1792 / 0.4014\n",
            "[1/15][218/469] Loss_D: 1.0215 Loss_G: 3.2144 D(x): 0.8637 D(G(z)): 0.5385 / 0.0505\n",
            "[1/15][219/469] Loss_D: 1.3304 Loss_G: 0.5698 D(x): 0.3533 D(G(z)): 0.1258 / 0.5860\n",
            "[1/15][220/469] Loss_D: 1.1917 Loss_G: 2.7815 D(x): 0.8791 D(G(z)): 0.6087 / 0.0791\n",
            "[1/15][221/469] Loss_D: 0.9537 Loss_G: 1.4075 D(x): 0.5165 D(G(z)): 0.1629 / 0.2754\n",
            "[1/15][222/469] Loss_D: 0.7500 Loss_G: 2.0827 D(x): 0.7985 D(G(z)): 0.3762 / 0.1487\n",
            "[1/15][223/469] Loss_D: 0.8312 Loss_G: 2.0808 D(x): 0.6737 D(G(z)): 0.2911 / 0.1546\n",
            "[1/15][224/469] Loss_D: 0.7809 Loss_G: 1.4554 D(x): 0.6510 D(G(z)): 0.2351 / 0.2612\n",
            "[1/15][225/469] Loss_D: 0.8884 Loss_G: 1.9769 D(x): 0.7176 D(G(z)): 0.3784 / 0.1728\n",
            "[1/15][226/469] Loss_D: 0.7081 Loss_G: 1.6930 D(x): 0.6885 D(G(z)): 0.2457 / 0.2195\n",
            "[1/15][227/469] Loss_D: 0.7661 Loss_G: 1.6020 D(x): 0.6962 D(G(z)): 0.2778 / 0.2328\n",
            "[1/15][228/469] Loss_D: 0.7088 Loss_G: 2.4088 D(x): 0.7743 D(G(z)): 0.3281 / 0.1162\n",
            "[1/15][229/469] Loss_D: 0.7504 Loss_G: 0.9710 D(x): 0.5952 D(G(z)): 0.1502 / 0.4116\n",
            "[1/15][230/469] Loss_D: 1.2255 Loss_G: 3.5186 D(x): 0.8891 D(G(z)): 0.6280 / 0.0436\n",
            "[1/15][231/469] Loss_D: 1.2684 Loss_G: 0.8352 D(x): 0.3768 D(G(z)): 0.0731 / 0.4676\n",
            "[1/15][232/469] Loss_D: 0.9176 Loss_G: 2.2587 D(x): 0.8691 D(G(z)): 0.5043 / 0.1228\n",
            "[1/15][233/469] Loss_D: 0.8951 Loss_G: 1.6392 D(x): 0.5899 D(G(z)): 0.2285 / 0.2216\n",
            "[1/15][234/469] Loss_D: 0.7250 Loss_G: 1.5873 D(x): 0.7170 D(G(z)): 0.2844 / 0.2376\n",
            "[1/15][235/469] Loss_D: 0.7637 Loss_G: 2.8962 D(x): 0.8081 D(G(z)): 0.3923 / 0.0736\n",
            "[1/15][236/469] Loss_D: 0.9521 Loss_G: 0.8065 D(x): 0.4856 D(G(z)): 0.1069 / 0.4817\n",
            "[1/15][237/469] Loss_D: 1.0240 Loss_G: 2.3361 D(x): 0.8604 D(G(z)): 0.5412 / 0.1217\n",
            "[1/15][238/469] Loss_D: 0.8124 Loss_G: 1.7632 D(x): 0.6299 D(G(z)): 0.2233 / 0.1993\n",
            "[1/15][239/469] Loss_D: 0.7173 Loss_G: 1.5743 D(x): 0.7106 D(G(z)): 0.2749 / 0.2351\n",
            "[1/15][240/469] Loss_D: 0.7143 Loss_G: 1.9406 D(x): 0.7365 D(G(z)): 0.2952 / 0.1772\n",
            "[1/15][241/469] Loss_D: 0.8550 Loss_G: 1.5398 D(x): 0.6463 D(G(z)): 0.2821 / 0.2515\n",
            "[1/15][242/469] Loss_D: 0.8420 Loss_G: 1.5966 D(x): 0.6800 D(G(z)): 0.3130 / 0.2320\n",
            "[1/15][243/469] Loss_D: 0.9321 Loss_G: 2.4235 D(x): 0.7313 D(G(z)): 0.4173 / 0.1120\n",
            "[1/15][244/469] Loss_D: 0.9948 Loss_G: 0.7736 D(x): 0.5017 D(G(z)): 0.1446 / 0.5085\n",
            "[1/15][245/469] Loss_D: 1.1390 Loss_G: 3.2051 D(x): 0.9030 D(G(z)): 0.5943 / 0.0545\n",
            "[1/15][246/469] Loss_D: 1.3272 Loss_G: 0.7390 D(x): 0.3603 D(G(z)): 0.0968 / 0.5200\n",
            "[1/15][247/469] Loss_D: 1.1260 Loss_G: 2.8330 D(x): 0.8851 D(G(z)): 0.5837 / 0.0782\n",
            "[1/15][248/469] Loss_D: 0.9957 Loss_G: 1.1194 D(x): 0.4959 D(G(z)): 0.1552 / 0.3695\n",
            "[1/15][249/469] Loss_D: 0.8465 Loss_G: 1.4501 D(x): 0.7194 D(G(z)): 0.3460 / 0.2710\n",
            "[1/15][250/469] Loss_D: 0.8871 Loss_G: 2.9167 D(x): 0.8115 D(G(z)): 0.4503 / 0.0745\n",
            "[1/15][251/469] Loss_D: 1.2402 Loss_G: 0.6578 D(x): 0.3888 D(G(z)): 0.1133 / 0.5489\n",
            "[1/15][252/469] Loss_D: 1.1869 Loss_G: 2.7932 D(x): 0.9035 D(G(z)): 0.6133 / 0.0829\n",
            "[1/15][253/469] Loss_D: 0.9071 Loss_G: 1.3966 D(x): 0.5265 D(G(z)): 0.1489 / 0.2911\n",
            "[1/15][254/469] Loss_D: 0.7043 Loss_G: 1.4588 D(x): 0.7368 D(G(z)): 0.2958 / 0.2628\n",
            "[1/15][255/469] Loss_D: 0.7466 Loss_G: 2.4293 D(x): 0.7992 D(G(z)): 0.3766 / 0.1077\n",
            "[1/15][256/469] Loss_D: 0.9012 Loss_G: 0.9387 D(x): 0.5418 D(G(z)): 0.1803 / 0.4269\n",
            "[1/15][257/469] Loss_D: 0.9192 Loss_G: 2.2984 D(x): 0.8196 D(G(z)): 0.4749 / 0.1183\n",
            "[1/15][258/469] Loss_D: 0.7798 Loss_G: 1.6769 D(x): 0.6286 D(G(z)): 0.2155 / 0.2199\n",
            "[1/15][259/469] Loss_D: 0.7635 Loss_G: 1.2654 D(x): 0.6776 D(G(z)): 0.2475 / 0.3226\n",
            "[1/15][260/469] Loss_D: 0.8573 Loss_G: 2.4298 D(x): 0.7786 D(G(z)): 0.4034 / 0.1130\n",
            "[1/15][261/469] Loss_D: 0.8004 Loss_G: 1.2974 D(x): 0.6028 D(G(z)): 0.1774 / 0.3068\n",
            "[1/15][262/469] Loss_D: 0.8532 Loss_G: 1.7663 D(x): 0.7393 D(G(z)): 0.3788 / 0.1983\n",
            "[1/15][263/469] Loss_D: 0.8021 Loss_G: 1.5699 D(x): 0.6443 D(G(z)): 0.2676 / 0.2434\n",
            "[1/15][264/469] Loss_D: 0.8715 Loss_G: 1.7777 D(x): 0.6999 D(G(z)): 0.3476 / 0.2047\n",
            "[1/15][265/469] Loss_D: 0.7735 Loss_G: 1.7724 D(x): 0.6985 D(G(z)): 0.2960 / 0.1985\n",
            "[1/15][266/469] Loss_D: 0.8052 Loss_G: 1.0191 D(x): 0.6197 D(G(z)): 0.2306 / 0.3912\n",
            "[1/15][267/469] Loss_D: 0.9785 Loss_G: 2.7797 D(x): 0.8377 D(G(z)): 0.5131 / 0.0835\n",
            "[1/15][268/469] Loss_D: 0.9885 Loss_G: 1.0195 D(x): 0.4798 D(G(z)): 0.1289 / 0.3938\n",
            "[1/15][269/469] Loss_D: 0.7627 Loss_G: 2.3418 D(x): 0.8679 D(G(z)): 0.4300 / 0.1220\n",
            "[1/15][270/469] Loss_D: 0.6331 Loss_G: 2.0264 D(x): 0.7078 D(G(z)): 0.2159 / 0.1662\n",
            "[1/15][271/469] Loss_D: 0.6907 Loss_G: 1.5434 D(x): 0.7135 D(G(z)): 0.2480 / 0.2487\n",
            "[1/15][272/469] Loss_D: 0.6077 Loss_G: 2.5568 D(x): 0.8467 D(G(z)): 0.3307 / 0.0987\n",
            "[1/15][273/469] Loss_D: 0.9173 Loss_G: 0.8409 D(x): 0.5254 D(G(z)): 0.1559 / 0.4701\n",
            "[1/15][274/469] Loss_D: 0.9033 Loss_G: 2.7339 D(x): 0.8548 D(G(z)): 0.4898 / 0.0790\n",
            "[1/15][275/469] Loss_D: 0.7525 Loss_G: 1.4346 D(x): 0.5988 D(G(z)): 0.1478 / 0.2727\n",
            "[1/15][276/469] Loss_D: 0.9520 Loss_G: 1.5628 D(x): 0.6838 D(G(z)): 0.3744 / 0.2573\n",
            "[1/15][277/469] Loss_D: 0.7703 Loss_G: 2.2027 D(x): 0.7307 D(G(z)): 0.3309 / 0.1410\n",
            "[1/15][278/469] Loss_D: 0.8308 Loss_G: 1.0585 D(x): 0.5879 D(G(z)): 0.1923 / 0.3913\n",
            "[1/15][279/469] Loss_D: 0.9848 Loss_G: 2.9639 D(x): 0.8516 D(G(z)): 0.5209 / 0.0671\n",
            "[1/15][280/469] Loss_D: 1.0118 Loss_G: 0.7956 D(x): 0.4664 D(G(z)): 0.1049 / 0.4906\n",
            "[1/15][281/469] Loss_D: 1.0280 Loss_G: 2.7544 D(x): 0.8878 D(G(z)): 0.5562 / 0.0844\n",
            "[1/15][282/469] Loss_D: 1.0077 Loss_G: 1.0440 D(x): 0.4829 D(G(z)): 0.1498 / 0.3974\n",
            "[1/15][283/469] Loss_D: 0.8552 Loss_G: 2.7138 D(x): 0.8403 D(G(z)): 0.4508 / 0.0838\n",
            "[1/15][284/469] Loss_D: 1.0392 Loss_G: 0.6394 D(x): 0.4622 D(G(z)): 0.1398 / 0.5641\n",
            "[1/15][285/469] Loss_D: 1.3983 Loss_G: 3.5599 D(x): 0.9011 D(G(z)): 0.6788 / 0.0461\n",
            "[1/15][286/469] Loss_D: 1.4712 Loss_G: 0.7654 D(x): 0.3090 D(G(z)): 0.0716 / 0.5277\n",
            "[1/15][287/469] Loss_D: 1.1150 Loss_G: 2.6158 D(x): 0.8825 D(G(z)): 0.5696 / 0.0975\n",
            "[1/15][288/469] Loss_D: 0.7392 Loss_G: 1.8789 D(x): 0.6326 D(G(z)): 0.1876 / 0.1928\n",
            "[1/15][289/469] Loss_D: 0.7792 Loss_G: 1.1791 D(x): 0.6620 D(G(z)): 0.2534 / 0.3421\n",
            "[1/15][290/469] Loss_D: 0.9503 Loss_G: 2.5448 D(x): 0.7937 D(G(z)): 0.4700 / 0.0999\n",
            "[1/15][291/469] Loss_D: 0.9737 Loss_G: 0.8922 D(x): 0.5012 D(G(z)): 0.1525 / 0.4555\n",
            "[1/15][292/469] Loss_D: 0.9577 Loss_G: 2.8584 D(x): 0.8724 D(G(z)): 0.5231 / 0.0772\n",
            "[1/15][293/469] Loss_D: 1.0352 Loss_G: 0.9330 D(x): 0.4631 D(G(z)): 0.1239 / 0.4379\n",
            "[1/15][294/469] Loss_D: 0.9778 Loss_G: 2.7719 D(x): 0.8685 D(G(z)): 0.5211 / 0.0764\n",
            "[1/15][295/469] Loss_D: 1.1064 Loss_G: 0.8992 D(x): 0.4456 D(G(z)): 0.1337 / 0.4613\n",
            "[1/15][296/469] Loss_D: 1.1752 Loss_G: 2.7917 D(x): 0.8478 D(G(z)): 0.5720 / 0.0846\n",
            "[1/15][297/469] Loss_D: 1.1075 Loss_G: 1.0404 D(x): 0.4515 D(G(z)): 0.1169 / 0.4026\n",
            "[1/15][298/469] Loss_D: 0.9538 Loss_G: 2.0875 D(x): 0.8331 D(G(z)): 0.4833 / 0.1590\n",
            "[1/15][299/469] Loss_D: 0.7914 Loss_G: 1.6455 D(x): 0.6248 D(G(z)): 0.2219 / 0.2279\n",
            "[1/15][300/469] Loss_D: 0.7367 Loss_G: 1.6311 D(x): 0.7204 D(G(z)): 0.2970 / 0.2298\n",
            "saving the output\n",
            "[1/15][301/469] Loss_D: 0.7740 Loss_G: 1.7587 D(x): 0.7108 D(G(z)): 0.3090 / 0.2014\n",
            "[1/15][302/469] Loss_D: 0.7449 Loss_G: 1.6120 D(x): 0.6916 D(G(z)): 0.2726 / 0.2432\n",
            "[1/15][303/469] Loss_D: 0.9627 Loss_G: 1.5569 D(x): 0.6589 D(G(z)): 0.3500 / 0.2359\n",
            "[1/15][304/469] Loss_D: 0.8171 Loss_G: 1.9267 D(x): 0.7253 D(G(z)): 0.3419 / 0.1735\n",
            "[1/15][305/469] Loss_D: 0.6842 Loss_G: 1.4826 D(x): 0.6773 D(G(z)): 0.2166 / 0.2542\n",
            "[1/15][306/469] Loss_D: 0.8036 Loss_G: 2.1270 D(x): 0.7623 D(G(z)): 0.3755 / 0.1368\n",
            "[1/15][307/469] Loss_D: 0.7162 Loss_G: 1.5171 D(x): 0.6617 D(G(z)): 0.2044 / 0.2426\n",
            "[1/15][308/469] Loss_D: 0.8397 Loss_G: 2.4313 D(x): 0.7778 D(G(z)): 0.4075 / 0.1059\n",
            "[1/15][309/469] Loss_D: 0.8493 Loss_G: 0.8676 D(x): 0.5436 D(G(z)): 0.1534 / 0.4508\n",
            "[1/15][310/469] Loss_D: 0.9681 Loss_G: 2.8967 D(x): 0.8509 D(G(z)): 0.5093 / 0.0765\n",
            "[1/15][311/469] Loss_D: 1.0764 Loss_G: 0.7801 D(x): 0.4515 D(G(z)): 0.1258 / 0.4949\n",
            "[1/15][312/469] Loss_D: 1.1300 Loss_G: 2.7041 D(x): 0.8610 D(G(z)): 0.5780 / 0.0840\n",
            "[1/15][313/469] Loss_D: 0.8948 Loss_G: 1.1818 D(x): 0.5118 D(G(z)): 0.1242 / 0.3536\n",
            "[1/15][314/469] Loss_D: 0.7757 Loss_G: 2.2932 D(x): 0.8489 D(G(z)): 0.4195 / 0.1213\n",
            "[1/15][315/469] Loss_D: 0.8231 Loss_G: 1.3100 D(x): 0.5794 D(G(z)): 0.1722 / 0.3057\n",
            "[1/15][316/469] Loss_D: 0.7397 Loss_G: 2.1013 D(x): 0.8090 D(G(z)): 0.3790 / 0.1451\n",
            "[1/15][317/469] Loss_D: 0.7214 Loss_G: 1.7993 D(x): 0.6917 D(G(z)): 0.2539 / 0.1899\n",
            "[1/15][318/469] Loss_D: 0.8751 Loss_G: 1.6954 D(x): 0.6934 D(G(z)): 0.3498 / 0.2163\n",
            "[1/15][319/469] Loss_D: 0.8202 Loss_G: 1.4244 D(x): 0.6501 D(G(z)): 0.2718 / 0.2680\n",
            "[1/15][320/469] Loss_D: 0.7090 Loss_G: 1.5363 D(x): 0.7156 D(G(z)): 0.2742 / 0.2473\n",
            "[1/15][321/469] Loss_D: 0.7348 Loss_G: 2.3285 D(x): 0.7713 D(G(z)): 0.3491 / 0.1156\n",
            "[1/15][322/469] Loss_D: 0.8028 Loss_G: 0.8860 D(x): 0.5683 D(G(z)): 0.1399 / 0.4574\n",
            "[1/15][323/469] Loss_D: 1.1618 Loss_G: 3.2867 D(x): 0.9089 D(G(z)): 0.5932 / 0.0498\n",
            "[1/15][324/469] Loss_D: 1.2295 Loss_G: 0.9568 D(x): 0.3828 D(G(z)): 0.0974 / 0.4399\n",
            "[1/15][325/469] Loss_D: 1.1209 Loss_G: 2.8123 D(x): 0.8226 D(G(z)): 0.5396 / 0.0761\n",
            "[1/15][326/469] Loss_D: 0.8447 Loss_G: 1.1417 D(x): 0.5424 D(G(z)): 0.1292 / 0.3689\n",
            "[1/15][327/469] Loss_D: 0.7236 Loss_G: 2.6327 D(x): 0.8781 D(G(z)): 0.4166 / 0.0862\n",
            "[1/15][328/469] Loss_D: 0.7355 Loss_G: 1.5573 D(x): 0.6228 D(G(z)): 0.1825 / 0.2467\n",
            "[1/15][329/469] Loss_D: 0.6120 Loss_G: 1.8424 D(x): 0.7818 D(G(z)): 0.2733 / 0.1907\n",
            "[1/15][330/469] Loss_D: 0.7492 Loss_G: 1.6619 D(x): 0.7041 D(G(z)): 0.2850 / 0.2191\n",
            "[1/15][331/469] Loss_D: 0.7124 Loss_G: 1.9813 D(x): 0.7470 D(G(z)): 0.3124 / 0.1650\n",
            "[1/15][332/469] Loss_D: 0.8338 Loss_G: 1.1047 D(x): 0.6157 D(G(z)): 0.2375 / 0.3677\n",
            "[1/15][333/469] Loss_D: 0.9301 Loss_G: 2.4326 D(x): 0.7935 D(G(z)): 0.4687 / 0.1025\n",
            "[1/15][334/469] Loss_D: 1.0028 Loss_G: 0.8355 D(x): 0.4847 D(G(z)): 0.1602 / 0.4673\n",
            "[1/15][335/469] Loss_D: 1.0648 Loss_G: 3.0975 D(x): 0.8554 D(G(z)): 0.5597 / 0.0640\n",
            "[1/15][336/469] Loss_D: 1.1136 Loss_G: 0.8993 D(x): 0.4345 D(G(z)): 0.1142 / 0.4583\n",
            "[1/15][337/469] Loss_D: 0.9685 Loss_G: 2.5676 D(x): 0.8470 D(G(z)): 0.5001 / 0.0986\n",
            "[1/15][338/469] Loss_D: 0.7662 Loss_G: 1.6003 D(x): 0.6235 D(G(z)): 0.2126 / 0.2512\n",
            "[1/15][339/469] Loss_D: 0.8413 Loss_G: 1.6128 D(x): 0.6905 D(G(z)): 0.3190 / 0.2311\n",
            "[1/15][340/469] Loss_D: 0.8193 Loss_G: 2.3723 D(x): 0.7283 D(G(z)): 0.3538 / 0.1194\n",
            "[1/15][341/469] Loss_D: 0.7704 Loss_G: 1.2664 D(x): 0.6183 D(G(z)): 0.1754 / 0.3339\n",
            "[1/15][342/469] Loss_D: 0.8955 Loss_G: 3.0150 D(x): 0.8259 D(G(z)): 0.4737 / 0.0633\n",
            "[1/15][343/469] Loss_D: 0.9981 Loss_G: 1.1360 D(x): 0.5305 D(G(z)): 0.1719 / 0.3575\n",
            "[1/15][344/469] Loss_D: 0.8730 Loss_G: 3.0006 D(x): 0.8458 D(G(z)): 0.4620 / 0.0696\n",
            "[1/15][345/469] Loss_D: 1.1144 Loss_G: 1.0122 D(x): 0.4857 D(G(z)): 0.1529 / 0.4290\n",
            "[1/15][346/469] Loss_D: 1.0956 Loss_G: 2.8230 D(x): 0.8290 D(G(z)): 0.5346 / 0.0833\n",
            "[1/15][347/469] Loss_D: 1.1299 Loss_G: 0.8581 D(x): 0.4507 D(G(z)): 0.1660 / 0.5112\n",
            "[1/15][348/469] Loss_D: 1.1950 Loss_G: 3.2138 D(x): 0.8593 D(G(z)): 0.5831 / 0.0567\n",
            "[1/15][349/469] Loss_D: 1.1159 Loss_G: 0.9049 D(x): 0.4422 D(G(z)): 0.1338 / 0.4392\n",
            "[1/15][350/469] Loss_D: 0.8480 Loss_G: 2.9055 D(x): 0.8910 D(G(z)): 0.4783 / 0.0684\n",
            "[1/15][351/469] Loss_D: 0.7796 Loss_G: 1.4614 D(x): 0.5912 D(G(z)): 0.1623 / 0.2666\n",
            "[1/15][352/469] Loss_D: 0.8438 Loss_G: 2.1836 D(x): 0.7612 D(G(z)): 0.3961 / 0.1365\n",
            "[1/15][353/469] Loss_D: 0.9392 Loss_G: 1.1867 D(x): 0.5837 D(G(z)): 0.2675 / 0.3355\n",
            "[1/15][354/469] Loss_D: 0.9938 Loss_G: 3.0866 D(x): 0.7984 D(G(z)): 0.5012 / 0.0599\n",
            "[1/15][355/469] Loss_D: 1.1741 Loss_G: 0.6021 D(x): 0.4020 D(G(z)): 0.1001 / 0.5750\n",
            "[1/15][356/469] Loss_D: 1.2515 Loss_G: 3.5449 D(x): 0.8975 D(G(z)): 0.6424 / 0.0377\n",
            "[1/15][357/469] Loss_D: 1.1079 Loss_G: 1.0200 D(x): 0.4249 D(G(z)): 0.1015 / 0.4101\n",
            "[1/15][358/469] Loss_D: 0.8866 Loss_G: 3.1472 D(x): 0.8850 D(G(z)): 0.4920 / 0.0552\n",
            "[1/15][359/469] Loss_D: 0.8358 Loss_G: 1.4506 D(x): 0.5656 D(G(z)): 0.1548 / 0.2799\n",
            "[1/15][360/469] Loss_D: 0.8513 Loss_G: 2.5558 D(x): 0.7962 D(G(z)): 0.4251 / 0.1072\n",
            "[1/15][361/469] Loss_D: 0.9242 Loss_G: 1.5033 D(x): 0.5981 D(G(z)): 0.2358 / 0.2785\n",
            "[1/15][362/469] Loss_D: 0.8613 Loss_G: 2.1727 D(x): 0.7338 D(G(z)): 0.3655 / 0.1445\n",
            "[1/15][363/469] Loss_D: 0.6967 Loss_G: 1.6511 D(x): 0.6731 D(G(z)): 0.2086 / 0.2241\n",
            "[1/15][364/469] Loss_D: 0.7747 Loss_G: 1.9683 D(x): 0.7299 D(G(z)): 0.3275 / 0.1670\n",
            "[1/15][365/469] Loss_D: 0.7831 Loss_G: 1.9850 D(x): 0.7084 D(G(z)): 0.3118 / 0.1601\n",
            "[1/15][366/469] Loss_D: 0.7937 Loss_G: 1.1776 D(x): 0.6320 D(G(z)): 0.2215 / 0.3366\n",
            "[1/15][367/469] Loss_D: 0.9353 Loss_G: 2.8121 D(x): 0.8033 D(G(z)): 0.4731 / 0.0737\n",
            "[1/15][368/469] Loss_D: 1.1096 Loss_G: 0.6656 D(x): 0.4396 D(G(z)): 0.1370 / 0.5412\n",
            "[1/15][369/469] Loss_D: 1.2408 Loss_G: 3.5208 D(x): 0.8952 D(G(z)): 0.6512 / 0.0373\n",
            "[1/15][370/469] Loss_D: 1.3625 Loss_G: 0.7866 D(x): 0.3359 D(G(z)): 0.0901 / 0.4943\n",
            "[1/15][371/469] Loss_D: 1.0024 Loss_G: 3.0414 D(x): 0.8862 D(G(z)): 0.5365 / 0.0633\n",
            "[1/15][372/469] Loss_D: 0.8549 Loss_G: 1.3072 D(x): 0.5510 D(G(z)): 0.1479 / 0.3276\n",
            "[1/15][373/469] Loss_D: 0.8392 Loss_G: 3.0338 D(x): 0.8773 D(G(z)): 0.4733 / 0.0613\n",
            "[1/15][374/469] Loss_D: 0.9867 Loss_G: 0.9563 D(x): 0.4706 D(G(z)): 0.1041 / 0.4145\n",
            "[1/15][375/469] Loss_D: 1.0648 Loss_G: 2.9924 D(x): 0.8527 D(G(z)): 0.5558 / 0.0714\n",
            "[1/15][376/469] Loss_D: 1.1294 Loss_G: 0.8440 D(x): 0.4458 D(G(z)): 0.1466 / 0.4830\n",
            "[1/15][377/469] Loss_D: 0.9633 Loss_G: 2.4623 D(x): 0.8548 D(G(z)): 0.5093 / 0.1039\n",
            "[1/15][378/469] Loss_D: 0.8506 Loss_G: 1.6359 D(x): 0.6048 D(G(z)): 0.2314 / 0.2224\n",
            "[1/15][379/469] Loss_D: 0.7214 Loss_G: 1.2930 D(x): 0.6675 D(G(z)): 0.2278 / 0.3088\n",
            "[1/15][380/469] Loss_D: 0.8962 Loss_G: 2.4685 D(x): 0.8016 D(G(z)): 0.4520 / 0.1106\n",
            "[1/15][381/469] Loss_D: 0.8679 Loss_G: 1.3230 D(x): 0.5772 D(G(z)): 0.2163 / 0.2963\n",
            "[1/15][382/469] Loss_D: 0.6605 Loss_G: 1.6844 D(x): 0.7656 D(G(z)): 0.2947 / 0.2105\n",
            "[1/15][383/469] Loss_D: 0.7295 Loss_G: 2.2505 D(x): 0.7672 D(G(z)): 0.3420 / 0.1276\n",
            "[1/15][384/469] Loss_D: 0.7401 Loss_G: 1.2194 D(x): 0.6062 D(G(z)): 0.1677 / 0.3316\n",
            "[1/15][385/469] Loss_D: 0.9031 Loss_G: 2.2905 D(x): 0.7954 D(G(z)): 0.4527 / 0.1197\n",
            "[1/15][386/469] Loss_D: 0.7659 Loss_G: 1.2026 D(x): 0.5882 D(G(z)): 0.1543 / 0.3412\n",
            "[1/15][387/469] Loss_D: 0.7676 Loss_G: 2.5489 D(x): 0.8378 D(G(z)): 0.4155 / 0.0926\n",
            "[1/15][388/469] Loss_D: 0.7896 Loss_G: 1.1124 D(x): 0.5844 D(G(z)): 0.1664 / 0.3681\n",
            "[1/15][389/469] Loss_D: 0.7429 Loss_G: 2.5026 D(x): 0.8525 D(G(z)): 0.4117 / 0.0994\n",
            "[1/15][390/469] Loss_D: 0.8747 Loss_G: 1.0004 D(x): 0.5469 D(G(z)): 0.1720 / 0.3994\n",
            "[1/15][391/469] Loss_D: 0.8088 Loss_G: 2.6565 D(x): 0.8545 D(G(z)): 0.4518 / 0.0831\n",
            "[1/15][392/469] Loss_D: 0.9854 Loss_G: 0.7285 D(x): 0.4864 D(G(z)): 0.1314 / 0.5059\n",
            "[1/15][393/469] Loss_D: 1.2200 Loss_G: 3.4669 D(x): 0.8898 D(G(z)): 0.6317 / 0.0434\n",
            "[1/15][394/469] Loss_D: 1.5031 Loss_G: 0.7421 D(x): 0.3017 D(G(z)): 0.0761 / 0.5141\n",
            "[1/15][395/469] Loss_D: 1.2039 Loss_G: 3.9806 D(x): 0.8891 D(G(z)): 0.6186 / 0.0259\n",
            "[1/15][396/469] Loss_D: 1.8156 Loss_G: 0.2478 D(x): 0.2244 D(G(z)): 0.0880 / 0.7975\n",
            "[1/15][397/469] Loss_D: 1.8984 Loss_G: 3.8742 D(x): 0.9465 D(G(z)): 0.8048 / 0.0310\n",
            "[1/15][398/469] Loss_D: 1.5028 Loss_G: 0.8832 D(x): 0.3176 D(G(z)): 0.0820 / 0.4687\n",
            "[1/15][399/469] Loss_D: 1.2064 Loss_G: 2.4318 D(x): 0.8360 D(G(z)): 0.5892 / 0.1138\n",
            "[1/15][400/469] Loss_D: 0.8468 Loss_G: 1.7550 D(x): 0.6074 D(G(z)): 0.2282 / 0.1998\n",
            "saving the output\n",
            "[1/15][401/469] Loss_D: 0.7464 Loss_G: 1.6990 D(x): 0.7075 D(G(z)): 0.2923 / 0.2083\n",
            "[1/15][402/469] Loss_D: 0.8687 Loss_G: 2.1111 D(x): 0.7247 D(G(z)): 0.3727 / 0.1486\n",
            "[1/15][403/469] Loss_D: 0.7562 Loss_G: 1.5541 D(x): 0.6435 D(G(z)): 0.2247 / 0.2358\n",
            "[1/15][404/469] Loss_D: 0.8244 Loss_G: 2.2840 D(x): 0.7600 D(G(z)): 0.3870 / 0.1362\n",
            "[1/15][405/469] Loss_D: 0.8073 Loss_G: 1.4229 D(x): 0.6261 D(G(z)): 0.2292 / 0.2804\n",
            "[1/15][406/469] Loss_D: 0.7773 Loss_G: 2.4214 D(x): 0.8193 D(G(z)): 0.4032 / 0.1109\n",
            "[1/15][407/469] Loss_D: 0.8464 Loss_G: 1.2918 D(x): 0.5846 D(G(z)): 0.1997 / 0.3149\n",
            "[1/15][408/469] Loss_D: 0.8868 Loss_G: 2.2972 D(x): 0.7842 D(G(z)): 0.4385 / 0.1174\n",
            "[1/15][409/469] Loss_D: 0.7928 Loss_G: 1.4986 D(x): 0.6171 D(G(z)): 0.2108 / 0.2555\n",
            "[1/15][410/469] Loss_D: 0.7635 Loss_G: 2.1588 D(x): 0.7863 D(G(z)): 0.3708 / 0.1376\n",
            "[1/15][411/469] Loss_D: 0.7175 Loss_G: 1.5070 D(x): 0.6486 D(G(z)): 0.2135 / 0.2542\n",
            "[1/15][412/469] Loss_D: 0.7654 Loss_G: 2.2926 D(x): 0.7921 D(G(z)): 0.3876 / 0.1136\n",
            "[1/15][413/469] Loss_D: 0.8207 Loss_G: 1.1672 D(x): 0.5809 D(G(z)): 0.1915 / 0.3504\n",
            "[1/15][414/469] Loss_D: 0.9372 Loss_G: 2.3894 D(x): 0.7699 D(G(z)): 0.4460 / 0.1115\n",
            "[1/15][415/469] Loss_D: 0.7268 Loss_G: 1.6944 D(x): 0.6451 D(G(z)): 0.2011 / 0.2105\n",
            "[1/15][416/469] Loss_D: 0.8413 Loss_G: 1.2702 D(x): 0.6638 D(G(z)): 0.2945 / 0.3329\n",
            "[1/15][417/469] Loss_D: 0.8808 Loss_G: 2.8698 D(x): 0.8224 D(G(z)): 0.4556 / 0.0711\n",
            "[1/15][418/469] Loss_D: 0.8543 Loss_G: 1.1941 D(x): 0.5501 D(G(z)): 0.1498 / 0.3382\n",
            "[1/15][419/469] Loss_D: 0.8162 Loss_G: 2.4018 D(x): 0.8253 D(G(z)): 0.4301 / 0.1064\n",
            "[1/15][420/469] Loss_D: 0.8471 Loss_G: 1.3057 D(x): 0.5801 D(G(z)): 0.1982 / 0.3035\n",
            "[1/15][421/469] Loss_D: 0.8890 Loss_G: 2.4057 D(x): 0.7823 D(G(z)): 0.4453 / 0.1036\n",
            "[1/15][422/469] Loss_D: 0.9350 Loss_G: 1.0136 D(x): 0.5350 D(G(z)): 0.1973 / 0.3989\n",
            "[1/15][423/469] Loss_D: 0.8111 Loss_G: 2.4363 D(x): 0.8194 D(G(z)): 0.4219 / 0.1046\n",
            "[1/15][424/469] Loss_D: 0.7107 Loss_G: 1.6420 D(x): 0.6434 D(G(z)): 0.1836 / 0.2352\n",
            "[1/15][425/469] Loss_D: 0.6951 Loss_G: 2.3896 D(x): 0.7956 D(G(z)): 0.3409 / 0.1183\n",
            "[1/15][426/469] Loss_D: 0.7359 Loss_G: 1.1744 D(x): 0.6182 D(G(z)): 0.1611 / 0.3475\n",
            "[1/15][427/469] Loss_D: 0.9075 Loss_G: 2.8682 D(x): 0.8380 D(G(z)): 0.4750 / 0.0725\n",
            "[1/15][428/469] Loss_D: 0.8547 Loss_G: 1.1647 D(x): 0.5427 D(G(z)): 0.1318 / 0.3551\n",
            "[1/15][429/469] Loss_D: 0.7999 Loss_G: 2.0194 D(x): 0.8054 D(G(z)): 0.4069 / 0.1575\n",
            "[1/15][430/469] Loss_D: 0.7644 Loss_G: 1.4437 D(x): 0.6301 D(G(z)): 0.2035 / 0.2752\n",
            "[1/15][431/469] Loss_D: 0.7120 Loss_G: 2.2430 D(x): 0.8095 D(G(z)): 0.3556 / 0.1221\n",
            "[1/15][432/469] Loss_D: 0.8075 Loss_G: 1.5481 D(x): 0.6436 D(G(z)): 0.2370 / 0.2453\n",
            "[1/15][433/469] Loss_D: 0.7571 Loss_G: 1.5785 D(x): 0.7182 D(G(z)): 0.3083 / 0.2319\n",
            "[1/15][434/469] Loss_D: 0.8199 Loss_G: 1.5964 D(x): 0.6813 D(G(z)): 0.3087 / 0.2375\n",
            "[1/15][435/469] Loss_D: 0.8006 Loss_G: 1.7115 D(x): 0.7020 D(G(z)): 0.3110 / 0.2167\n",
            "[1/15][436/469] Loss_D: 0.7109 Loss_G: 1.5469 D(x): 0.6976 D(G(z)): 0.2607 / 0.2500\n",
            "[1/15][437/469] Loss_D: 0.8105 Loss_G: 2.1201 D(x): 0.7423 D(G(z)): 0.3657 / 0.1453\n",
            "[1/15][438/469] Loss_D: 0.7841 Loss_G: 1.0947 D(x): 0.6137 D(G(z)): 0.2083 / 0.3773\n",
            "[1/15][439/469] Loss_D: 0.7658 Loss_G: 2.4560 D(x): 0.8073 D(G(z)): 0.3911 / 0.1002\n",
            "[1/15][440/469] Loss_D: 0.7944 Loss_G: 1.1906 D(x): 0.5919 D(G(z)): 0.1725 / 0.3506\n",
            "[1/15][441/469] Loss_D: 0.7548 Loss_G: 2.4053 D(x): 0.8262 D(G(z)): 0.3965 / 0.1069\n",
            "[1/15][442/469] Loss_D: 0.8060 Loss_G: 1.0154 D(x): 0.5696 D(G(z)): 0.1554 / 0.3964\n",
            "[1/15][443/469] Loss_D: 0.8209 Loss_G: 2.9761 D(x): 0.8724 D(G(z)): 0.4595 / 0.0656\n",
            "[1/15][444/469] Loss_D: 0.8567 Loss_G: 0.9180 D(x): 0.5282 D(G(z)): 0.1166 / 0.4488\n",
            "[1/15][445/469] Loss_D: 0.8873 Loss_G: 2.6500 D(x): 0.8708 D(G(z)): 0.4977 / 0.0832\n",
            "[1/15][446/469] Loss_D: 0.8119 Loss_G: 1.2557 D(x): 0.5502 D(G(z)): 0.1305 / 0.3319\n",
            "[1/15][447/469] Loss_D: 0.6351 Loss_G: 2.2674 D(x): 0.8551 D(G(z)): 0.3528 / 0.1247\n",
            "[1/15][448/469] Loss_D: 0.6160 Loss_G: 1.6404 D(x): 0.6942 D(G(z)): 0.1902 / 0.2201\n",
            "[1/15][449/469] Loss_D: 0.6588 Loss_G: 1.9114 D(x): 0.7657 D(G(z)): 0.2955 / 0.1779\n",
            "[1/15][450/469] Loss_D: 0.7150 Loss_G: 1.9042 D(x): 0.7177 D(G(z)): 0.2813 / 0.1703\n",
            "[1/15][451/469] Loss_D: 0.7495 Loss_G: 1.3024 D(x): 0.6490 D(G(z)): 0.2223 / 0.3016\n",
            "[1/15][452/469] Loss_D: 0.7952 Loss_G: 2.4671 D(x): 0.7874 D(G(z)): 0.3832 / 0.1074\n",
            "[1/15][453/469] Loss_D: 0.9207 Loss_G: 0.6605 D(x): 0.5228 D(G(z)): 0.1713 / 0.5454\n",
            "[1/15][454/469] Loss_D: 1.1472 Loss_G: 3.9086 D(x): 0.9015 D(G(z)): 0.6088 / 0.0268\n",
            "[1/15][455/469] Loss_D: 1.3582 Loss_G: 0.6317 D(x): 0.3264 D(G(z)): 0.0616 / 0.5764\n",
            "[1/15][456/469] Loss_D: 1.1653 Loss_G: 3.6674 D(x): 0.9411 D(G(z)): 0.6131 / 0.0374\n",
            "[1/15][457/469] Loss_D: 1.1489 Loss_G: 0.8583 D(x): 0.4090 D(G(z)): 0.0959 / 0.4590\n",
            "[1/15][458/469] Loss_D: 0.9008 Loss_G: 2.9926 D(x): 0.8748 D(G(z)): 0.4982 / 0.0661\n",
            "[1/15][459/469] Loss_D: 0.8364 Loss_G: 1.3784 D(x): 0.5786 D(G(z)): 0.1672 / 0.3006\n",
            "[1/15][460/469] Loss_D: 0.7834 Loss_G: 2.2371 D(x): 0.7812 D(G(z)): 0.3755 / 0.1359\n",
            "[1/15][461/469] Loss_D: 0.7283 Loss_G: 1.9928 D(x): 0.7042 D(G(z)): 0.2456 / 0.1597\n",
            "[1/15][462/469] Loss_D: 0.6998 Loss_G: 1.5525 D(x): 0.7058 D(G(z)): 0.2406 / 0.2523\n",
            "[1/15][463/469] Loss_D: 0.6542 Loss_G: 2.3376 D(x): 0.8040 D(G(z)): 0.3272 / 0.1170\n",
            "[1/15][464/469] Loss_D: 0.7437 Loss_G: 1.4636 D(x): 0.6611 D(G(z)): 0.2293 / 0.2642\n",
            "[1/15][465/469] Loss_D: 0.7833 Loss_G: 2.1407 D(x): 0.7694 D(G(z)): 0.3640 / 0.1377\n",
            "[1/15][466/469] Loss_D: 0.9056 Loss_G: 0.9447 D(x): 0.5609 D(G(z)): 0.2037 / 0.4176\n",
            "[1/15][467/469] Loss_D: 0.9039 Loss_G: 3.0440 D(x): 0.8495 D(G(z)): 0.4874 / 0.0612\n",
            "[1/15][468/469] Loss_D: 1.2806 Loss_G: 0.4785 D(x): 0.3649 D(G(z)): 0.1100 / 0.6431\n",
            "[2/15][0/469] Loss_D: 1.6672 Loss_G: 3.8922 D(x): 0.9272 D(G(z)): 0.7585 / 0.0288\n",
            "saving the output\n",
            "[2/15][1/469] Loss_D: 1.5951 Loss_G: 0.8495 D(x): 0.2912 D(G(z)): 0.0647 / 0.4700\n",
            "[2/15][2/469] Loss_D: 1.1452 Loss_G: 3.6366 D(x): 0.8880 D(G(z)): 0.5966 / 0.0426\n",
            "[2/15][3/469] Loss_D: 1.6556 Loss_G: 0.3470 D(x): 0.2713 D(G(z)): 0.1029 / 0.7228\n",
            "[2/15][4/469] Loss_D: 1.6513 Loss_G: 3.3397 D(x): 0.9499 D(G(z)): 0.7604 / 0.0556\n",
            "[2/15][5/469] Loss_D: 0.9886 Loss_G: 2.1483 D(x): 0.5333 D(G(z)): 0.1357 / 0.1645\n",
            "[2/15][6/469] Loss_D: 0.7441 Loss_G: 1.5713 D(x): 0.7209 D(G(z)): 0.2927 / 0.2446\n",
            "[2/15][7/469] Loss_D: 0.7554 Loss_G: 2.4221 D(x): 0.7820 D(G(z)): 0.3598 / 0.1127\n",
            "[2/15][8/469] Loss_D: 0.9058 Loss_G: 1.6159 D(x): 0.6163 D(G(z)): 0.2573 / 0.2255\n",
            "[2/15][9/469] Loss_D: 0.8915 Loss_G: 2.8964 D(x): 0.7582 D(G(z)): 0.4163 / 0.0736\n",
            "[2/15][10/469] Loss_D: 1.0792 Loss_G: 0.8428 D(x): 0.4622 D(G(z)): 0.1602 / 0.4726\n",
            "[2/15][11/469] Loss_D: 1.0479 Loss_G: 3.5403 D(x): 0.9002 D(G(z)): 0.5741 / 0.0390\n",
            "[2/15][12/469] Loss_D: 1.0545 Loss_G: 1.2291 D(x): 0.4554 D(G(z)): 0.0953 / 0.3348\n",
            "[2/15][13/469] Loss_D: 0.9523 Loss_G: 2.9493 D(x): 0.8729 D(G(z)): 0.5176 / 0.0751\n",
            "[2/15][14/469] Loss_D: 0.9065 Loss_G: 1.2813 D(x): 0.5293 D(G(z)): 0.1397 / 0.3191\n",
            "[2/15][15/469] Loss_D: 0.8363 Loss_G: 2.6989 D(x): 0.8557 D(G(z)): 0.4496 / 0.0861\n",
            "[2/15][16/469] Loss_D: 0.8609 Loss_G: 1.4284 D(x): 0.5683 D(G(z)): 0.1766 / 0.2792\n",
            "[2/15][17/469] Loss_D: 0.8536 Loss_G: 2.3819 D(x): 0.8151 D(G(z)): 0.4436 / 0.1115\n",
            "[2/15][18/469] Loss_D: 0.8506 Loss_G: 1.2055 D(x): 0.5419 D(G(z)): 0.1472 / 0.3354\n",
            "[2/15][19/469] Loss_D: 0.8695 Loss_G: 2.4245 D(x): 0.8425 D(G(z)): 0.4715 / 0.1006\n",
            "[2/15][20/469] Loss_D: 0.8454 Loss_G: 1.2555 D(x): 0.5462 D(G(z)): 0.1479 / 0.3212\n",
            "[2/15][21/469] Loss_D: 0.7844 Loss_G: 2.4980 D(x): 0.8567 D(G(z)): 0.4365 / 0.0978\n",
            "[2/15][22/469] Loss_D: 0.8528 Loss_G: 1.2176 D(x): 0.5544 D(G(z)): 0.1679 / 0.3344\n",
            "[2/15][23/469] Loss_D: 0.8936 Loss_G: 2.3873 D(x): 0.8371 D(G(z)): 0.4609 / 0.1186\n",
            "[2/15][24/469] Loss_D: 0.8919 Loss_G: 1.2983 D(x): 0.5614 D(G(z)): 0.1870 / 0.3031\n",
            "[2/15][25/469] Loss_D: 0.8548 Loss_G: 2.5757 D(x): 0.8125 D(G(z)): 0.4451 / 0.0933\n",
            "[2/15][26/469] Loss_D: 1.0462 Loss_G: 0.6794 D(x): 0.4457 D(G(z)): 0.1301 / 0.5444\n",
            "[2/15][27/469] Loss_D: 1.3180 Loss_G: 3.0607 D(x): 0.8766 D(G(z)): 0.6562 / 0.0650\n",
            "[2/15][28/469] Loss_D: 0.9962 Loss_G: 1.4447 D(x): 0.4878 D(G(z)): 0.1331 / 0.2754\n",
            "[2/15][29/469] Loss_D: 0.6969 Loss_G: 1.5806 D(x): 0.7528 D(G(z)): 0.2911 / 0.2447\n",
            "[2/15][30/469] Loss_D: 0.7580 Loss_G: 2.4821 D(x): 0.7829 D(G(z)): 0.3633 / 0.0993\n",
            "[2/15][31/469] Loss_D: 0.7122 Loss_G: 1.2613 D(x): 0.6130 D(G(z)): 0.1488 / 0.3104\n",
            "[2/15][32/469] Loss_D: 0.6351 Loss_G: 2.2393 D(x): 0.8620 D(G(z)): 0.3611 / 0.1236\n",
            "[2/15][33/469] Loss_D: 0.6741 Loss_G: 1.5228 D(x): 0.6537 D(G(z)): 0.1790 / 0.2543\n",
            "[2/15][34/469] Loss_D: 0.6499 Loss_G: 1.9119 D(x): 0.7954 D(G(z)): 0.3214 / 0.1672\n",
            "[2/15][35/469] Loss_D: 0.6465 Loss_G: 1.7180 D(x): 0.7175 D(G(z)): 0.2390 / 0.2047\n",
            "[2/15][36/469] Loss_D: 0.6471 Loss_G: 1.8036 D(x): 0.7526 D(G(z)): 0.2782 / 0.1880\n",
            "[2/15][37/469] Loss_D: 0.7523 Loss_G: 1.6010 D(x): 0.6943 D(G(z)): 0.2760 / 0.2305\n",
            "[2/15][38/469] Loss_D: 0.7462 Loss_G: 2.0278 D(x): 0.7553 D(G(z)): 0.3288 / 0.1618\n",
            "[2/15][39/469] Loss_D: 0.7235 Loss_G: 1.4535 D(x): 0.6580 D(G(z)): 0.2196 / 0.2614\n",
            "[2/15][40/469] Loss_D: 0.7375 Loss_G: 1.7786 D(x): 0.7373 D(G(z)): 0.3153 / 0.1930\n",
            "[2/15][41/469] Loss_D: 0.6320 Loss_G: 1.8724 D(x): 0.7363 D(G(z)): 0.2448 / 0.1811\n",
            "[2/15][42/469] Loss_D: 0.7467 Loss_G: 1.5951 D(x): 0.6938 D(G(z)): 0.2794 / 0.2374\n",
            "[2/15][43/469] Loss_D: 0.6429 Loss_G: 2.0169 D(x): 0.7634 D(G(z)): 0.2790 / 0.1594\n",
            "[2/15][44/469] Loss_D: 0.7021 Loss_G: 1.4647 D(x): 0.6876 D(G(z)): 0.2404 / 0.2632\n",
            "[2/15][45/469] Loss_D: 0.7633 Loss_G: 2.1053 D(x): 0.7713 D(G(z)): 0.3641 / 0.1465\n",
            "[2/15][46/469] Loss_D: 0.6614 Loss_G: 1.2324 D(x): 0.6435 D(G(z)): 0.1474 / 0.3237\n",
            "[2/15][47/469] Loss_D: 0.7207 Loss_G: 2.3983 D(x): 0.8418 D(G(z)): 0.3916 / 0.1118\n",
            "[2/15][48/469] Loss_D: 0.7100 Loss_G: 1.4741 D(x): 0.6379 D(G(z)): 0.1871 / 0.2616\n",
            "[2/15][49/469] Loss_D: 0.6629 Loss_G: 2.0190 D(x): 0.7841 D(G(z)): 0.3141 / 0.1526\n",
            "[2/15][50/469] Loss_D: 0.7166 Loss_G: 1.3372 D(x): 0.6544 D(G(z)): 0.2065 / 0.2929\n",
            "[2/15][51/469] Loss_D: 0.7594 Loss_G: 2.3892 D(x): 0.8104 D(G(z)): 0.3921 / 0.1126\n",
            "[2/15][52/469] Loss_D: 0.8078 Loss_G: 1.0887 D(x): 0.5777 D(G(z)): 0.1627 / 0.3795\n",
            "[2/15][53/469] Loss_D: 0.8951 Loss_G: 2.8260 D(x): 0.8483 D(G(z)): 0.4784 / 0.0740\n",
            "[2/15][54/469] Loss_D: 1.0314 Loss_G: 0.7860 D(x): 0.4670 D(G(z)): 0.1133 / 0.4928\n",
            "[2/15][55/469] Loss_D: 0.9745 Loss_G: 2.8416 D(x): 0.8869 D(G(z)): 0.5375 / 0.0743\n",
            "[2/15][56/469] Loss_D: 0.7828 Loss_G: 1.5433 D(x): 0.5763 D(G(z)): 0.1363 / 0.2659\n",
            "[2/15][57/469] Loss_D: 0.7015 Loss_G: 2.0279 D(x): 0.8050 D(G(z)): 0.3441 / 0.1599\n",
            "[2/15][58/469] Loss_D: 0.6892 Loss_G: 1.4809 D(x): 0.6645 D(G(z)): 0.2016 / 0.2594\n",
            "[2/15][59/469] Loss_D: 0.6336 Loss_G: 2.8084 D(x): 0.8614 D(G(z)): 0.3559 / 0.0769\n",
            "[2/15][60/469] Loss_D: 0.6997 Loss_G: 1.1723 D(x): 0.6033 D(G(z)): 0.1248 / 0.3534\n",
            "[2/15][61/469] Loss_D: 0.6545 Loss_G: 2.4362 D(x): 0.8577 D(G(z)): 0.3650 / 0.1021\n",
            "[2/15][62/469] Loss_D: 0.6472 Loss_G: 1.4675 D(x): 0.6588 D(G(z)): 0.1647 / 0.2575\n",
            "[2/15][63/469] Loss_D: 0.7092 Loss_G: 2.4739 D(x): 0.8070 D(G(z)): 0.3620 / 0.1020\n",
            "[2/15][64/469] Loss_D: 0.7405 Loss_G: 1.2797 D(x): 0.6277 D(G(z)): 0.2012 / 0.3052\n",
            "[2/15][65/469] Loss_D: 0.6771 Loss_G: 2.1674 D(x): 0.7925 D(G(z)): 0.3279 / 0.1321\n",
            "[2/15][66/469] Loss_D: 0.7649 Loss_G: 1.0962 D(x): 0.6210 D(G(z)): 0.2039 / 0.3710\n",
            "[2/15][67/469] Loss_D: 0.9164 Loss_G: 3.3242 D(x): 0.8339 D(G(z)): 0.4908 / 0.0429\n",
            "[2/15][68/469] Loss_D: 1.1791 Loss_G: 0.5321 D(x): 0.3785 D(G(z)): 0.0913 / 0.6133\n",
            "[2/15][69/469] Loss_D: 1.2028 Loss_G: 4.4240 D(x): 0.9391 D(G(z)): 0.6411 / 0.0165\n",
            "[2/15][70/469] Loss_D: 1.7809 Loss_G: 0.3136 D(x): 0.2222 D(G(z)): 0.0398 / 0.7499\n",
            "[2/15][71/469] Loss_D: 1.7786 Loss_G: 4.6202 D(x): 0.9673 D(G(z)): 0.7673 / 0.0166\n",
            "[2/15][72/469] Loss_D: 1.5071 Loss_G: 0.5640 D(x): 0.2987 D(G(z)): 0.0579 / 0.5953\n",
            "[2/15][73/469] Loss_D: 1.3925 Loss_G: 2.8989 D(x): 0.9177 D(G(z)): 0.6925 / 0.0676\n",
            "[2/15][74/469] Loss_D: 0.8501 Loss_G: 1.6087 D(x): 0.5218 D(G(z)): 0.1224 / 0.2336\n",
            "[2/15][75/469] Loss_D: 0.7419 Loss_G: 1.4285 D(x): 0.7270 D(G(z)): 0.3104 / 0.2674\n",
            "[2/15][76/469] Loss_D: 0.7573 Loss_G: 2.3085 D(x): 0.8007 D(G(z)): 0.3807 / 0.1222\n",
            "[2/15][77/469] Loss_D: 0.7176 Loss_G: 1.5419 D(x): 0.6461 D(G(z)): 0.2021 / 0.2410\n",
            "[2/15][78/469] Loss_D: 0.7113 Loss_G: 2.1992 D(x): 0.8107 D(G(z)): 0.3643 / 0.1326\n",
            "[2/15][79/469] Loss_D: 0.7370 Loss_G: 1.2981 D(x): 0.6163 D(G(z)): 0.1818 / 0.3085\n",
            "[2/15][80/469] Loss_D: 0.9404 Loss_G: 2.3264 D(x): 0.7959 D(G(z)): 0.4736 / 0.1161\n",
            "[2/15][81/469] Loss_D: 0.7927 Loss_G: 1.6524 D(x): 0.6221 D(G(z)): 0.2226 / 0.2196\n",
            "[2/15][82/469] Loss_D: 0.8310 Loss_G: 1.0287 D(x): 0.6368 D(G(z)): 0.2633 / 0.3933\n",
            "[2/15][83/469] Loss_D: 0.8932 Loss_G: 2.6897 D(x): 0.8285 D(G(z)): 0.4669 / 0.0870\n",
            "[2/15][84/469] Loss_D: 0.7511 Loss_G: 1.4073 D(x): 0.5833 D(G(z)): 0.1325 / 0.2721\n",
            "[2/15][85/469] Loss_D: 0.7658 Loss_G: 2.0171 D(x): 0.7763 D(G(z)): 0.3677 / 0.1570\n",
            "[2/15][86/469] Loss_D: 0.6280 Loss_G: 1.8335 D(x): 0.7241 D(G(z)): 0.2259 / 0.1784\n",
            "[2/15][87/469] Loss_D: 0.6854 Loss_G: 1.8525 D(x): 0.7293 D(G(z)): 0.2696 / 0.1804\n",
            "[2/15][88/469] Loss_D: 0.5898 Loss_G: 1.8286 D(x): 0.7556 D(G(z)): 0.2379 / 0.1854\n",
            "[2/15][89/469] Loss_D: 0.6990 Loss_G: 2.1870 D(x): 0.7541 D(G(z)): 0.3062 / 0.1364\n",
            "[2/15][90/469] Loss_D: 0.7622 Loss_G: 1.2964 D(x): 0.6442 D(G(z)): 0.2273 / 0.3048\n",
            "[2/15][91/469] Loss_D: 0.8021 Loss_G: 2.4596 D(x): 0.7960 D(G(z)): 0.3951 / 0.1081\n",
            "[2/15][92/469] Loss_D: 0.7182 Loss_G: 1.4468 D(x): 0.6374 D(G(z)): 0.1836 / 0.2672\n",
            "[2/15][93/469] Loss_D: 0.7229 Loss_G: 2.2881 D(x): 0.7990 D(G(z)): 0.3567 / 0.1295\n",
            "[2/15][94/469] Loss_D: 0.6292 Loss_G: 1.7166 D(x): 0.6847 D(G(z)): 0.1840 / 0.2148\n",
            "[2/15][95/469] Loss_D: 0.6310 Loss_G: 2.2939 D(x): 0.8042 D(G(z)): 0.3089 / 0.1135\n",
            "[2/15][96/469] Loss_D: 0.7995 Loss_G: 1.1458 D(x): 0.6019 D(G(z)): 0.1905 / 0.3427\n",
            "[2/15][97/469] Loss_D: 0.8481 Loss_G: 3.4098 D(x): 0.8350 D(G(z)): 0.4569 / 0.0451\n",
            "[2/15][98/469] Loss_D: 1.2624 Loss_G: 0.3960 D(x): 0.3740 D(G(z)): 0.0994 / 0.6921\n",
            "[2/15][99/469] Loss_D: 1.5099 Loss_G: 4.1995 D(x): 0.9388 D(G(z)): 0.7319 / 0.0205\n",
            "[2/15][100/469] Loss_D: 1.6161 Loss_G: 0.8066 D(x): 0.2638 D(G(z)): 0.0412 / 0.4875\n",
            "saving the output\n",
            "[2/15][101/469] Loss_D: 1.0759 Loss_G: 3.0347 D(x): 0.9273 D(G(z)): 0.5983 / 0.0592\n",
            "[2/15][102/469] Loss_D: 0.6892 Loss_G: 1.7689 D(x): 0.6015 D(G(z)): 0.1158 / 0.2042\n",
            "[2/15][103/469] Loss_D: 0.6338 Loss_G: 2.1288 D(x): 0.8275 D(G(z)): 0.3259 / 0.1484\n",
            "[2/15][104/469] Loss_D: 0.6634 Loss_G: 2.0923 D(x): 0.7398 D(G(z)): 0.2703 / 0.1417\n",
            "[2/15][105/469] Loss_D: 0.7212 Loss_G: 1.5890 D(x): 0.6789 D(G(z)): 0.2360 / 0.2333\n",
            "[2/15][106/469] Loss_D: 0.7206 Loss_G: 2.3560 D(x): 0.7955 D(G(z)): 0.3572 / 0.1160\n",
            "[2/15][107/469] Loss_D: 0.8125 Loss_G: 1.1341 D(x): 0.5881 D(G(z)): 0.1870 / 0.3532\n",
            "[2/15][108/469] Loss_D: 0.8210 Loss_G: 2.4632 D(x): 0.8273 D(G(z)): 0.4305 / 0.1017\n",
            "[2/15][109/469] Loss_D: 0.7814 Loss_G: 1.4490 D(x): 0.5992 D(G(z)): 0.1794 / 0.2600\n",
            "[2/15][110/469] Loss_D: 0.7567 Loss_G: 1.9521 D(x): 0.7637 D(G(z)): 0.3547 / 0.1607\n",
            "[2/15][111/469] Loss_D: 0.7634 Loss_G: 1.6314 D(x): 0.6568 D(G(z)): 0.2549 / 0.2332\n",
            "[2/15][112/469] Loss_D: 0.6480 Loss_G: 2.0724 D(x): 0.7691 D(G(z)): 0.2814 / 0.1558\n",
            "[2/15][113/469] Loss_D: 0.7062 Loss_G: 2.2021 D(x): 0.7386 D(G(z)): 0.2776 / 0.1382\n",
            "[2/15][114/469] Loss_D: 0.5717 Loss_G: 1.6284 D(x): 0.7229 D(G(z)): 0.1845 / 0.2231\n",
            "[2/15][115/469] Loss_D: 0.6881 Loss_G: 2.1010 D(x): 0.7688 D(G(z)): 0.3149 / 0.1627\n",
            "[2/15][116/469] Loss_D: 0.6676 Loss_G: 1.7629 D(x): 0.7150 D(G(z)): 0.2376 / 0.2007\n",
            "[2/15][117/469] Loss_D: 0.6487 Loss_G: 1.4835 D(x): 0.7183 D(G(z)): 0.2431 / 0.2613\n",
            "[2/15][118/469] Loss_D: 0.6528 Loss_G: 2.9129 D(x): 0.8471 D(G(z)): 0.3590 / 0.0689\n",
            "[2/15][119/469] Loss_D: 0.8324 Loss_G: 0.7371 D(x): 0.5238 D(G(z)): 0.0843 / 0.5083\n",
            "[2/15][120/469] Loss_D: 1.2460 Loss_G: 3.6829 D(x): 0.9185 D(G(z)): 0.6563 / 0.0321\n",
            "[2/15][121/469] Loss_D: 1.2488 Loss_G: 0.9426 D(x): 0.3450 D(G(z)): 0.0500 / 0.4310\n",
            "[2/15][122/469] Loss_D: 1.1695 Loss_G: 3.3349 D(x): 0.8884 D(G(z)): 0.6070 / 0.0467\n",
            "[2/15][123/469] Loss_D: 1.3279 Loss_G: 0.5818 D(x): 0.3482 D(G(z)): 0.0862 / 0.5846\n",
            "[2/15][124/469] Loss_D: 1.3017 Loss_G: 3.3825 D(x): 0.9554 D(G(z)): 0.6819 / 0.0442\n",
            "[2/15][125/469] Loss_D: 1.0239 Loss_G: 1.1637 D(x): 0.4532 D(G(z)): 0.1047 / 0.3505\n",
            "[2/15][126/469] Loss_D: 0.9503 Loss_G: 2.0330 D(x): 0.7891 D(G(z)): 0.4658 / 0.1482\n",
            "[2/15][127/469] Loss_D: 0.7558 Loss_G: 1.8572 D(x): 0.6550 D(G(z)): 0.2355 / 0.1858\n",
            "[2/15][128/469] Loss_D: 0.7861 Loss_G: 1.3916 D(x): 0.6564 D(G(z)): 0.2684 / 0.2818\n",
            "[2/15][129/469] Loss_D: 0.8458 Loss_G: 2.4849 D(x): 0.7786 D(G(z)): 0.4183 / 0.0989\n",
            "[2/15][130/469] Loss_D: 0.8498 Loss_G: 1.1969 D(x): 0.5481 D(G(z)): 0.1674 / 0.3405\n",
            "[2/15][131/469] Loss_D: 0.8480 Loss_G: 2.7060 D(x): 0.8728 D(G(z)): 0.4781 / 0.0872\n",
            "[2/15][132/469] Loss_D: 0.8601 Loss_G: 1.0631 D(x): 0.5247 D(G(z)): 0.1210 / 0.3927\n",
            "[2/15][133/469] Loss_D: 0.9510 Loss_G: 2.6948 D(x): 0.8685 D(G(z)): 0.5100 / 0.0854\n",
            "[2/15][134/469] Loss_D: 0.8132 Loss_G: 1.4976 D(x): 0.5650 D(G(z)): 0.1398 / 0.2586\n",
            "[2/15][135/469] Loss_D: 0.7034 Loss_G: 1.9233 D(x): 0.7922 D(G(z)): 0.3360 / 0.1712\n",
            "[2/15][136/469] Loss_D: 0.7379 Loss_G: 1.8930 D(x): 0.6858 D(G(z)): 0.2585 / 0.1764\n",
            "[2/15][137/469] Loss_D: 0.7911 Loss_G: 1.7228 D(x): 0.6772 D(G(z)): 0.2837 / 0.2125\n",
            "[2/15][138/469] Loss_D: 0.6936 Loss_G: 1.6214 D(x): 0.7047 D(G(z)): 0.2514 / 0.2244\n",
            "[2/15][139/469] Loss_D: 0.7194 Loss_G: 2.0208 D(x): 0.7623 D(G(z)): 0.3312 / 0.1520\n",
            "[2/15][140/469] Loss_D: 0.7658 Loss_G: 1.3143 D(x): 0.6420 D(G(z)): 0.2333 / 0.2933\n",
            "[2/15][141/469] Loss_D: 0.6820 Loss_G: 2.6732 D(x): 0.8487 D(G(z)): 0.3842 / 0.0821\n",
            "[2/15][142/469] Loss_D: 0.7859 Loss_G: 1.0788 D(x): 0.5547 D(G(z)): 0.1270 / 0.3667\n",
            "[2/15][143/469] Loss_D: 0.7776 Loss_G: 2.6606 D(x): 0.8900 D(G(z)): 0.4570 / 0.0857\n",
            "[2/15][144/469] Loss_D: 0.6099 Loss_G: 2.0062 D(x): 0.6748 D(G(z)): 0.1365 / 0.1678\n",
            "[2/15][145/469] Loss_D: 0.5399 Loss_G: 1.7439 D(x): 0.7769 D(G(z)): 0.2238 / 0.2094\n",
            "[2/15][146/469] Loss_D: 0.6229 Loss_G: 2.0546 D(x): 0.7828 D(G(z)): 0.2893 / 0.1508\n",
            "[2/15][147/469] Loss_D: 0.5520 Loss_G: 2.2266 D(x): 0.7745 D(G(z)): 0.2325 / 0.1292\n",
            "[2/15][148/469] Loss_D: 0.6415 Loss_G: 1.6126 D(x): 0.6956 D(G(z)): 0.2045 / 0.2360\n",
            "[2/15][149/469] Loss_D: 0.6778 Loss_G: 1.9992 D(x): 0.7755 D(G(z)): 0.3092 / 0.1579\n",
            "[2/15][150/469] Loss_D: 0.7819 Loss_G: 1.4354 D(x): 0.6502 D(G(z)): 0.2549 / 0.2674\n",
            "[2/15][151/469] Loss_D: 0.7324 Loss_G: 2.4318 D(x): 0.7793 D(G(z)): 0.3483 / 0.1015\n",
            "[2/15][152/469] Loss_D: 0.6720 Loss_G: 1.5737 D(x): 0.6569 D(G(z)): 0.1821 / 0.2420\n",
            "[2/15][153/469] Loss_D: 0.7032 Loss_G: 2.8408 D(x): 0.8110 D(G(z)): 0.3549 / 0.0696\n",
            "[2/15][154/469] Loss_D: 0.7719 Loss_G: 1.1381 D(x): 0.5847 D(G(z)): 0.1473 / 0.3506\n",
            "[2/15][155/469] Loss_D: 0.8214 Loss_G: 3.6794 D(x): 0.9056 D(G(z)): 0.4856 / 0.0382\n",
            "[2/15][156/469] Loss_D: 0.9159 Loss_G: 1.1254 D(x): 0.4914 D(G(z)): 0.0718 / 0.3634\n",
            "[2/15][157/469] Loss_D: 0.9079 Loss_G: 2.9230 D(x): 0.8932 D(G(z)): 0.4996 / 0.0748\n",
            "[2/15][158/469] Loss_D: 0.5848 Loss_G: 2.1434 D(x): 0.6860 D(G(z)): 0.1383 / 0.1515\n",
            "[2/15][159/469] Loss_D: 0.6877 Loss_G: 1.6993 D(x): 0.7339 D(G(z)): 0.2766 / 0.2145\n",
            "[2/15][160/469] Loss_D: 0.6970 Loss_G: 2.3162 D(x): 0.7636 D(G(z)): 0.3180 / 0.1160\n",
            "[2/15][161/469] Loss_D: 0.7779 Loss_G: 1.1688 D(x): 0.6080 D(G(z)): 0.1839 / 0.3466\n",
            "[2/15][162/469] Loss_D: 0.8629 Loss_G: 3.0510 D(x): 0.8363 D(G(z)): 0.4553 / 0.0629\n",
            "[2/15][163/469] Loss_D: 1.0441 Loss_G: 0.6870 D(x): 0.4669 D(G(z)): 0.1509 / 0.5339\n",
            "[2/15][164/469] Loss_D: 0.9664 Loss_G: 3.1084 D(x): 0.8929 D(G(z)): 0.5331 / 0.0529\n",
            "[2/15][165/469] Loss_D: 0.9027 Loss_G: 1.3612 D(x): 0.5133 D(G(z)): 0.1404 / 0.3215\n",
            "[2/15][166/469] Loss_D: 0.7523 Loss_G: 2.3415 D(x): 0.8163 D(G(z)): 0.3669 / 0.1151\n",
            "[2/15][167/469] Loss_D: 0.6267 Loss_G: 2.2524 D(x): 0.7433 D(G(z)): 0.2430 / 0.1268\n",
            "[2/15][168/469] Loss_D: 0.5881 Loss_G: 1.5762 D(x): 0.7017 D(G(z)): 0.1786 / 0.2417\n",
            "[2/15][169/469] Loss_D: 0.6409 Loss_G: 2.1898 D(x): 0.8033 D(G(z)): 0.3126 / 0.1319\n",
            "[2/15][170/469] Loss_D: 0.6691 Loss_G: 1.5781 D(x): 0.6854 D(G(z)): 0.2168 / 0.2396\n",
            "[2/15][171/469] Loss_D: 0.6583 Loss_G: 2.2228 D(x): 0.7846 D(G(z)): 0.3136 / 0.1290\n",
            "[2/15][172/469] Loss_D: 0.6540 Loss_G: 1.5258 D(x): 0.6772 D(G(z)): 0.1790 / 0.2524\n",
            "[2/15][173/469] Loss_D: 0.5435 Loss_G: 2.6259 D(x): 0.8595 D(G(z)): 0.3005 / 0.0834\n",
            "[2/15][174/469] Loss_D: 0.6581 Loss_G: 1.3387 D(x): 0.6522 D(G(z)): 0.1614 / 0.2980\n",
            "[2/15][175/469] Loss_D: 0.7476 Loss_G: 2.7059 D(x): 0.8436 D(G(z)): 0.3999 / 0.0819\n",
            "[2/15][176/469] Loss_D: 0.7713 Loss_G: 1.0159 D(x): 0.5615 D(G(z)): 0.1034 / 0.3930\n",
            "[2/15][177/469] Loss_D: 1.0402 Loss_G: 3.3230 D(x): 0.9027 D(G(z)): 0.5717 / 0.0438\n",
            "[2/15][178/469] Loss_D: 1.2638 Loss_G: 0.6548 D(x): 0.3631 D(G(z)): 0.0977 / 0.5525\n",
            "[2/15][179/469] Loss_D: 1.0269 Loss_G: 3.4615 D(x): 0.9250 D(G(z)): 0.5734 / 0.0430\n",
            "[2/15][180/469] Loss_D: 0.9975 Loss_G: 1.0957 D(x): 0.4734 D(G(z)): 0.1308 / 0.3764\n",
            "[2/15][181/469] Loss_D: 0.8669 Loss_G: 3.6237 D(x): 0.8935 D(G(z)): 0.4886 / 0.0338\n",
            "[2/15][182/469] Loss_D: 1.0325 Loss_G: 0.8333 D(x): 0.4365 D(G(z)): 0.0775 / 0.4887\n",
            "[2/15][183/469] Loss_D: 1.1395 Loss_G: 4.1311 D(x): 0.9250 D(G(z)): 0.6188 / 0.0244\n",
            "[2/15][184/469] Loss_D: 1.1560 Loss_G: 0.7894 D(x): 0.3944 D(G(z)): 0.0665 / 0.4913\n",
            "[2/15][185/469] Loss_D: 1.1078 Loss_G: 3.6654 D(x): 0.9354 D(G(z)): 0.6071 / 0.0334\n",
            "[2/15][186/469] Loss_D: 0.8144 Loss_G: 1.7506 D(x): 0.5191 D(G(z)): 0.0672 / 0.2207\n",
            "[2/15][187/469] Loss_D: 0.6185 Loss_G: 2.2694 D(x): 0.8657 D(G(z)): 0.3518 / 0.1273\n",
            "[2/15][188/469] Loss_D: 0.6560 Loss_G: 2.3085 D(x): 0.7404 D(G(z)): 0.2577 / 0.1200\n",
            "[2/15][189/469] Loss_D: 0.7481 Loss_G: 1.4361 D(x): 0.6533 D(G(z)): 0.2207 / 0.2676\n",
            "[2/15][190/469] Loss_D: 0.9020 Loss_G: 2.6779 D(x): 0.7881 D(G(z)): 0.4445 / 0.0839\n",
            "[2/15][191/469] Loss_D: 0.7885 Loss_G: 1.2589 D(x): 0.5645 D(G(z)): 0.1395 / 0.3191\n",
            "[2/15][192/469] Loss_D: 0.8612 Loss_G: 2.7943 D(x): 0.8270 D(G(z)): 0.4529 / 0.0805\n",
            "[2/15][193/469] Loss_D: 0.6943 Loss_G: 1.6131 D(x): 0.6319 D(G(z)): 0.1585 / 0.2329\n",
            "[2/15][194/469] Loss_D: 0.7045 Loss_G: 2.0007 D(x): 0.7733 D(G(z)): 0.3279 / 0.1535\n",
            "[2/15][195/469] Loss_D: 0.6681 Loss_G: 1.6543 D(x): 0.6872 D(G(z)): 0.2184 / 0.2227\n",
            "[2/15][196/469] Loss_D: 0.7192 Loss_G: 1.9078 D(x): 0.7386 D(G(z)): 0.3018 / 0.1746\n",
            "[2/15][197/469] Loss_D: 0.6234 Loss_G: 2.0350 D(x): 0.7405 D(G(z)): 0.2465 / 0.1560\n",
            "[2/15][198/469] Loss_D: 0.5967 Loss_G: 1.9369 D(x): 0.7393 D(G(z)): 0.2214 / 0.1705\n",
            "[2/15][199/469] Loss_D: 0.6995 Loss_G: 2.3059 D(x): 0.7630 D(G(z)): 0.3127 / 0.1159\n",
            "[2/15][200/469] Loss_D: 0.5801 Loss_G: 1.5105 D(x): 0.6958 D(G(z)): 0.1571 / 0.2557\n",
            "saving the output\n",
            "[2/15][201/469] Loss_D: 0.7896 Loss_G: 2.9920 D(x): 0.8269 D(G(z)): 0.4142 / 0.0714\n",
            "[2/15][202/469] Loss_D: 0.9148 Loss_G: 0.7447 D(x): 0.4990 D(G(z)): 0.1103 / 0.4990\n",
            "[2/15][203/469] Loss_D: 1.1541 Loss_G: 4.2652 D(x): 0.9084 D(G(z)): 0.6205 / 0.0197\n",
            "[2/15][204/469] Loss_D: 1.2847 Loss_G: 1.0343 D(x): 0.3479 D(G(z)): 0.0528 / 0.4162\n",
            "[2/15][205/469] Loss_D: 0.9966 Loss_G: 3.3580 D(x): 0.8805 D(G(z)): 0.5384 / 0.0443\n",
            "[2/15][206/469] Loss_D: 0.9349 Loss_G: 0.8001 D(x): 0.4963 D(G(z)): 0.1130 / 0.4936\n",
            "[2/15][207/469] Loss_D: 1.0694 Loss_G: 3.5439 D(x): 0.9373 D(G(z)): 0.5941 / 0.0416\n",
            "[2/15][208/469] Loss_D: 0.8760 Loss_G: 1.6964 D(x): 0.5392 D(G(z)): 0.1248 / 0.2256\n",
            "[2/15][209/469] Loss_D: 0.7759 Loss_G: 1.8402 D(x): 0.7525 D(G(z)): 0.3445 / 0.1871\n",
            "[2/15][210/469] Loss_D: 0.8261 Loss_G: 3.1732 D(x): 0.7692 D(G(z)): 0.3829 / 0.0612\n",
            "[2/15][211/469] Loss_D: 1.0208 Loss_G: 0.9092 D(x): 0.4883 D(G(z)): 0.1416 / 0.4573\n",
            "[2/15][212/469] Loss_D: 1.1725 Loss_G: 3.8268 D(x): 0.8715 D(G(z)): 0.6085 / 0.0357\n",
            "[2/15][213/469] Loss_D: 1.3461 Loss_G: 0.7657 D(x): 0.3533 D(G(z)): 0.0717 / 0.5019\n",
            "[2/15][214/469] Loss_D: 1.4910 Loss_G: 4.4242 D(x): 0.9223 D(G(z)): 0.7019 / 0.0185\n",
            "[2/15][215/469] Loss_D: 1.6324 Loss_G: 0.9272 D(x): 0.2842 D(G(z)): 0.0384 / 0.4394\n",
            "[2/15][216/469] Loss_D: 1.2501 Loss_G: 3.7850 D(x): 0.9127 D(G(z)): 0.6532 / 0.0315\n",
            "[2/15][217/469] Loss_D: 1.0888 Loss_G: 0.9795 D(x): 0.4330 D(G(z)): 0.0962 / 0.4195\n",
            "[2/15][218/469] Loss_D: 0.9141 Loss_G: 2.7426 D(x): 0.9055 D(G(z)): 0.5114 / 0.0816\n",
            "[2/15][219/469] Loss_D: 0.6406 Loss_G: 2.1196 D(x): 0.6609 D(G(z)): 0.1620 / 0.1417\n",
            "[2/15][220/469] Loss_D: 0.7241 Loss_G: 2.0688 D(x): 0.7477 D(G(z)): 0.3145 / 0.1532\n",
            "[2/15][221/469] Loss_D: 0.5820 Loss_G: 2.3766 D(x): 0.7882 D(G(z)): 0.2621 / 0.1160\n",
            "[2/15][222/469] Loss_D: 0.6681 Loss_G: 1.7068 D(x): 0.6826 D(G(z)): 0.2115 / 0.2122\n",
            "[2/15][223/469] Loss_D: 0.7166 Loss_G: 2.3394 D(x): 0.7806 D(G(z)): 0.3429 / 0.1135\n",
            "[2/15][224/469] Loss_D: 0.6582 Loss_G: 1.8436 D(x): 0.6764 D(G(z)): 0.1951 / 0.1853\n",
            "[2/15][225/469] Loss_D: 0.7720 Loss_G: 2.3331 D(x): 0.7573 D(G(z)): 0.3533 / 0.1158\n",
            "[2/15][226/469] Loss_D: 0.7241 Loss_G: 1.4297 D(x): 0.6414 D(G(z)): 0.2021 / 0.2706\n",
            "[2/15][227/469] Loss_D: 0.7107 Loss_G: 2.6674 D(x): 0.8181 D(G(z)): 0.3623 / 0.0898\n",
            "[2/15][228/469] Loss_D: 0.8409 Loss_G: 0.9697 D(x): 0.5700 D(G(z)): 0.1718 / 0.4311\n",
            "[2/15][229/469] Loss_D: 1.1794 Loss_G: 3.6584 D(x): 0.8727 D(G(z)): 0.5842 / 0.0365\n",
            "[2/15][230/469] Loss_D: 1.0127 Loss_G: 1.2273 D(x): 0.4353 D(G(z)): 0.0686 / 0.3669\n",
            "[2/15][231/469] Loss_D: 0.7787 Loss_G: 2.0849 D(x): 0.8478 D(G(z)): 0.4076 / 0.1525\n",
            "[2/15][232/469] Loss_D: 0.6125 Loss_G: 2.5622 D(x): 0.7692 D(G(z)): 0.2587 / 0.0934\n",
            "[2/15][233/469] Loss_D: 0.8093 Loss_G: 1.2646 D(x): 0.5902 D(G(z)): 0.1725 / 0.3379\n",
            "[2/15][234/469] Loss_D: 0.9176 Loss_G: 2.4181 D(x): 0.7953 D(G(z)): 0.4523 / 0.1078\n",
            "[2/15][235/469] Loss_D: 0.6925 Loss_G: 1.5658 D(x): 0.6338 D(G(z)): 0.1618 / 0.2436\n",
            "[2/15][236/469] Loss_D: 0.6698 Loss_G: 1.9075 D(x): 0.7793 D(G(z)): 0.3162 / 0.1681\n",
            "[2/15][237/469] Loss_D: 0.6431 Loss_G: 2.0564 D(x): 0.7466 D(G(z)): 0.2661 / 0.1527\n",
            "[2/15][238/469] Loss_D: 0.6030 Loss_G: 1.8188 D(x): 0.7185 D(G(z)): 0.2052 / 0.1995\n",
            "[2/15][239/469] Loss_D: 0.5678 Loss_G: 2.3417 D(x): 0.8196 D(G(z)): 0.2857 / 0.1141\n",
            "[2/15][240/469] Loss_D: 0.5984 Loss_G: 1.6066 D(x): 0.7011 D(G(z)): 0.1767 / 0.2330\n",
            "[2/15][241/469] Loss_D: 0.6427 Loss_G: 2.4257 D(x): 0.8233 D(G(z)): 0.3374 / 0.1063\n",
            "[2/15][242/469] Loss_D: 0.6093 Loss_G: 1.4619 D(x): 0.6620 D(G(z)): 0.1479 / 0.2683\n",
            "[2/15][243/469] Loss_D: 0.7306 Loss_G: 2.4527 D(x): 0.8229 D(G(z)): 0.3787 / 0.1021\n",
            "[2/15][244/469] Loss_D: 0.6828 Loss_G: 1.2502 D(x): 0.6186 D(G(z)): 0.1280 / 0.3241\n",
            "[2/15][245/469] Loss_D: 0.6498 Loss_G: 2.7560 D(x): 0.8847 D(G(z)): 0.3887 / 0.0763\n",
            "[2/15][246/469] Loss_D: 0.7723 Loss_G: 1.0890 D(x): 0.5806 D(G(z)): 0.1415 / 0.3688\n",
            "[2/15][247/469] Loss_D: 0.6770 Loss_G: 2.7883 D(x): 0.8867 D(G(z)): 0.4016 / 0.0749\n",
            "[2/15][248/469] Loss_D: 0.6297 Loss_G: 1.7761 D(x): 0.6541 D(G(z)): 0.1462 / 0.1951\n",
            "[2/15][249/469] Loss_D: 0.6486 Loss_G: 1.4952 D(x): 0.7354 D(G(z)): 0.2525 / 0.2557\n",
            "[2/15][250/469] Loss_D: 0.6857 Loss_G: 2.8225 D(x): 0.8357 D(G(z)): 0.3643 / 0.0754\n",
            "[2/15][251/469] Loss_D: 0.8191 Loss_G: 0.9846 D(x): 0.5337 D(G(z)): 0.1099 / 0.4098\n",
            "[2/15][252/469] Loss_D: 0.9813 Loss_G: 3.2383 D(x): 0.9126 D(G(z)): 0.5579 / 0.0516\n",
            "[2/15][253/469] Loss_D: 0.9166 Loss_G: 1.2979 D(x): 0.4782 D(G(z)): 0.0711 / 0.3228\n",
            "[2/15][254/469] Loss_D: 0.7112 Loss_G: 2.3296 D(x): 0.8766 D(G(z)): 0.4056 / 0.1203\n",
            "[2/15][255/469] Loss_D: 0.6165 Loss_G: 1.7428 D(x): 0.6856 D(G(z)): 0.1712 / 0.2018\n",
            "[2/15][256/469] Loss_D: 0.6647 Loss_G: 2.2001 D(x): 0.7994 D(G(z)): 0.3272 / 0.1308\n",
            "[2/15][257/469] Loss_D: 0.5596 Loss_G: 1.7795 D(x): 0.7170 D(G(z)): 0.1756 / 0.1986\n",
            "[2/15][258/469] Loss_D: 0.6582 Loss_G: 1.9733 D(x): 0.7611 D(G(z)): 0.2785 / 0.1634\n",
            "[2/15][259/469] Loss_D: 0.5778 Loss_G: 2.0839 D(x): 0.7643 D(G(z)): 0.2392 / 0.1502\n",
            "[2/15][260/469] Loss_D: 0.6231 Loss_G: 2.4727 D(x): 0.7863 D(G(z)): 0.2933 / 0.0998\n",
            "[2/15][261/469] Loss_D: 0.7257 Loss_G: 1.3070 D(x): 0.6317 D(G(z)): 0.1733 / 0.3030\n",
            "[2/15][262/469] Loss_D: 1.0234 Loss_G: 4.0130 D(x): 0.8768 D(G(z)): 0.5590 / 0.0279\n",
            "[2/15][263/469] Loss_D: 1.4609 Loss_G: 0.8121 D(x): 0.2886 D(G(z)): 0.0364 / 0.5150\n",
            "[2/15][264/469] Loss_D: 1.4167 Loss_G: 3.2902 D(x): 0.9393 D(G(z)): 0.6823 / 0.0523\n",
            "[2/15][265/469] Loss_D: 0.9770 Loss_G: 1.2962 D(x): 0.4884 D(G(z)): 0.1083 / 0.3094\n",
            "[2/15][266/469] Loss_D: 0.8719 Loss_G: 3.1122 D(x): 0.8845 D(G(z)): 0.4946 / 0.0588\n",
            "[2/15][267/469] Loss_D: 0.8881 Loss_G: 1.1822 D(x): 0.5200 D(G(z)): 0.1155 / 0.3541\n",
            "[2/15][268/469] Loss_D: 0.8589 Loss_G: 2.8247 D(x): 0.8695 D(G(z)): 0.4747 / 0.0737\n",
            "[2/15][269/469] Loss_D: 0.8277 Loss_G: 1.2492 D(x): 0.5344 D(G(z)): 0.1238 / 0.3234\n",
            "[2/15][270/469] Loss_D: 0.9401 Loss_G: 2.6815 D(x): 0.8323 D(G(z)): 0.4885 / 0.0831\n",
            "[2/15][271/469] Loss_D: 0.7354 Loss_G: 1.4612 D(x): 0.5874 D(G(z)): 0.1364 / 0.2576\n",
            "[2/15][272/469] Loss_D: 0.6519 Loss_G: 2.2413 D(x): 0.8185 D(G(z)): 0.3358 / 0.1332\n",
            "[2/15][273/469] Loss_D: 0.6404 Loss_G: 2.1042 D(x): 0.7256 D(G(z)): 0.2381 / 0.1418\n",
            "[2/15][274/469] Loss_D: 0.8183 Loss_G: 1.0485 D(x): 0.6083 D(G(z)): 0.2219 / 0.3903\n",
            "[2/15][275/469] Loss_D: 0.7808 Loss_G: 2.6217 D(x): 0.8375 D(G(z)): 0.4181 / 0.0914\n",
            "[2/15][276/469] Loss_D: 0.7023 Loss_G: 1.4852 D(x): 0.6218 D(G(z)): 0.1444 / 0.2632\n",
            "[2/15][277/469] Loss_D: 0.6167 Loss_G: 1.9133 D(x): 0.8032 D(G(z)): 0.3009 / 0.1730\n",
            "[2/15][278/469] Loss_D: 0.6213 Loss_G: 2.2401 D(x): 0.7717 D(G(z)): 0.2675 / 0.1242\n",
            "[2/15][279/469] Loss_D: 0.6471 Loss_G: 1.3739 D(x): 0.6676 D(G(z)): 0.1766 / 0.2884\n",
            "[2/15][280/469] Loss_D: 0.7740 Loss_G: 2.7259 D(x): 0.8479 D(G(z)): 0.4180 / 0.0843\n",
            "[2/15][281/469] Loss_D: 0.6396 Loss_G: 1.6826 D(x): 0.6403 D(G(z)): 0.1364 / 0.2217\n",
            "[2/15][282/469] Loss_D: 0.6417 Loss_G: 1.3844 D(x): 0.7322 D(G(z)): 0.2498 / 0.2821\n",
            "[2/15][283/469] Loss_D: 0.7045 Loss_G: 2.8476 D(x): 0.8557 D(G(z)): 0.3890 / 0.0713\n",
            "[2/15][284/469] Loss_D: 0.6634 Loss_G: 1.3336 D(x): 0.5989 D(G(z)): 0.0955 / 0.2957\n",
            "[2/15][285/469] Loss_D: 0.7708 Loss_G: 2.2824 D(x): 0.8399 D(G(z)): 0.4151 / 0.1267\n",
            "[2/15][286/469] Loss_D: 0.6121 Loss_G: 1.9280 D(x): 0.6983 D(G(z)): 0.1807 / 0.1746\n",
            "[2/15][287/469] Loss_D: 0.6556 Loss_G: 1.7124 D(x): 0.7409 D(G(z)): 0.2625 / 0.2140\n",
            "[2/15][288/469] Loss_D: 0.6441 Loss_G: 2.2551 D(x): 0.7919 D(G(z)): 0.2995 / 0.1288\n",
            "[2/15][289/469] Loss_D: 0.6546 Loss_G: 1.5983 D(x): 0.6896 D(G(z)): 0.1961 / 0.2334\n",
            "[2/15][290/469] Loss_D: 0.6863 Loss_G: 2.5524 D(x): 0.8124 D(G(z)): 0.3490 / 0.0972\n",
            "[2/15][291/469] Loss_D: 0.6911 Loss_G: 1.0497 D(x): 0.6036 D(G(z)): 0.1263 / 0.3883\n",
            "[2/15][292/469] Loss_D: 1.0159 Loss_G: 3.4668 D(x): 0.8824 D(G(z)): 0.5497 / 0.0418\n",
            "[2/15][293/469] Loss_D: 1.0613 Loss_G: 0.9114 D(x): 0.4115 D(G(z)): 0.0439 / 0.4358\n",
            "[2/15][294/469] Loss_D: 1.0944 Loss_G: 3.7110 D(x): 0.9071 D(G(z)): 0.5880 / 0.0361\n",
            "[2/15][295/469] Loss_D: 1.2748 Loss_G: 0.8727 D(x): 0.3644 D(G(z)): 0.0711 / 0.4998\n",
            "[2/15][296/469] Loss_D: 1.0950 Loss_G: 4.1144 D(x): 0.9240 D(G(z)): 0.5775 / 0.0235\n",
            "[2/15][297/469] Loss_D: 1.4283 Loss_G: 0.4148 D(x): 0.3195 D(G(z)): 0.0879 / 0.6975\n",
            "[2/15][298/469] Loss_D: 1.6461 Loss_G: 4.3180 D(x): 0.9552 D(G(z)): 0.7319 / 0.0202\n",
            "[2/15][299/469] Loss_D: 0.9781 Loss_G: 1.8198 D(x): 0.5110 D(G(z)): 0.1171 / 0.2268\n",
            "[2/15][300/469] Loss_D: 0.5785 Loss_G: 1.8705 D(x): 0.8048 D(G(z)): 0.2678 / 0.1873\n",
            "saving the output\n",
            "[2/15][301/469] Loss_D: 0.5898 Loss_G: 2.6878 D(x): 0.8331 D(G(z)): 0.3043 / 0.0835\n",
            "[2/15][302/469] Loss_D: 0.7839 Loss_G: 1.4664 D(x): 0.6361 D(G(z)): 0.2238 / 0.2620\n",
            "[2/15][303/469] Loss_D: 0.9752 Loss_G: 2.2625 D(x): 0.7156 D(G(z)): 0.4256 / 0.1348\n",
            "[2/15][304/469] Loss_D: 0.8530 Loss_G: 1.4430 D(x): 0.6062 D(G(z)): 0.2441 / 0.2701\n",
            "[2/15][305/469] Loss_D: 0.8321 Loss_G: 2.9285 D(x): 0.7903 D(G(z)): 0.4129 / 0.0675\n",
            "[2/15][306/469] Loss_D: 0.9979 Loss_G: 0.7949 D(x): 0.4823 D(G(z)): 0.1303 / 0.4798\n",
            "[2/15][307/469] Loss_D: 1.0399 Loss_G: 3.7315 D(x): 0.9234 D(G(z)): 0.5852 / 0.0340\n",
            "[2/15][308/469] Loss_D: 0.8762 Loss_G: 1.3434 D(x): 0.4943 D(G(z)): 0.0645 / 0.2964\n",
            "[2/15][309/469] Loss_D: 0.6966 Loss_G: 2.2250 D(x): 0.8466 D(G(z)): 0.3854 / 0.1254\n",
            "[2/15][310/469] Loss_D: 0.5868 Loss_G: 2.1883 D(x): 0.7562 D(G(z)): 0.2245 / 0.1328\n",
            "[2/15][311/469] Loss_D: 0.5786 Loss_G: 1.5488 D(x): 0.7143 D(G(z)): 0.1810 / 0.2392\n",
            "[2/15][312/469] Loss_D: 0.6493 Loss_G: 2.4025 D(x): 0.8358 D(G(z)): 0.3505 / 0.1093\n",
            "[2/15][313/469] Loss_D: 0.6885 Loss_G: 1.5342 D(x): 0.6472 D(G(z)): 0.1839 / 0.2453\n",
            "[2/15][314/469] Loss_D: 0.7006 Loss_G: 2.2733 D(x): 0.7916 D(G(z)): 0.3455 / 0.1186\n",
            "[2/15][315/469] Loss_D: 0.5966 Loss_G: 1.6694 D(x): 0.6853 D(G(z)): 0.1698 / 0.2123\n",
            "[2/15][316/469] Loss_D: 0.6287 Loss_G: 2.0645 D(x): 0.7794 D(G(z)): 0.2853 / 0.1563\n",
            "[2/15][317/469] Loss_D: 0.5417 Loss_G: 2.2514 D(x): 0.7798 D(G(z)): 0.2190 / 0.1336\n",
            "[2/15][318/469] Loss_D: 0.5754 Loss_G: 1.7448 D(x): 0.7434 D(G(z)): 0.2141 / 0.2046\n",
            "[2/15][319/469] Loss_D: 0.5532 Loss_G: 1.9589 D(x): 0.7865 D(G(z)): 0.2424 / 0.1592\n",
            "[2/15][320/469] Loss_D: 0.5719 Loss_G: 2.2053 D(x): 0.7735 D(G(z)): 0.2435 / 0.1294\n",
            "[2/15][321/469] Loss_D: 0.5744 Loss_G: 1.5382 D(x): 0.7128 D(G(z)): 0.1650 / 0.2383\n",
            "[2/15][322/469] Loss_D: 0.6935 Loss_G: 3.4843 D(x): 0.8852 D(G(z)): 0.4085 / 0.0394\n",
            "[2/15][323/469] Loss_D: 1.0777 Loss_G: 0.6350 D(x): 0.4153 D(G(z)): 0.0696 / 0.5696\n",
            "[2/15][324/469] Loss_D: 1.2056 Loss_G: 4.1115 D(x): 0.9507 D(G(z)): 0.6426 / 0.0233\n",
            "[2/15][325/469] Loss_D: 1.1063 Loss_G: 1.0724 D(x): 0.4072 D(G(z)): 0.0333 / 0.3860\n",
            "[2/15][326/469] Loss_D: 0.9904 Loss_G: 4.1502 D(x): 0.9263 D(G(z)): 0.5437 / 0.0217\n",
            "[2/15][327/469] Loss_D: 1.1180 Loss_G: 0.5596 D(x): 0.3970 D(G(z)): 0.0564 / 0.5936\n",
            "[2/15][328/469] Loss_D: 1.3684 Loss_G: 4.3555 D(x): 0.9541 D(G(z)): 0.6897 / 0.0186\n",
            "[2/15][329/469] Loss_D: 1.2065 Loss_G: 1.1751 D(x): 0.3846 D(G(z)): 0.0657 / 0.3440\n",
            "[2/15][330/469] Loss_D: 0.8529 Loss_G: 2.7201 D(x): 0.8959 D(G(z)): 0.4839 / 0.0865\n",
            "[2/15][331/469] Loss_D: 0.6657 Loss_G: 2.0147 D(x): 0.6697 D(G(z)): 0.1836 / 0.1601\n",
            "[2/15][332/469] Loss_D: 0.6645 Loss_G: 1.4870 D(x): 0.7054 D(G(z)): 0.2296 / 0.2563\n",
            "[2/15][333/469] Loss_D: 0.7213 Loss_G: 2.7726 D(x): 0.8482 D(G(z)): 0.3979 / 0.0780\n",
            "[2/15][334/469] Loss_D: 0.5847 Loss_G: 1.9888 D(x): 0.6570 D(G(z)): 0.1174 / 0.1610\n",
            "[2/15][335/469] Loss_D: 0.6247 Loss_G: 1.6066 D(x): 0.7579 D(G(z)): 0.2548 / 0.2286\n",
            "[2/15][336/469] Loss_D: 0.6370 Loss_G: 2.7156 D(x): 0.8403 D(G(z)): 0.3428 / 0.0810\n",
            "[2/15][337/469] Loss_D: 0.8129 Loss_G: 1.1465 D(x): 0.5601 D(G(z)): 0.1312 / 0.3537\n",
            "[2/15][338/469] Loss_D: 0.7668 Loss_G: 2.4901 D(x): 0.8546 D(G(z)): 0.4254 / 0.0999\n",
            "[2/15][339/469] Loss_D: 0.6924 Loss_G: 1.9092 D(x): 0.6696 D(G(z)): 0.2140 / 0.1710\n",
            "[2/15][340/469] Loss_D: 0.6954 Loss_G: 1.9245 D(x): 0.7420 D(G(z)): 0.2938 / 0.1760\n",
            "[2/15][341/469] Loss_D: 0.6635 Loss_G: 2.4152 D(x): 0.7651 D(G(z)): 0.2921 / 0.1049\n",
            "[2/15][342/469] Loss_D: 0.8104 Loss_G: 1.2666 D(x): 0.5800 D(G(z)): 0.1682 / 0.3486\n",
            "[2/15][343/469] Loss_D: 0.9948 Loss_G: 3.7736 D(x): 0.8596 D(G(z)): 0.5261 / 0.0297\n",
            "[2/15][344/469] Loss_D: 1.0205 Loss_G: 1.0306 D(x): 0.4387 D(G(z)): 0.0711 / 0.4034\n",
            "[2/15][345/469] Loss_D: 1.0503 Loss_G: 3.3222 D(x): 0.9102 D(G(z)): 0.5688 / 0.0456\n",
            "[2/15][346/469] Loss_D: 0.7719 Loss_G: 1.6335 D(x): 0.5619 D(G(z)): 0.1086 / 0.2438\n",
            "[2/15][347/469] Loss_D: 0.7336 Loss_G: 2.2209 D(x): 0.8117 D(G(z)): 0.3740 / 0.1288\n",
            "[2/15][348/469] Loss_D: 0.6268 Loss_G: 2.2186 D(x): 0.7334 D(G(z)): 0.2325 / 0.1366\n",
            "[2/15][349/469] Loss_D: 0.6731 Loss_G: 1.7225 D(x): 0.7009 D(G(z)): 0.2357 / 0.2189\n",
            "[2/15][350/469] Loss_D: 0.7145 Loss_G: 2.6395 D(x): 0.7929 D(G(z)): 0.3532 / 0.0880\n",
            "[2/15][351/469] Loss_D: 0.6915 Loss_G: 1.6576 D(x): 0.6546 D(G(z)): 0.1815 / 0.2154\n",
            "[2/15][352/469] Loss_D: 0.8235 Loss_G: 2.3017 D(x): 0.7461 D(G(z)): 0.3741 / 0.1352\n",
            "[2/15][353/469] Loss_D: 0.8515 Loss_G: 1.4444 D(x): 0.6171 D(G(z)): 0.2392 / 0.2702\n",
            "[2/15][354/469] Loss_D: 0.7513 Loss_G: 2.8678 D(x): 0.7972 D(G(z)): 0.3766 / 0.0716\n",
            "[2/15][355/469] Loss_D: 0.7604 Loss_G: 1.1631 D(x): 0.5970 D(G(z)): 0.1549 / 0.3646\n",
            "[2/15][356/469] Loss_D: 0.8992 Loss_G: 4.2094 D(x): 0.8811 D(G(z)): 0.4929 / 0.0206\n",
            "[2/15][357/469] Loss_D: 1.0931 Loss_G: 0.7902 D(x): 0.4071 D(G(z)): 0.0398 / 0.4938\n",
            "[2/15][358/469] Loss_D: 1.1541 Loss_G: 3.7423 D(x): 0.9233 D(G(z)): 0.6163 / 0.0312\n",
            "[2/15][359/469] Loss_D: 1.0211 Loss_G: 1.0017 D(x): 0.4452 D(G(z)): 0.0705 / 0.4176\n",
            "[2/15][360/469] Loss_D: 1.0233 Loss_G: 2.8740 D(x): 0.8854 D(G(z)): 0.5596 / 0.0677\n",
            "[2/15][361/469] Loss_D: 0.7652 Loss_G: 1.7578 D(x): 0.5945 D(G(z)): 0.1521 / 0.2004\n",
            "[2/15][362/469] Loss_D: 0.6120 Loss_G: 1.6843 D(x): 0.7537 D(G(z)): 0.2468 / 0.2193\n",
            "[2/15][363/469] Loss_D: 0.7745 Loss_G: 2.9509 D(x): 0.8248 D(G(z)): 0.3996 / 0.0675\n",
            "[2/15][364/469] Loss_D: 1.0112 Loss_G: 0.7618 D(x): 0.4555 D(G(z)): 0.1014 / 0.4939\n",
            "[2/15][365/469] Loss_D: 1.1296 Loss_G: 3.2145 D(x): 0.9147 D(G(z)): 0.6127 / 0.0526\n",
            "[2/15][366/469] Loss_D: 0.8301 Loss_G: 1.5132 D(x): 0.5179 D(G(z)): 0.0885 / 0.2575\n",
            "[2/15][367/469] Loss_D: 0.6564 Loss_G: 2.2409 D(x): 0.8325 D(G(z)): 0.3423 / 0.1297\n",
            "[2/15][368/469] Loss_D: 0.5638 Loss_G: 2.0806 D(x): 0.7472 D(G(z)): 0.2054 / 0.1474\n",
            "[2/15][369/469] Loss_D: 0.5991 Loss_G: 1.9859 D(x): 0.7617 D(G(z)): 0.2470 / 0.1608\n",
            "[2/15][370/469] Loss_D: 0.6863 Loss_G: 2.5233 D(x): 0.7916 D(G(z)): 0.3291 / 0.0984\n",
            "[2/15][371/469] Loss_D: 0.7745 Loss_G: 1.0423 D(x): 0.5764 D(G(z)): 0.1492 / 0.3900\n",
            "[2/15][372/469] Loss_D: 0.8949 Loss_G: 2.3028 D(x): 0.8309 D(G(z)): 0.4673 / 0.1203\n",
            "[2/15][373/469] Loss_D: 0.7243 Loss_G: 1.5584 D(x): 0.6310 D(G(z)): 0.1852 / 0.2375\n",
            "[2/15][374/469] Loss_D: 0.6196 Loss_G: 2.0293 D(x): 0.7871 D(G(z)): 0.2862 / 0.1565\n",
            "[2/15][375/469] Loss_D: 0.5860 Loss_G: 1.9728 D(x): 0.7510 D(G(z)): 0.2306 / 0.1596\n",
            "[2/15][376/469] Loss_D: 0.6295 Loss_G: 1.5642 D(x): 0.7079 D(G(z)): 0.2079 / 0.2365\n",
            "[2/15][377/469] Loss_D: 0.6431 Loss_G: 2.6282 D(x): 0.8337 D(G(z)): 0.3452 / 0.0907\n",
            "[2/15][378/469] Loss_D: 0.7023 Loss_G: 1.2786 D(x): 0.6091 D(G(z)): 0.1231 / 0.3061\n",
            "[2/15][379/469] Loss_D: 0.7985 Loss_G: 3.0888 D(x): 0.8667 D(G(z)): 0.4490 / 0.0562\n",
            "[2/15][380/469] Loss_D: 0.7691 Loss_G: 1.2507 D(x): 0.5587 D(G(z)): 0.1069 / 0.3204\n",
            "[2/15][381/469] Loss_D: 0.7581 Loss_G: 2.6759 D(x): 0.8784 D(G(z)): 0.4415 / 0.0808\n",
            "[2/15][382/469] Loss_D: 0.6995 Loss_G: 1.4059 D(x): 0.6013 D(G(z)): 0.1248 / 0.2806\n",
            "[2/15][383/469] Loss_D: 0.7137 Loss_G: 2.4566 D(x): 0.8427 D(G(z)): 0.3811 / 0.1144\n",
            "[2/15][384/469] Loss_D: 0.6245 Loss_G: 1.6652 D(x): 0.6646 D(G(z)): 0.1463 / 0.2177\n",
            "[2/15][385/469] Loss_D: 0.6264 Loss_G: 2.4198 D(x): 0.8211 D(G(z)): 0.3247 / 0.1083\n",
            "[2/15][386/469] Loss_D: 0.6327 Loss_G: 1.8262 D(x): 0.6943 D(G(z)): 0.1971 / 0.1910\n",
            "[2/15][387/469] Loss_D: 0.6031 Loss_G: 1.9359 D(x): 0.7660 D(G(z)): 0.2621 / 0.1685\n",
            "[2/15][388/469] Loss_D: 0.6201 Loss_G: 1.6088 D(x): 0.7210 D(G(z)): 0.2259 / 0.2247\n",
            "[2/15][389/469] Loss_D: 0.5915 Loss_G: 2.7448 D(x): 0.8258 D(G(z)): 0.3009 / 0.0779\n",
            "[2/15][390/469] Loss_D: 0.6235 Loss_G: 1.1379 D(x): 0.6391 D(G(z)): 0.1163 / 0.3536\n",
            "[2/15][391/469] Loss_D: 0.8065 Loss_G: 3.4655 D(x): 0.9013 D(G(z)): 0.4748 / 0.0397\n",
            "[2/15][392/469] Loss_D: 1.0358 Loss_G: 0.7625 D(x): 0.4227 D(G(z)): 0.0642 / 0.5085\n",
            "[2/15][393/469] Loss_D: 1.0541 Loss_G: 3.4529 D(x): 0.9335 D(G(z)): 0.5925 / 0.0410\n",
            "[2/15][394/469] Loss_D: 0.7992 Loss_G: 1.6676 D(x): 0.5531 D(G(z)): 0.0996 / 0.2235\n",
            "[2/15][395/469] Loss_D: 0.5924 Loss_G: 2.3303 D(x): 0.8466 D(G(z)): 0.3260 / 0.1212\n",
            "[2/15][396/469] Loss_D: 0.6514 Loss_G: 2.0948 D(x): 0.7296 D(G(z)): 0.2461 / 0.1430\n",
            "[2/15][397/469] Loss_D: 0.5838 Loss_G: 2.0688 D(x): 0.7615 D(G(z)): 0.2398 / 0.1502\n",
            "[2/15][398/469] Loss_D: 0.6033 Loss_G: 2.2790 D(x): 0.7714 D(G(z)): 0.2690 / 0.1177\n",
            "[2/15][399/469] Loss_D: 0.6516 Loss_G: 1.4149 D(x): 0.6716 D(G(z)): 0.1758 / 0.2699\n",
            "[2/15][400/469] Loss_D: 0.8469 Loss_G: 3.6711 D(x): 0.8576 D(G(z)): 0.4611 / 0.0353\n",
            "saving the output\n",
            "[2/15][401/469] Loss_D: 0.9400 Loss_G: 1.0253 D(x): 0.4616 D(G(z)): 0.0803 / 0.4114\n",
            "[2/15][402/469] Loss_D: 1.0552 Loss_G: 3.7709 D(x): 0.9259 D(G(z)): 0.5783 / 0.0338\n",
            "[2/15][403/469] Loss_D: 0.8387 Loss_G: 1.7161 D(x): 0.5032 D(G(z)): 0.0493 / 0.2203\n",
            "[2/15][404/469] Loss_D: 0.7227 Loss_G: 3.0743 D(x): 0.9172 D(G(z)): 0.4372 / 0.0586\n",
            "[2/15][405/469] Loss_D: 0.7016 Loss_G: 1.4746 D(x): 0.5862 D(G(z)): 0.0849 / 0.2874\n",
            "[2/15][406/469] Loss_D: 0.8017 Loss_G: 3.3051 D(x): 0.9096 D(G(z)): 0.4651 / 0.0444\n",
            "[2/15][407/469] Loss_D: 0.6123 Loss_G: 1.9424 D(x): 0.6511 D(G(z)): 0.1088 / 0.1769\n",
            "[2/15][408/469] Loss_D: 0.8048 Loss_G: 2.1748 D(x): 0.7575 D(G(z)): 0.3597 / 0.1374\n",
            "[2/15][409/469] Loss_D: 0.6950 Loss_G: 1.6935 D(x): 0.6783 D(G(z)): 0.2041 / 0.2099\n",
            "[2/15][410/469] Loss_D: 0.7069 Loss_G: 2.0488 D(x): 0.7373 D(G(z)): 0.2867 / 0.1558\n",
            "[2/15][411/469] Loss_D: 0.5967 Loss_G: 2.1235 D(x): 0.7489 D(G(z)): 0.2357 / 0.1396\n",
            "[2/15][412/469] Loss_D: 0.5391 Loss_G: 2.6284 D(x): 0.8093 D(G(z)): 0.2555 / 0.0837\n",
            "[2/15][413/469] Loss_D: 0.5557 Loss_G: 1.2552 D(x): 0.6858 D(G(z)): 0.1216 / 0.3132\n",
            "[2/15][414/469] Loss_D: 0.7143 Loss_G: 3.0532 D(x): 0.8850 D(G(z)): 0.4186 / 0.0591\n",
            "[2/15][415/469] Loss_D: 0.7422 Loss_G: 1.1380 D(x): 0.5621 D(G(z)): 0.0912 / 0.3706\n",
            "[2/15][416/469] Loss_D: 0.8382 Loss_G: 3.2477 D(x): 0.9092 D(G(z)): 0.4890 / 0.0512\n",
            "[2/15][417/469] Loss_D: 0.8764 Loss_G: 1.1400 D(x): 0.5136 D(G(z)): 0.1091 / 0.3644\n",
            "[2/15][418/469] Loss_D: 0.8824 Loss_G: 3.1807 D(x): 0.8871 D(G(z)): 0.4851 / 0.0528\n",
            "[2/15][419/469] Loss_D: 0.8676 Loss_G: 1.0815 D(x): 0.4935 D(G(z)): 0.0740 / 0.3766\n",
            "[2/15][420/469] Loss_D: 0.8165 Loss_G: 3.4060 D(x): 0.9301 D(G(z)): 0.4877 / 0.0440\n",
            "[2/15][421/469] Loss_D: 0.7749 Loss_G: 1.2698 D(x): 0.5606 D(G(z)): 0.1027 / 0.3105\n",
            "[2/15][422/469] Loss_D: 0.7459 Loss_G: 2.8717 D(x): 0.8568 D(G(z)): 0.4177 / 0.0768\n",
            "[2/15][423/469] Loss_D: 0.6166 Loss_G: 1.5370 D(x): 0.6458 D(G(z)): 0.1084 / 0.2445\n",
            "[2/15][424/469] Loss_D: 0.7055 Loss_G: 2.5144 D(x): 0.8385 D(G(z)): 0.3822 / 0.0972\n",
            "[2/15][425/469] Loss_D: 0.7592 Loss_G: 1.1841 D(x): 0.6055 D(G(z)): 0.1445 / 0.3367\n",
            "[2/15][426/469] Loss_D: 0.8305 Loss_G: 2.7950 D(x): 0.8549 D(G(z)): 0.4569 / 0.0768\n",
            "[2/15][427/469] Loss_D: 0.9407 Loss_G: 0.9022 D(x): 0.4906 D(G(z)): 0.1151 / 0.4421\n",
            "[2/15][428/469] Loss_D: 0.9534 Loss_G: 3.1978 D(x): 0.9008 D(G(z)): 0.5338 / 0.0575\n",
            "[2/15][429/469] Loss_D: 0.8532 Loss_G: 1.3226 D(x): 0.5230 D(G(z)): 0.1088 / 0.3018\n",
            "[2/15][430/469] Loss_D: 0.7754 Loss_G: 2.6033 D(x): 0.8634 D(G(z)): 0.4402 / 0.0910\n",
            "[2/15][431/469] Loss_D: 0.8824 Loss_G: 1.3383 D(x): 0.5558 D(G(z)): 0.1782 / 0.2998\n",
            "[2/15][432/469] Loss_D: 0.7558 Loss_G: 2.6350 D(x): 0.8392 D(G(z)): 0.4079 / 0.0881\n",
            "[2/15][433/469] Loss_D: 0.7559 Loss_G: 1.4301 D(x): 0.6045 D(G(z)): 0.1524 / 0.2736\n",
            "[2/15][434/469] Loss_D: 0.8233 Loss_G: 3.1982 D(x): 0.8508 D(G(z)): 0.4542 / 0.0539\n",
            "[2/15][435/469] Loss_D: 0.8423 Loss_G: 0.9520 D(x): 0.4996 D(G(z)): 0.0665 / 0.4206\n",
            "[2/15][436/469] Loss_D: 1.1186 Loss_G: 3.6841 D(x): 0.9269 D(G(z)): 0.6022 / 0.0328\n",
            "[2/15][437/469] Loss_D: 0.9101 Loss_G: 1.3969 D(x): 0.4907 D(G(z)): 0.0814 / 0.2848\n",
            "[2/15][438/469] Loss_D: 0.6520 Loss_G: 2.9679 D(x): 0.8915 D(G(z)): 0.3915 / 0.0622\n",
            "[2/15][439/469] Loss_D: 0.6489 Loss_G: 1.5875 D(x): 0.6261 D(G(z)): 0.1213 / 0.2529\n",
            "[2/15][440/469] Loss_D: 0.7847 Loss_G: 2.6494 D(x): 0.8024 D(G(z)): 0.3761 / 0.0895\n",
            "[2/15][441/469] Loss_D: 0.8186 Loss_G: 1.7806 D(x): 0.6384 D(G(z)): 0.2400 / 0.2079\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-11f8bb3772f6>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0merrD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0merrD_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mD_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# train with fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r /content/outputs_fashmnist.zip /content/output"
      ],
      "metadata": {
        "id": "VRKolxv_t9ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summary(net):\n",
        "    assert isinstance(net, nn.Module)\n",
        "    print(\"Layer id\\tType\\t\\tParameter\\tNon-zero parameter\\tSparsity(\\%)\")\n",
        "    layer_id = 0\n",
        "    num_total_params = 0\n",
        "    num_total_nonzero_params = 0\n",
        "    for n, m in net.named_modules():\n",
        "        if isinstance(m, PruneLinear):\n",
        "            weight = m.linear.weight.data.cpu().numpy()\n",
        "            weight = weight.flatten()\n",
        "            num_parameters = weight.shape[0]\n",
        "            num_nonzero_parameters = (weight != 0).sum()\n",
        "            sparisty = 1 - num_nonzero_parameters / num_parameters\n",
        "            layer_id += 1\n",
        "            print(\"%d\\t\\tLinear\\t\\t%d\\t\\t%d\\t\\t\\t%f\" %(layer_id, num_parameters, num_nonzero_parameters, sparisty))\n",
        "            num_total_params += num_parameters\n",
        "            num_total_nonzero_params += num_nonzero_parameters\n",
        "        elif isinstance(m, PrunedConv) or isinstance(m, PrunedConvTrans):\n",
        "            weight = m.conv.weight.data.cpu().numpy()\n",
        "            weight = weight.flatten()\n",
        "            num_parameters = weight.shape[0]\n",
        "            print(num_parameters)\n",
        "            num_nonzero_parameters = (weight != 0).sum()\n",
        "            sparisty = 1 - num_nonzero_parameters / num_parameters\n",
        "            layer_id += 1\n",
        "            print(\"%d\\t\\tConvolutional\\t%d\\t\\t%d\\t\\t\\t%f\" % (layer_id, num_parameters, num_nonzero_parameters, sparisty))\n",
        "            num_total_params += num_parameters\n",
        "            num_total_nonzero_params += num_nonzero_parameters\n",
        "        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "            layer_id += 1\n",
        "            print(\"%d\\t\\tBatchNorm\\tN/A\\t\\tN/A\\t\\t\\tN/A\" % (layer_id))\n",
        "        elif isinstance(m, nn.ReLU):\n",
        "            layer_id += 1\n",
        "            print(\"%d\\t\\tReLU\\t\\tN/A\\t\\tN/A\\t\\t\\tN/A\" % (layer_id))\n",
        "\n",
        "    print(\"Total nonzero parameters: %d\" %num_total_nonzero_params)\n",
        "    print(\"Total parameters: %d\" %num_total_params)\n",
        "    total_sparisty = 1. - num_total_nonzero_params / num_total_params\n",
        "    print(\"Total sparsity: %f\" %total_sparisty)\n",
        "\n",
        "    return total_sparisty\n",
        "#####\n"
      ],
      "metadata": {
        "id": "IsrtEI6Bt52o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prune(net, method='std', q=5.0, s=0.25):\n",
        "    # Before training started, please generate the mask\n",
        "    assert isinstance(net, nn.Module)\n",
        "    for n, m in net.named_modules():\n",
        "        if isinstance(m, PrunedConv) or isinstance(m, PruneLinear) or isinstance(m, PrunedConvTrans):\n",
        "            if method == 'percentage':\n",
        "                m.prune_by_percentage(q)\n",
        "            elif method == 'std':\n",
        "                m.prune_by_std(s)\n"
      ],
      "metadata": {
        "id": "_baD48GfQLuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netD = Discriminator(ngpu).to(device)\n",
        "netD.load_state_dict(torch.load(\"netD_epoch_14.pth\"))\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.load_state_dict(torch.load(\"netG_epoch_14.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOEzXlZ_pT0O",
        "outputId": "92f041d1-d2ca-4c28-ac5e-c8af4cfdc514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_noise = torch.randn(256, nz, 1, 1, device=device)\n",
        "fake_images = netG(test_noise)"
      ],
      "metadata": {
        "id": "dAHMe7oPf_B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vutils.save_image(fake_images.detach(),'fake_samples_before_pruning.png', normalize=True)"
      ],
      "metadata": {
        "id": "4Z-_qdezgWRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(netG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4H01Pv5p4vL",
        "outputId": "a8bda867-2587-4011-a5c6-f2905e650c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
            "819200\n",
            "1\t\tConvolutional\t819200\t\t819200\t\t\t0.000000\n",
            "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "2097152\n",
            "4\t\tConvolutional\t2097152\t\t2097152\t\t\t0.000000\n",
            "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "524288\n",
            "7\t\tConvolutional\t524288\t\t524288\t\t\t0.000000\n",
            "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "131072\n",
            "10\t\tConvolutional\t131072\t\t131072\t\t\t0.000000\n",
            "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "1024\n",
            "13\t\tConvolutional\t1024\t\t1024\t\t\t0.000000\n",
            "Total nonzero parameters: 3572736\n",
            "Total parameters: 3572736\n",
            "Total sparsity: 0.000000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(netD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkWHoihZp4ls",
        "outputId": "b41e1885-66a8-46ea-de33-84918e0000b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
            "1024\n",
            "1\t\tConvolutional\t1024\t\t1024\t\t\t0.000000\n",
            "131072\n",
            "2\t\tConvolutional\t131072\t\t131072\t\t\t0.000000\n",
            "3\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "524288\n",
            "4\t\tConvolutional\t524288\t\t524288\t\t\t0.000000\n",
            "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "2097152\n",
            "6\t\tConvolutional\t2097152\t\t2097152\t\t\t0.000000\n",
            "7\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "8192\n",
            "8\t\tConvolutional\t8192\t\t8192\t\t\t0.000000\n",
            "Total nonzero parameters: 2761728\n",
            "Total parameters: 2761728\n",
            "Total sparsity: 0.000000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prune(netD, method='percentage', q=50)\n",
        "prune(netG, method='percentage', q=50)"
      ],
      "metadata": {
        "id": "VTJkfzZrqgBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(netG)"
      ],
      "metadata": {
        "id": "hbmGtCuPRfiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71fd2111-0057-4f2a-ea53-d738fcb62122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
            "819200\n",
            "1\t\tConvolutional\t819200\t\t409600\t\t\t0.500000\n",
            "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "2097152\n",
            "4\t\tConvolutional\t2097152\t\t1048576\t\t\t0.500000\n",
            "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "524288\n",
            "7\t\tConvolutional\t524288\t\t262144\t\t\t0.500000\n",
            "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "131072\n",
            "10\t\tConvolutional\t131072\t\t65536\t\t\t0.500000\n",
            "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "1024\n",
            "13\t\tConvolutional\t1024\t\t512\t\t\t0.500000\n",
            "Total nonzero parameters: 1786368\n",
            "Total parameters: 3572736\n",
            "Total sparsity: 0.500000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(netD)"
      ],
      "metadata": {
        "id": "kVTFgS9BR2by",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9484715f-fb11-4956-c504-9cc58fe66c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
            "1024\n",
            "1\t\tConvolutional\t1024\t\t512\t\t\t0.500000\n",
            "131072\n",
            "2\t\tConvolutional\t131072\t\t65536\t\t\t0.500000\n",
            "3\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "524288\n",
            "4\t\tConvolutional\t524288\t\t262144\t\t\t0.500000\n",
            "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "2097152\n",
            "6\t\tConvolutional\t2097152\t\t1048576\t\t\t0.500000\n",
            "7\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "8192\n",
            "8\t\tConvolutional\t8192\t\t4096\t\t\t0.500000\n",
            "Total nonzero parameters: 1380864\n",
            "Total parameters: 2761728\n",
            "Total sparsity: 0.500000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_noise = torch.randn(256, nz, 1, 1, device=device)\n",
        "pruned_fake_images = netG(test_noise)"
      ],
      "metadata": {
        "id": "fAdmRyVwfpLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vutils.save_image(pruned_fake_images.detach(),'fake_samples_after_pruning.png', normalize=True)"
      ],
      "metadata": {
        "id": "aorr1k5JfpIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sewar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuRIcGCxfpEj",
        "outputId": "4a8b9454-7400-47df-e3f7-95dc6aabe78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sewar\n",
            "  Downloading sewar-0.4.6.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sewar) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sewar) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sewar) (9.4.0)\n",
            "Building wheels for collected packages: sewar\n",
            "  Building wheel for sewar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sewar: filename=sewar-0.4.6-py3-none-any.whl size=11420 sha256=9a833b64fad34c3c7440e0c3e8a8193efbcdb5a87136e8feb78b8dd85bca95bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/af/02/9c6556ba287b62a945d737def09b8b8c35c9e1d82b9dfae84c\n",
            "Successfully built sewar\n",
            "Installing collected packages: sewar\n",
            "Successfully installed sewar-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sewar.full_ref import ssim, msssim, uqi"
      ],
      "metadata": {
        "id": "Vie0h8cofpBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uqi_list = []\n",
        "ssim_list = []\n",
        "msssim_list = []\n",
        "\n",
        "for idx in range(256):\n",
        "    fake_img = fake_images[idx].detach().cpu().numpy().squeeze()\n",
        "    fake_img = (fake_img + 1) * 255 / 2\n",
        "    fake_img = fake_img.astype(int)\n",
        "\n",
        "    fake_img_pruned = pruned_fake_images[idx].detach().cpu().numpy().squeeze()\n",
        "    fake_img_pruned = (fake_img_pruned + 1) * 255 / 2\n",
        "    fake_img_pruned = fake_img_pruned.astype(int)\n",
        "\n",
        "    uqi_list.append(uqi(fake_img_pruned, fake_img))\n",
        "    ssim_list.append(msssim(fake_img_pruned, fake_img))\n",
        "    msssim_list.append(ssim(fake_img_pruned, fake_img))"
      ],
      "metadata": {
        "id": "9t1GFIdafo_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(uqi_list), np.mean(ssim_list), np.mean(msssim_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQdwKXBbfo7Q",
        "outputId": "43221deb-562c-492b-a879-43820a0e6cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9220898153613754, (1+0j), 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-55Lw8PImKDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = pd.DataFrame(columns = [\"UQI\", \"SSIM\", \"MSSSIM\"], index = [\"Mean\", \"Median\", \"Mode\", \"Max\", \"Min\", \"Variance\", \"Std\"])"
      ],
      "metadata": {
        "id": "Il3YgEO5mJ7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(_, idx, counts) = np.unique(uqi_list, return_index=True, return_counts=True)\n",
        "index = idx[np.argmax(counts)]\n",
        "uqi_mode = uqi_list[index]\n",
        "(_, idx, counts) = np.unique(ssim_list, return_index=True, return_counts=True)\n",
        "index = idx[np.argmax(counts)]\n",
        "ssim_mode = ssim_list[index]\n",
        "(_, idx, counts) = np.unique(msssim_list, return_index=True, return_counts=True)\n",
        "index = idx[np.argmax(counts)]\n",
        "msssim_mode = msssim_list[index]\n",
        "\n",
        "stats[\"UQI\"] = [np.mean(uqi_list), np.median(uqi_list), uqi_mode, np.max(uqi_list), np.min(uqi_list), np.var(uqi_list), np.std(uqi_list)]\n",
        "stats[\"SSIM\"] = [np.mean(ssim_list), np.median(ssim_list), ssim_mode, np.max(ssim_list), np.min(ssim_list), np.var(ssim_list), np.std(ssim_list)]\n",
        "stats[\"MSSSIM\"] = [np.mean(msssim_list), np.median(msssim_list), msssim_mode, np.max(msssim_list), np.min(msssim_list), np.var(msssim_list), np.std(msssim_list)]"
      ],
      "metadata": {
        "id": "Z9__CKGkmog4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "N1vYw_zdfo2I",
        "outputId": "0091ba94-83ca-4f51-8388-7273ef26d633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               UQI      SSIM      MSSSIM\n",
              "Mean      0.950989  1.0+0.0j         1.0\n",
              "Median    0.954953  1.0+0.0j         1.0\n",
              "Mode      0.859114  1.0+0.0j  (1.0, 1.0)\n",
              "Max       0.993308  1.0+0.0j         1.0\n",
              "Min       0.859114  1.0+0.0j         1.0\n",
              "Variance  0.000772  0.0+0.0j         0.0\n",
              "Std       0.027780  0.0+0.0j         0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6baf3cc-7f33-45d2-983b-892b6c6a5ef5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UQI</th>\n",
              "      <th>SSIM</th>\n",
              "      <th>MSSSIM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.950989</td>\n",
              "      <td>1.0+0.0j</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Median</th>\n",
              "      <td>0.954953</td>\n",
              "      <td>1.0+0.0j</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mode</th>\n",
              "      <td>0.859114</td>\n",
              "      <td>1.0+0.0j</td>\n",
              "      <td>(1.0, 1.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Max</th>\n",
              "      <td>0.993308</td>\n",
              "      <td>1.0+0.0j</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Min</th>\n",
              "      <td>0.859114</td>\n",
              "      <td>1.0+0.0j</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Variance</th>\n",
              "      <td>0.000772</td>\n",
              "      <td>0.0+0.0j</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Std</th>\n",
              "      <td>0.027780</td>\n",
              "      <td>0.0+0.0j</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6baf3cc-7f33-45d2-983b-892b6c6a5ef5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d6baf3cc-7f33-45d2-983b-892b6c6a5ef5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d6baf3cc-7f33-45d2-983b-892b6c6a5ef5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e163892b-11ea-4df3-9c9b-de656d28b8d7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e163892b-11ea-4df3-9c9b-de656d28b8d7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e163892b-11ea-4df3-9c9b-de656d28b8d7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aq7mHwDroQuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_values = [20, 40, 60, 80]\n",
        "test_noise = torch.randn(256, nz, 1, 1, device=device)\n",
        "fake_images = netG(test_noise)\n",
        "stat_lists = []\n",
        "\n",
        "for q_val in q_values:\n",
        "    netG = Generator(ngpu).to(device)\n",
        "    netG.load_state_dict(torch.load(\"netG_epoch_14.pth\"))\n",
        "    prune(netG, method='percentage', q=q_val)\n",
        "    summary(netG)\n",
        "    pruned_fake_images = netG(test_noise)\n",
        "\n",
        "    uqi_list = []\n",
        "    ssim_list = []\n",
        "    msssim_list = []\n",
        "\n",
        "    for idx in range(256):\n",
        "        fake_img = fake_images[idx].detach().cpu().numpy().squeeze()\n",
        "        fake_img = (fake_img + 1) * 255 / 2\n",
        "        fake_img = fake_img.astype(int)\n",
        "\n",
        "        fake_img_pruned = pruned_fake_images[idx].detach().cpu().numpy().squeeze()\n",
        "        fake_img_pruned = (fake_img_pruned + 1) * 255 / 2\n",
        "        fake_img_pruned = fake_img_pruned.astype(int)\n",
        "\n",
        "        uqi_list.append(uqi(fake_img_pruned, fake_img))\n",
        "        ssim_list.append(msssim(fake_img_pruned, fake_img))\n",
        "        msssim_list.append(ssim(fake_img_pruned, fake_img))\n",
        "\n",
        "    stats = pd.DataFrame(columns = [\"UQI\", \"SSIM\", \"MSSSIM\"], index = [\"Mean\", \"Median\", \"Mode\", \"Max\", \"Min\", \"Variance\", \"Std\"])\n",
        "    (_, idx, counts) = np.unique(uqi_list, return_index=True, return_counts=True)\n",
        "    index = idx[np.argmax(counts)]\n",
        "    uqi_mode = uqi_list[index]\n",
        "    (_, idx, counts) = np.unique(ssim_list, return_index=True, return_counts=True)\n",
        "    index = idx[np.argmax(counts)]\n",
        "    ssim_mode = ssim_list[index]\n",
        "    (_, idx, counts) = np.unique(msssim_list, return_index=True, return_counts=True)\n",
        "    index = idx[np.argmax(counts)]\n",
        "    msssim_mode = msssim_list[index]\n",
        "\n",
        "    stats[\"UQI\"] = [np.mean(uqi_list), np.median(uqi_list), uqi_mode, np.max(uqi_list), np.min(uqi_list), np.var(uqi_list), np.std(uqi_list)]\n",
        "    stats[\"SSIM\"] = [np.mean(ssim_list), np.median(ssim_list), ssim_mode, np.max(ssim_list), np.min(ssim_list), np.var(ssim_list), np.std(ssim_list)]\n",
        "    stats[\"MSSSIM\"] = [np.mean(msssim_list), np.median(msssim_list), msssim_mode, np.max(msssim_list), np.min(msssim_list), np.var(msssim_list), np.std(msssim_list)]\n",
        "\n",
        "\n",
        "    stat_lists.append(stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4FcRWb5rNnm",
        "outputId": "fb1a00b4-cfc5-48e4-8937-c68d517debd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
            "819200\n",
            "1\t\tConvolutional\t819200\t\t655360\t\t\t0.200000\n",
            "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "2097152\n",
            "4\t\tConvolutional\t2097152\t\t1677721\t\t\t0.200000\n",
            "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "524288\n",
            "7\t\tConvolutional\t524288\t\t419430\t\t\t0.200001\n",
            "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "131072\n",
            "10\t\tConvolutional\t131072\t\t104857\t\t\t0.200005\n",
            "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "1024\n",
            "13\t\tConvolutional\t1024\t\t819\t\t\t0.200195\n",
            "Total nonzero parameters: 2858187\n",
            "Total parameters: 3572736\n",
            "Total sparsity: 0.200001\n",
            "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
            "819200\n",
            "1\t\tConvolutional\t819200\t\t491520\t\t\t0.400000\n",
            "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "2097152\n",
            "4\t\tConvolutional\t2097152\t\t1258291\t\t\t0.400000\n",
            "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "524288\n",
            "7\t\tConvolutional\t524288\t\t314573\t\t\t0.400000\n",
            "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "131072\n",
            "10\t\tConvolutional\t131072\t\t78643\t\t\t0.400002\n",
            "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "1024\n",
            "13\t\tConvolutional\t1024\t\t614\t\t\t0.400391\n",
            "Total nonzero parameters: 2143641\n",
            "Total parameters: 3572736\n",
            "Total sparsity: 0.400000\n",
            "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
            "819200\n",
            "1\t\tConvolutional\t819200\t\t327680\t\t\t0.600000\n",
            "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "2097152\n",
            "4\t\tConvolutional\t2097152\t\t838861\t\t\t0.600000\n",
            "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "524288\n",
            "7\t\tConvolutional\t524288\t\t209715\t\t\t0.600000\n",
            "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "131072\n",
            "10\t\tConvolutional\t131072\t\t52429\t\t\t0.599998\n",
            "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "1024\n",
            "13\t\tConvolutional\t1024\t\t410\t\t\t0.599609\n",
            "Total nonzero parameters: 1429095\n",
            "Total parameters: 3572736\n",
            "Total sparsity: 0.600000\n",
            "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
            "819200\n",
            "1\t\tConvolutional\t819200\t\t163840\t\t\t0.800000\n",
            "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "2097152\n",
            "4\t\tConvolutional\t2097152\t\t419432\t\t\t0.799999\n",
            "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "524288\n",
            "7\t\tConvolutional\t524288\t\t104858\t\t\t0.799999\n",
            "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "131072\n",
            "10\t\tConvolutional\t131072\t\t26215\t\t\t0.799995\n",
            "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
            "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
            "1024\n",
            "13\t\tConvolutional\t1024\t\t205\t\t\t0.799805\n",
            "Total nonzero parameters: 714550\n",
            "Total parameters: 3572736\n",
            "Total sparsity: 0.799999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stat_lists[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "fMgGzRYkuYPD",
        "outputId": "eb27704f-e082-4ab4-8d96-2ee95e27c387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               UQI      SSIM      MSSSIM\n",
              "Mean      0.704175  1.0+0.0j         1.0\n",
              "Median    0.707740  1.0+0.0j         1.0\n",
              "Mode      0.398456  1.0+0.0j  (1.0, 1.0)\n",
              "Max       0.925480  1.0+0.0j         1.0\n",
              "Min       0.398456  1.0+0.0j         1.0\n",
              "Variance  0.010979  0.0+0.0j         0.0\n",
              "Std       0.104783  0.0+0.0j         0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2684af77-7301-41e4-a5b9-af3fd99c860c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UQI</th>\n",
              "      <th>SSIM</th>\n",
              "      <th>MSSSIM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.704175</td>\n",
              "      <td>1.0+0.0j</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Median</th>\n",
              "      <td>0.707740</td>\n",
              "      <td>1.0+0.0j</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mode</th>\n",
              "      <td>0.398456</td>\n",
              "      <td>1.0+0.0j</td>\n",
              "      <td>(1.0, 1.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Max</th>\n",
              "      <td>0.925480</td>\n",
              "      <td>1.0+0.0j</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Min</th>\n",
              "      <td>0.398456</td>\n",
              "      <td>1.0+0.0j</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Variance</th>\n",
              "      <td>0.010979</td>\n",
              "      <td>0.0+0.0j</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Std</th>\n",
              "      <td>0.104783</td>\n",
              "      <td>0.0+0.0j</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2684af77-7301-41e4-a5b9-af3fd99c860c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2684af77-7301-41e4-a5b9-af3fd99c860c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2684af77-7301-41e4-a5b9-af3fd99c860c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e3afbb7d-1e10-4f5d-be58-12676cfd3750\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3afbb7d-1e10-4f5d-be58-12676cfd3750')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e3afbb7d-1e10-4f5d-be58-12676cfd3750 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-qT3LpCuYKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6IyokgXC11_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "amCcF_uuqxnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extra experimentation for further analysis in the future"
      ],
      "metadata": {
        "id": "atL2zDew1YRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,   # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False):\n",
        "\n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \\\n",
        "            'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "\n",
        "        inception = models.inception_v3(pretrained=True)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = torch.nn.functional.interpolate(x,\n",
        "                                                size=(299, 299),\n",
        "                                                mode='bilinear',\n",
        "                                                align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp\n",
        "\n",
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "model = InceptionV3([block_idx])\n",
        "model=model.cuda()"
      ],
      "metadata": {
        "id": "65CM2UubR2e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581d44ca-7505-4986-ad77-bffc16a64fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 146MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_activation_statistics(images,model,batch_size=128, dims=10,\n",
        "                    cuda=False):\n",
        "    model.eval()\n",
        "    act=np.empty((len(images), dims))\n",
        "\n",
        "    if cuda:\n",
        "        batch=images.cuda()\n",
        "    else:\n",
        "        batch=images\n",
        "    pred = model(batch)[0]\n",
        "\n",
        "        # If model output is not scalar, apply global spatial average pooling.\n",
        "        # This happens if you choose a dimensionality not equal 2048.\n",
        "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "        pred = torch.nn.functional.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
        "\n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma"
      ],
      "metadata": {
        "id": "BaLSszTQqy3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "\n",
        "    if np.iscomplexobj(covmean):\n",
        "        # if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            # m = np.max(np.abs(covmean.imag))\n",
        "            # raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) +\n",
        "            np.trace(sigma2) - 2 * tr_covmean)"
      ],
      "metadata": {
        "id": "pbuwTcrFqyzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fretchet(images_real,images_fake,model):\n",
        "     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
        "     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
        "\n",
        "     \"\"\"get fretched distance\"\"\"\n",
        "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
        "     return fid_value"
      ],
      "metadata": {
        "id": "rqvJ4CLxqyvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
        "                                              shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "TsWmB7BHsOEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fid_scores = []\n",
        "for img_batch in test_dataloader:\n",
        "    test_noisenoise = torch.randn(128, nz, 1, 1, device=device)\n",
        "    fake_images = netG(test_noisenoise).detach().cpu().numpy()\n",
        "    rgb_fake_images = np.repeat(fake_images[..., np.newaxis], 3, 1).squeeze()\n",
        "    reshaped_img_batch = img_batch[0].detach().cpu().numpy()\n",
        "    rgb_reshaped_img_batch = np.repeat(reshaped_img_batch[..., np.newaxis], 3, 1).squeeze()\n",
        "    fid_scores.append(calculate_fretchet(torch.Tensor(rgb_reshaped_img_batch), torch.Tensor(rgb_fake_images), model))\n",
        "\n",
        "# real = next(test_dataloader)"
      ],
      "metadata": {
        "id": "ZArK3Ox1qpT2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "6c9f4cdf-51cf-4a6b-b40f-024430db3609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-de4e156ae6ba>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreshaped_img_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrgb_reshaped_img_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_img_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfid_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_fretchet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_reshaped_img_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_fake_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# real = next(test_dataloader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-1b3f301ed24a>\u001b[0m in \u001b[0;36mcalculate_fretchet\u001b[0;34m(images_real, images_fake, model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0;34m\"\"\"get fretched distance\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m      \u001b[0mfid_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_frechet_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mfid_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-e830b7848ab5>\u001b[0m in \u001b[0;36mcalculate_frechet_distance\u001b[0;34m(mu1, sigma1, mu2, sigma2, eps)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mcovmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrtm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         msg = ('fid calculation produces singular product; '\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/_matfuncs_sqrtm.py\u001b[0m in \u001b[0;36msqrtm\u001b[0;34m(A, disp, blocksize)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mkeep_it_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misrealobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeep_it_real\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrsf2csf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_schur.py\u001b[0m in \u001b[0;36mschur\u001b[0;34m(a, output, lwork, overwrite_a, sort, check_finite)\u001b[0m\n\u001b[1;32m    162\u001b[0m                              \"callable, or one of ('lhp','rhp','iuc','ouc')\")\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     result = gees(sfunction, a1, lwork=lwork, overwrite_a=overwrite_a,\n\u001b[0m\u001b[1;32m    165\u001b[0m                   sort_t=sort_t)\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FFuO9kxzLSNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803f90f9-2fec-40f7-fa1e-4727a5c11987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sewar\n",
            "  Downloading sewar-0.4.6.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sewar) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sewar) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sewar) (9.4.0)\n",
            "Building wheels for collected packages: sewar\n",
            "  Building wheel for sewar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sewar: filename=sewar-0.4.6-py3-none-any.whl size=11420 sha256=0c0b892659315592b2ee60cc60db87be6218282583a62c8e9bea142cfbfa835d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/af/02/9c6556ba287b62a945d737def09b8b8c35c9e1d82b9dfae84c\n",
            "Successfully built sewar\n",
            "Installing collected packages: sewar\n",
            "Successfully installed sewar-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sewar.full_ref import mse, rmse, psnr, uqi, ssim, ergas, scc, rase, sam, msssim, vifp"
      ],
      "metadata": {
        "id": "tW9ShnsVON7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "P1vxrHLSOODg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YWjv0j_KPOfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6DwTlGyYS6ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJ2sTjPDS6eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GhF79bvqS6h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom resnet for calculating FID"
      ],
      "metadata": {
        "id": "bNZMsIvgS7OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import time\n",
        "from tqdm.autonotebook import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import inspect\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jbeM5IT2S6mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistResNet(nn.Module):\n",
        "  def __init__(self, in_channels=1):\n",
        "    super(MnistResNet, self).__init__()\n",
        "\n",
        "    # Load a pretrained resnet model from torchvision.models in Pytorch\n",
        "    self.model = models.resnet50(pretrained=True)\n",
        "\n",
        "    # Change the input layer to take Grayscale image, instead of RGB images.\n",
        "    # Hence in_channels is set as 1 or 3 respectively\n",
        "    # original definition of the first layer on the ResNet class\n",
        "    # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "    # Change the output layer to output 10 classes instead of 1000 classes\n",
        "    num_ftrs = self.model.fc.in_features\n",
        "    self.model.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n"
      ],
      "metadata": {
        "id": "ahYtz1IfS6og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_resnet = MnistResNet()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTPUrB_wPOkB",
        "outputId": "aded83d4-91bf-4a1f-9ce8-c15b72786791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 158MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "OkHwa7njTE6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(train_batch_size, val_batch_size):\n",
        "    fashion_mnist = torchvision.datasets.FashionMNIST(download=True, train=True, root=\".\").train_data.float()\n",
        "\n",
        "    data_transform = transforms.Compose([ transforms.Resize((224, 224)),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((fashion_mnist.mean()/255,), (fashion_mnist.std()/255,))])\n",
        "\n",
        "    train_loader = DataLoader(torchvision.datasets.FashionMNIST(download=True, root=\".\", transform=data_transform, train=True),\n",
        "                              batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "    val_loader = DataLoader(torchvision.datasets.FashionMNIST(download=True, root=\".\", transform=data_transform, train=False),\n",
        "                            batch_size=val_batch_size, shuffle=False)\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "gbJjlRZ9TE-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metric(metric_fn, true_y, pred_y):\n",
        "    if \"average\" in inspect.getfullargspec(metric_fn).args:\n",
        "        return metric_fn(true_y, pred_y, average=\"macro\")\n",
        "    else:\n",
        "        return metric_fn(true_y, pred_y)\n",
        "\n",
        "def print_scores(p, r, f1, a, batch_size):\n",
        "    for name, scores in zip((\"precision\", \"recall\", \"F1\", \"accuracy\"), (p, r, f1, a)):\n",
        "        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores)/batch_size:.4f}\")"
      ],
      "metadata": {
        "id": "WcWrQy5qTFBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model:\n",
        "model = MnistResNet().to(device)\n",
        "\n",
        "# params you need to specify:\n",
        "epochs = 1\n",
        "batch_size = 128\n",
        "\n",
        "# Dataloaders\n",
        "train_loader, val_loader = get_data_loaders(batch_size, batch_size)\n",
        "\n",
        "# loss function and optimiyer\n",
        "loss_function = nn.CrossEntropyLoss() # your loss function, cross entropy works well for multi-class problems\n",
        "\n",
        "# optimizer, I've used Adadelta, as it wokrs well without any magic numbers\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4) # Using Karpathy's learning rate constant\n",
        "\n",
        "start_ts = time.time()\n",
        "\n",
        "losses = []\n",
        "batches = len(train_loader)\n",
        "val_batches = len(val_loader)\n",
        "\n",
        "# loop for every epoch (training + evaluation)\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    # progress bar (works in Jupyter notebook too!)\n",
        "    progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=batches)\n",
        "\n",
        "    # ----------------- TRAINING  --------------------\n",
        "    # set model to training\n",
        "    model.train()\n",
        "\n",
        "    for i, data in progress:\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # training step for single batch\n",
        "        model.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = loss_function(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # getting training quality data\n",
        "        current_loss = loss.item()\n",
        "        total_loss += current_loss\n",
        "\n",
        "        # updating progress bar\n",
        "        progress.set_description(\"Loss: {:.4f}\".format(total_loss/(i+1)))\n",
        "\n",
        "    # releasing unceseccary memory in GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # ----------------- VALIDATION  -----------------\n",
        "    val_losses = 0\n",
        "    precision, recall, f1, accuracy = [], [], [], []\n",
        "\n",
        "    # set model to evaluating (testing)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            X, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            outputs = model(X) # this get's the prediction from the network\n",
        "\n",
        "            val_losses += loss_function(outputs, y)\n",
        "\n",
        "            predicted_classes = torch.max(outputs, 1)[1] # get class from network's prediction\n",
        "\n",
        "            # calculate P/R/F1/A metrics for batch\n",
        "            for acc, metric in zip((precision, recall, f1, accuracy),\n",
        "                                   (precision_score, recall_score, f1_score, accuracy_score)):\n",
        "                acc.append(\n",
        "                    calculate_metric(metric, y.cpu(), predicted_classes.cpu())\n",
        "                )\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, training loss: {total_loss/batches}, validation loss: {val_losses/val_batches}\")\n",
        "    print_scores(precision, recall, f1, accuracy, val_batches)\n",
        "    losses.append(total_loss/batches) # for plotting learning curve\n",
        "print(f\"Training time: {time.time()-start_ts}s\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467,
          "referenced_widgets": [
            "65b1b1a51c1e4fd2841a901e6f98d864",
            "ee4466ad132b44e681b57338a55412a5",
            "73f62d667f6144dba1c0dd27fad46635",
            "7b879f935b334ab9b22d3465064848d8",
            "b7a11f94bcee44f3a33d25c3cc389655",
            "0a299ea708994b7a9cfa5b6534ae818e",
            "55736365d0664a38a87d40390a870525",
            "31a7887a944c4c19a2f8635ad77d2bb0",
            "2adb166b406c46cb918de2af13199d71",
            "d17cfb2d1e4b4717b9fb49413ecff0bb",
            "4e68c81934c34106bb85109bcc2c278d"
          ]
        },
        "id": "Vpbs1UpcTbUR",
        "outputId": "c036fccb-2756-4c68-a13a-f835c2b4d80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loss:   0%|          | 0/469 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65b1b1a51c1e4fd2841a901e6f98d864"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-c2164189fe19>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m                                    (precision_score, recall_score, f1_score, accuracy_score)):\n\u001b[1;32m     74\u001b[0m                 acc.append(\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mcalculate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 )\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-08343cce142c>\u001b[0m in \u001b[0;36mcalculate_metric\u001b[0;34m(metric_fn, true_y, pred_y)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"macro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1952\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m     \"\"\"\n\u001b[0;32m-> 1954\u001b[0;31m     p, _, _, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   1955\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m                 \u001b[0maverage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1392\u001b[0m                 \u001b[0;34m\"Target is %s but average='binary'. Please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m                 \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), \"MnistResNet.pt\")\n",
        "custom_model = MnistResNet()\n",
        "custom_model.load_state_dict(torch.load(\"MnistResNet.pt\"))\n",
        "custom_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FgSHQ5mTbX5",
        "outputId": "60a321f2-9c9e-4766-d0c9-4060067572b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MnistResNet(\n",
              "  (model): ResNet(\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
        "                                              shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "nyXZ4XBIXTDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fid_scores = []\n",
        "p = transforms.Compose([transforms.Resize((224,224))])\n",
        "\n",
        "for img_batch in test_dataloader:\n",
        "\n",
        "    test_noisenoise = torch.randn(128, nz, 1, 1, device=device)\n",
        "    fake_images = netG(test_noisenoise)\n",
        "    fake_images = p(fake_images)\n",
        "\n",
        "    real_images = p(img_batch[0])\n",
        "\n",
        "\n",
        "\n",
        "    # rgb_reshaped_img_batch = np.repeat(reshaped_img_batch[..., np.newaxis], 3, 1).squeeze()\n",
        "\n",
        "    fid_scores.append(calculate_fretchet(real_images, fake_images, custom_model))\n",
        "    break\n"
      ],
      "metadata": {
        "id": "u3IZl5B7TbbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = transforms.Compose([transforms.Resize((224,224))])"
      ],
      "metadata": {
        "id": "9WI_WZcgYJ4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_noisenoise = torch.randn(128, nz, 1, 1, device=device)\n",
        "fake_images = netG(test_noisenoise)\n",
        "fake_images = p(fake_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJdqyB4dYG1u",
        "outputId": "5bc89533-0c45-49bf-cf45-6c24b95b737f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ErvznlR6ZwfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}